{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNanEbMvZhqz0chRsgW9uCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC4050/blob/main/Week_12-Classification_I/Data_Science_Fiction_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science Fiction II"
      ],
      "metadata": {
        "id": "4SUvtSLX7Zcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC4050 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link"
      ],
      "metadata": {
        "id": "A3G6lZ76KRm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "The goal of this assignment is to take messy data, clean it up, and then analyze it using logistic regression.\n",
        "\n",
        "* Run Parts 1 and 2 being careful to take in what's going on\n",
        "* In Part 3 you are asked to clean the data in preparation for modeling\n",
        "* Part 4 - perform necessary feature engineering\n",
        "* Part 5 - select the variables that will be more useful for classification\n",
        "* Part 6 - model the data and evaluate, explain concepts when asked"
      ],
      "metadata": {
        "id": "lUT-IAfltFMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - The Data"
      ],
      "metadata": {
        "id": "RpWcbBh6rAKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seed the Project"
      ],
      "metadata": {
        "id": "FMVaAmuqAWhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def generate_user_seed():\n",
        "    # Get current time in nanoseconds (more granular)\n",
        "    nanoseconds = time.time_ns()\n",
        "\n",
        "    # Add a small random component to further reduce collision chances\n",
        "    random_component = random.randint(0, 1000)  # Adjust range as needed\n",
        "\n",
        "    # Combine them (XOR is a good way to mix values)\n",
        "    seed = nanoseconds ^ random_component\n",
        "\n",
        "    # Ensure the seed is within the valid range for numpy's seed\n",
        "    seed = seed % (2**32)  # Modulo to keep it within 32-bit range\n",
        "\n",
        "    return seed\n",
        "\n",
        "user_seed = generate_user_seed()\n",
        "print(user_seed)\n",
        "random_state = np.random.seed(user_seed)"
      ],
      "metadata": {
        "id": "YUMcsfNhXbRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faker"
      ],
      "metadata": {
        "id": "_j1l2Lr8_bS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Faker -q"
      ],
      "metadata": {
        "id": "hViR5zhSGxq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "habitable_planets = [\n",
        "    \"Alpha Centauri III\",\n",
        "    \"Eden\",\n",
        "    \"Terra Nova\",\n",
        "    \"Tiberius\",\n",
        "    \"Vega Colony\",\n",
        "    \"Cait\",\n",
        "    \"Andoria\",\n",
        "    \"Vulcanis\",\n",
        "    \"Risa\",\n",
        "    \"Betazed\",\n",
        "    \"Ba'ku\",\n",
        "    \"Aldea\",\n",
        "    \"Nimbus III\",\n",
        "    \"Deneva\",\n",
        "    \"Capella IV\",\n",
        "    \"Organia\",\n",
        "    \"Trillius Prime\",\n",
        "    \"Kaelon II\",\n",
        "    \"Mintaka III\",\n",
        "    \"Rubicun III\",\n",
        "    \"Pacifica\",\n",
        "    \"Tau Ceti III\",\n",
        "    \"Melina\",\n",
        "    \"Argelius II\",\n",
        "    \"Iconia\",\n",
        "    \"Alderaan\",\n",
        "    \"Naboo\",\n",
        "    \"Bespin (Cloud City)\",\n",
        "    \"Yavin IV\",\n",
        "    \"Endor (Forest Moon)\",\n",
        "    \"Kashyyyk\",\n",
        "    \"Mon Cala\",\n",
        "    \"Corellia\",\n",
        "    \"Chandrila\",\n",
        "    \"Ryloth\",\n",
        "    \"Cato Neimoidia\",\n",
        "    \"Felucia\",\n",
        "    \"Saleucami\",\n",
        "    \"Stewjon\",\n",
        "    \"Iego\",\n",
        "    \"Glee Anselm\",\n",
        "    \"Mirial\",\n",
        "    \"Serenno\",\n",
        "    \"Malastare\",\n",
        "    \"Dantooine\",\n",
        "    \"Haruun Kal\",\n",
        "    \"Manaan\",\n",
        "    \"Zolan\",\n",
        "    \"Ord Mantell\",\n",
        "    \"Pantora\"\n",
        "]"
      ],
      "metadata": {
        "id": "Qpa4mbcgKoEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create demographic data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "fake = Faker()\n",
        "\n",
        "n = 1000\n",
        "\n",
        "output = []\n",
        "for x in range(n):\n",
        "    biology = np.random.choice(['Cytophore', 'Kymete'], p=[0.5, 0.5])\n",
        "    output.append({\n",
        "        'categorical_1': biology,\n",
        "        'categorical_2': np.random.choice(['Xylosian', 'Veridian', 'CKaeltharr']),\n",
        "        'name_1': fake.first_name_female() if biology == 'Cytophore' else fake.first_name_male(),\n",
        "        'name_2': fake.last_name(),\n",
        "        'code': fake.zipcode(),\n",
        "        'date': fake.date_of_birth(),\n",
        "        'location': np.random.choice(habitable_planets)\n",
        "    })\n",
        "\n",
        "demographics = pd.DataFrame(output)\n",
        "print(demographics.shape)\n",
        "demographics.head()"
      ],
      "metadata": {
        "id": "HqvkXvuWKbXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Independent Variable Correlated with Class"
      ],
      "metadata": {
        "id": "4A99Q0IknJCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_feature(df, class_col, coeff, intercept):\n",
        "    \"\"\"\n",
        "    Generates normally distributed feature data for a logistic regression model.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame containing the class column.\n",
        "        class_col: The name of the class column (containing 0s and 1s).\n",
        "        coeff: The coefficient for the feature in the logistic regression model.\n",
        "        intercept: The intercept of the logistic regression model.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series containing the generated feature data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate probabilities based on the class\n",
        "    probs = np.random.rand(len(df))  # Initial random probabilities\n",
        "    probs = np.where(df[class_col] == 1, probs * 0.8 + 0.2, probs * 0.8)  # Adjust for class\n",
        "\n",
        "    # Apply the inverse logit (logit) function\n",
        "    logits = np.log(probs / (1 - probs))\n",
        "\n",
        "    # Calculate the feature values\n",
        "    feature_values = (logits - intercept) / coeff\n",
        "\n",
        "    return pd.Series(feature_values)\n",
        "\n"
      ],
      "metadata": {
        "id": "n3VmXvwBnRr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Classification"
      ],
      "metadata": {
        "id": "kmOFYI7T_eoP"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def make_linear_y(row):\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X, y)\n",
        "  coefficients = model.coef_\n",
        "  intercept = model.intercept_\n",
        "  f_of_x = intercept + coefficients[0][0]*row['informative_1'] + coefficients[0][1]*row['informative_2']\n",
        "  # print(f_of_x[0])\n",
        "  return f_of_x[0]\n",
        "\n",
        "# Adjust the make_classification parameters:\n",
        "# Set n_informative and n_redundant to values that sum to less than n_features\n",
        "X, y = make_classification(n_samples=n, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=42)\n",
        "df = pd.DataFrame(X, columns=['informative_1', 'informative_2'])\n",
        "df = pd.concat([demographics, df], axis=1).reset_index(drop=True)\n",
        "\n",
        "df['target'] = df.apply(make_linear_y, axis=1) # an independent variable\n",
        "df['class'] = y # the dependent variable\n",
        "df['corr_feature_class'] = generate_feature(df, 'class', 0.5, -1)\n",
        "df.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1UYr8YXrJkX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automation Functions\n",
        "\n",
        "1. gen_null(series, perc)\n",
        "2. gen_quasi_constants(primary_label, variation_percentage=.2, replace=False)\n",
        "3. gen_normal_data(mu=0, std=1, size=len(df))\n",
        "4. gen_uniform_data(size=len(df))\n",
        "5. gen_multivariate_normal_data(mean=[0, 0], cov=[[1, 0], [0, 1]], size=len(df))\n",
        "6. gen_correlated_normal_series(original_series, target_correlation, size=len(df))\n",
        "7. gen_correlated_uniform_series(original_series, correlation_coefficient=0, size=len(df))\n",
        "8. gen_outliers(mean=0, std_dev=1, size=len(df), outlier_percentage=0.1, outlier_magnitude=3)\n",
        "9. gen_standard_scaling(mean=50, std_dev=10, size=len(df), scale_factor=1000)\n",
        "10. gen_minmax_scaling(mean=50, std_dev=10, size=len(df), range_factor=10)\n",
        "11. random_choice_data(choices, size)"
      ],
      "metadata": {
        "id": "FlnsQrSN8FLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "def gen_null(series, perc):\n",
        "  \"\"\"\n",
        "  Introduces null values (np.nan) into a list based on a specified percentage.\n",
        "\n",
        "  Args:\n",
        "      var: The variable to modify.\n",
        "      perc: The percentage of values to replace with nulls (0-100).\n",
        "\n",
        "  Returns:\n",
        "      The modified variable with null.\n",
        "  \"\"\"\n",
        "  var = series.copy()\n",
        "  num_nulls = int(len(var) * (perc / 100))\n",
        "  indices_to_replace = np.random.choice(len(var), num_nulls, replace=False)\n",
        "\n",
        "  for idx in indices_to_replace:\n",
        "      var[idx] = np.nan\n",
        "\n",
        "  return var\n",
        "\n",
        "def gen_quasi_constants(primary_label, variation_percentage=.2, size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates quasi-constant labels for a Series, with a small percentage of variation.\n",
        "\n",
        "  Args:\n",
        "      primary_label: The main label to use for most values.\n",
        "      variation_percentage: The percentage of labels to vary (0-100).\n",
        "\n",
        "  Returns:\n",
        "      A new Series containing the quasi-constant labels.\n",
        "  \"\"\"\n",
        "\n",
        "  series = pd.Series(np.full(size, primary_label))\n",
        "  num_variations = int(size * (variation_percentage / 100))\n",
        "  variation_indices = np.random.choice(series.index, num_variations, replace=False)\n",
        "  primary_label = primary_label + '_0'\n",
        "  variation1 = primary_label + '_1'\n",
        "  variation2 = primary_label + '_2'\n",
        "\n",
        "  labels = pd.Series([primary_label] * len(series), index=series.index)\n",
        "  labels.loc[variation_indices] = np.random.choice([variation1, variation2], size=num_variations)  # Adjust variations as needed\n",
        "\n",
        "  return labels\n",
        "\n",
        "def gen_normal_data(mu=0, std=1, size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates a normal dataset given the mean and standard deviation\n",
        "\n",
        "  Args:\n",
        "        mu: The mean of the normal distribution.\n",
        "        std: The standard deviation of the normal distribution.\n",
        "        size: The number of data points to generate.\n",
        "\n",
        "  Returns:\n",
        "        A normally distributed series.\n",
        "  \"\"\"\n",
        "  return np.random.normal(mu, std, size)\n",
        "\n",
        "def gen_uniform_data(size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates a uniform dataset\n",
        "\n",
        "  Args:\n",
        "        size: The number of data points to generate.\n",
        "\n",
        "  Returns:\n",
        "        A uniform distributed series.\n",
        "  \"\"\"\n",
        "  return np.random.uniform(size=size)\n",
        "\n",
        "def gen_multivariate_normal_data(mean=[0, 0], cov=[[1, 0], [0, 1]], size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates two datasets with a multivariate normal distribution given the mean and covariance matrix\n",
        "\n",
        "  Args:\n",
        "        mean: The mean of each of the datasets.\n",
        "        cov: The covariance matrix of the datasets.\n",
        "        size: The number of data points to generate.\n",
        "\n",
        "  Returns:\n",
        "        Two correlated series.\n",
        "  \"\"\"\n",
        "  ds1, ds2 = np.random.multivariate_normal(mean, cov, size, tol=1e-6).T # ds = dataset\n",
        "  return ds1, ds2\n",
        "\n",
        "def gen_correlated_normal_series(original_series, target_correlation, size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates a correlated series based on a given series.\n",
        "\n",
        "  This function takes an original series as input and generates a new series\n",
        "  that is correlated with the original series. The correlation between the\n",
        "  original and generated series is approximately equal to the specified\n",
        "  target correlation.\n",
        "\n",
        "  The generated series is created by linearly transforming the original series\n",
        "  and adding Gaussian noise with an adjusted standard deviation to achieve the\n",
        "  desired correlation.\n",
        "\n",
        "  Args:\n",
        "      original_series (numpy.ndarray): The original series.\n",
        "      target_correlation (float): The desired Pearson correlation coefficient\n",
        "          between the original and generated series.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: The generated correlated series.\n",
        "  \"\"\"\n",
        "  return np.mean(original_series) + target_correlation * (original_series - np.mean(original_series)) \\\n",
        "  +  np.random.normal(0, np.sqrt(1 - target_correlation**2) * np.std(original_series), len(original_series))\n",
        "  \"\"\"\n",
        "  Explanation\n",
        "\n",
        "  This one-liner leverages the properties of linear transformations and normal distributions to generate a correlated series.\n",
        "\n",
        "  It first centers the original_series by subtracting its mean.\n",
        "  It then scales this centered series by the target_correlation.\n",
        "  Finally, it adds Gaussian noise with a standard deviation adjusted to ensure the overall correlation matches the target_correlation.\n",
        "  \"\"\"\n",
        "\n",
        "def gen_correlated_uniform_series(original_series, correlation_coefficient=0, size=len(df)):\n",
        "  \"\"\"\n",
        "  Work in progress\n",
        "\n",
        "  Generates a new series correlated with the given series based on the specified correlation coefficient,\n",
        "  using rank correlation to ensure the generated series follows a uniform distribution.\n",
        "\n",
        "  Args:\n",
        "      original_series (numpy.ndarray or list): The original series.\n",
        "      correlation_coefficient (float): The desired correlation coefficient between the original and generated series.\n",
        "      size: The number of data points to generate.\n",
        "\n",
        "  Returns:\n",
        "      The generated correlated series with a uniform distribution.\n",
        "  \"\"\"\n",
        "  z_scores = (original_series - np.mean(original_series)) / np.std(original_series)\n",
        "  correlation_coefficient=.7\n",
        "  return norm.cdf(correlation_coefficient * norm.ppf(np.random.uniform(size=size)) + np.sqrt(1 - correlation_coefficient**2) * z_scores)\n",
        "\n",
        "def pearson_r_func(x, y, y_mean, y_std, desired_r):\n",
        "    x_mean = np.mean(x)\n",
        "    x_std = np.std(x)\n",
        "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
        "    denominator = x_std * y_std * len(x)\n",
        "    calculated_r = numerator / denominator\n",
        "    return (calculated_r - desired_r)**2  # Minimize the squared difference\n",
        "\n",
        "def minimize_r(original_series, target_correlation, size=len(df)):\n",
        "    y = original_series\n",
        "    y_mean = np.mean(y)\n",
        "    y_std = np.std(y)\n",
        "    desired_r = target_correlation\n",
        "\n",
        "    # Initial guess for x values\n",
        "    x0 = np.random.uniform(size=len(original_series))\n",
        "\n",
        "    # Solve for x\n",
        "    result = minimize(pearson_r_func, x0, args=(y, y_mean, y_std, desired_r))\n",
        "\n",
        "    if result.success:\n",
        "        x_solution = result.x\n",
        "        # print(\"Solution for x:\", x_solution)\n",
        "        return x_solution\n",
        "    else:\n",
        "        print(\"Optimization failed.\")\n",
        "\n",
        "def gen_outliers(mean=0, std_dev=1, size=len(df), outlier_percentage=0.1, outlier_magnitude=3):\n",
        "    \"\"\"\n",
        "    Generates a normal distribution with outliers.\n",
        "\n",
        "    Args:\n",
        "        mean (float): The mean of the normal distribution.\n",
        "        std_dev (float): The standard deviation of the normal distribution.\n",
        "        size (int): The number of samples to generate.\n",
        "        outlier_percentage (float): The percentage of outliers to introduce (between 0 and 1).\n",
        "        outlier_magnitude (float): The magnitude by which outliers deviate from the mean.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The generated data with outliers.\n",
        "    \"\"\"\n",
        "    data = np.random.normal(mean, std_dev, size)\n",
        "    num_outliers = int(size * outlier_percentage)\n",
        "    outlier_indices = np.random.choice(size, num_outliers, replace=False)\n",
        "    for index in outlier_indices:\n",
        "        if np.random.rand() < 0.5:\n",
        "            data[index] += outlier_magnitude\n",
        "        else:\n",
        "            data[index] -= outlier_magnitude\n",
        "\n",
        "    return data\n",
        "\n",
        "def gen_standard_scaling(mean=50, std_dev=10, size=len(df), scale_factor=1000):\n",
        "  \"\"\"\n",
        "  Generates data with a specified mean and standard deviation, then scales it by a factor to create a distribution needing scaling.\n",
        "\n",
        "  Args:\n",
        "      mean (float): The mean of the original distribution.\n",
        "      std_dev (float): The standard deviation of the original distribution.\n",
        "      size (int): The number of samples to generate.\n",
        "      scale_factor (float): The factor by which to scale the original distribution.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: The generated data needing scaling.\n",
        "  \"\"\"\n",
        "  original_data = np.random.normal(mean, std_dev, size)\n",
        "  return original_data * scale_factor\n",
        "\n",
        "def gen_minmax_scaling(mean=50, std_dev=10, size=len(df), range_factor=10):\n",
        "  \"\"\"\n",
        "  Generates data with a specified mean and standard deviation, then scales and shifts it to create a distribution needing MinMax scaling.\n",
        "\n",
        "  Args:\n",
        "      mean (float): The mean of the original distribution.\n",
        "      std_dev (float): The standard deviation of the original distribution.\n",
        "      size (int): The number of samples to generate.\n",
        "      range_factor (float): The factor to expand the range of the original distribution.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: The generated data needing scaling.\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate the original data\n",
        "  original_data = np.random.normal(mean, std_dev, size)\n",
        "\n",
        "  # Expand the range of the data\n",
        "  min_val = np.min(original_data)\n",
        "  max_val = np.max(original_data)\n",
        "  return (original_data - min_val) * range_factor + min_val\n",
        "\n",
        "def random_choice_data(choices, size):\n",
        "  \"\"\"\n",
        "  Generates a new series correlated with the given series based on the specified correlation coefficient,\n",
        "  using rank correlation to ensure the generated series follows a uniform distribution.\n",
        "\n",
        "  Args:\n",
        "      original_series (numpy.ndarray or list): The original series.\n",
        "      correlation_coefficient (float): The desired correlation coefficient between the original and generated series.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: The generated correlated series with a uniform distribution.\n",
        "  \"\"\"\n",
        "  return np.random.choice(choices, size=size)\n"
      ],
      "metadata": {
        "id": "JWOa9CmWQQ3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical variables with little correlation to target\n",
        "df['random choice 2'] = random_choice_data(['Rand Choice 1', 'Rand Choice 2'], size=len(df))\n",
        "df['random choice 4'] = random_choice_data(['North', 'South', 'East', 'West'], size=len(df))\n",
        "df['random choice 7'] = random_choice_data(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], size=len(df))\n",
        "\n",
        "# categorical random choices with random # of labels\n",
        "num_labels = np.random.randint(3, 5)\n",
        "df[f'random label num {num_labels}'] = random_choice_data([f'label num lo {i}' for i in range(1, num_labels + 1)], size=len(df))\n",
        "\n",
        "num_labels = np.random.randint(10, 15)\n",
        "df[f'random label num {num_labels}'] = random_choice_data([f'label num hi {i}' for i in range(1, num_labels + 1)], size=len(df))"
      ],
      "metadata": {
        "id": "OwAolbPsxTxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical variables correlated with target\n",
        "df['pd qcut1'] = pd.qcut(df['target'], 2, labels=['Low', 'High']) # bi label\n",
        "df['pd qcut2'] = pd.qcut(df['target'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4']) # 4 labels\n",
        "\n",
        "quantiles = [0, 0.1, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "df['pd qcut3'] = pd.qcut(df['target'], quantiles, labels=['G1', 'G2', 'G3', 'G4', 'G5', 'G6']) # 6 labels"
      ],
      "metadata": {
        "id": "oiR1SK-QR7Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate four numerical normally distributed continuous features that have a correlation greater than absolute value of .5 with each other\n",
        "# gen_multivariate_normal_data(mean=[0, 0], cov=[[1, 0], [0, 1]], size=len(df))\n",
        "df['multicollinearity 1'], df['multicollinearity 2'] = gen_multivariate_normal_data(mean=[0, 0], cov=[[1, .7], [.7, 1]], size=len(df))\n",
        "df['multicollinearity 3'], df['multicollinearity 4'] = gen_multivariate_normal_data(mean=[0, 0], cov=[[1, .9], [.9, 1]], size=len(df))"
      ],
      "metadata": {
        "id": "Q6QTLoR2uPkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate two normally distributed features that are correlated with the target\n",
        "# gen_correlated_normal_series(original_series, target_correlation, size=len(df))\n",
        "df['correlated w target 1'] = gen_correlated_normal_series(df['target'], target_correlation=.5)\n",
        "df['correlated w target 2'] = gen_correlated_normal_series(df['target'], target_correlation=.7)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "EyUCReGZlSSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate two uniformly distributed features that are correlated with the target\n",
        "# gen_correlated_uniform_series(original_series, correlation_coefficient=0, size=len(df))\n",
        "df['uniform corr 1'] = gen_correlated_uniform_series(df['target'])\n",
        "df['uniform corr 2'] = gen_correlated_uniform_series(df['target'])"
      ],
      "metadata": {
        "id": "nEQxB18Juh8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two features that are duplicates of other features\n",
        "df['duplicate_1'] = df['informative_1']\n",
        "df['duplicate_2'] = df['informative_2']"
      ],
      "metadata": {
        "id": "Vx1_vq2FIxan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two numerical features with outliers\n",
        "df['outliers 1'] = gen_outliers(mean=0, std_dev=1, size=len(df), outlier_percentage=0.1, outlier_magnitude=3)\n",
        "df['outliers 2'] = gen_outliers(mean=3, std_dev=2, size=len(df), outlier_percentage=0.2, outlier_magnitude=2)"
      ],
      "metadata": {
        "id": "wJGVW0OMJKCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a numerical feature that needs standard scaling\n",
        "df['standard scaling'] = gen_standard_scaling()"
      ],
      "metadata": {
        "id": "oNXKVbukJUX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a numerical feature that needs min max scaling\n",
        "df['min max scaling'] = gen_minmax_scaling()"
      ],
      "metadata": {
        "id": "YffL2FIjJZBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate null values\n",
        "for col in df.drop(['class', 'informative_1', 'informative_2', 'target', 'duplicate_1', 'duplicate_2'], axis=1).columns:\n",
        "    df[col] = gen_null(df[col], np.random.choice([0, 5, 10, 20, 30, 50], size=1).item())"
      ],
      "metadata": {
        "id": "BSB0O7puJmJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two features that have constant values\n",
        "df['constant_1'] = 'constant_value'\n",
        "df['constant_2'] = 'constant_value'"
      ],
      "metadata": {
        "id": "2sfkZqmYIjDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two features with semi constant values\n",
        "df['semi_constant_1'] = gen_quasi_constants('q_const', variation_percentage = 1)\n",
        "df['semi_constant_2'] = gen_quasi_constants('q_const', variation_percentage = 1)"
      ],
      "metadata": {
        "id": "gWSpRTvfIp1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())  # check progress"
      ],
      "metadata": {
        "id": "nQ8w6T9WYLf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add duplicates\n",
        "dupes = df.loc[0:9]\n",
        "df = pd.concat([df, dupes], axis=0)\n",
        "\n",
        "# shuffle all columns\n",
        "# df = df.sample(frac=1).reset_index(drop=True)\n",
        "# df = df.sample(frac=1, axis=1)\n",
        "\n",
        "# shuffle selected columns\n",
        "demographic_columns = demographics.columns\n",
        "remaining_columns = [col for col in df.columns if col not in demographic_columns]\n",
        "# print(remaining_columns)\n",
        "np.random.shuffle(remaining_columns)\n",
        "\n",
        "# Reassemble the DataFrame with the shuffled columns\n",
        "df = df[list(demographic_columns) + list(remaining_columns)]\n",
        "\n",
        "# move target to the end of the list\n",
        "class_var = 'class'\n",
        "df = df[df.drop('class', axis=1).columns.tolist() + [class_var]]\n",
        "\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CiFb0YwCXupw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data science fiction ii pt 1.csv', index=False)"
      ],
      "metadata": {
        "id": "Legtg7PmvNor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Exploratory Data Analysis (EDA)\n",
        "\n",
        "Exploratory data analysis (EDA) is a data analysis method that helps data scientists understand their data and identify patterns. It's often used as the first step in data analysis."
      ],
      "metadata": {
        "id": "Qkomq9RKTv9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "nDvqqYpHshxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data science fiction ii pt 1.csv')\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KwzNuey3sjtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Var Types"
      ],
      "metadata": {
        "id": "nMZvDVF51DcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_numerical = df.select_dtypes(include='number').columns\n",
        "df_object = df.select_dtypes(include=['object']).columns\n",
        "df_discreet = df.select_dtypes(include=['category']).columns\n",
        "df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n",
        "print(df_numerical)\n",
        "print(df_object)\n",
        "print(df_discreet)\n",
        "print(df_categorical_features)"
      ],
      "metadata": {
        "id": "pVRA3GCN73QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation"
      ],
      "metadata": {
        "id": "UlDP5W6MhwA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code along\n",
        "df[df_numerical].corr().round(2)"
      ],
      "metadata": {
        "id": "-WOkym8CmK9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show correlation between the features\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# correlation matrix\n",
        "sns.set(style=\"white\")\n",
        "\n",
        "# compute the correlation matrix\n",
        "corr = df[df_numerical].corr().round(1)\n",
        "\n",
        "# generate a mask for the upper triangle\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# set up the matplotlib figure\n",
        "# f, ax = plt.subplots()\n",
        "f = plt.figure(figsize=(12, 12))\n",
        "\n",
        "# generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "4MiwGUpVd8AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the correlation matrix\n",
        "corr_matrix = df[df_numerical].corr()\n",
        "\n",
        "# Create a mask for the upper triangle (to avoid duplicates)\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "# Convert the correlation matrix to a long format\n",
        "corr_df = corr_matrix.stack().reset_index()\n",
        "corr_df.columns = ['feature1', 'feature2', 'correlation']\n",
        "\n",
        "# Filter for correlations above a certain threshold (e.g., 0.7)\n",
        "high_corr_df = corr_df[(abs(corr_df['correlation']) > 0.7) & (corr_df['feature1'] != corr_df['feature2'])]\n",
        "\n",
        "# Sort by absolute correlation in descending order\n",
        "high_corr_df = high_corr_df.sort_values(by='correlation', ascending=False, key=abs)\n",
        "\n",
        "# Print the top correlated features\n",
        "# print(high_corr_df['feature1'].to_list()[4:10])\n",
        "print(high_corr_df)\n",
        "\n",
        "# Create a variable to pickle\n",
        "data = {'correlation scores': high_corr_df}"
      ],
      "metadata": {
        "id": "rK9vXDx28Sxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for vif\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# handle null values (using mean imputation for simplicity)\n",
        "x_copy = df.drop('class', axis=1)._get_numeric_data()\n",
        "x_copy.fillna(x_copy.mean(), inplace=True)\n",
        "\n",
        "print(max([variance_inflation_factor(x_copy, i) for i in range(x_copy.shape[1])]))\n",
        "\n",
        "# calculate VIF\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = x_copy.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(x_copy, i) for i in range(x_copy.shape[1])]\n",
        "print(vif)"
      ],
      "metadata": {
        "id": "phR0PhZWrOFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multicollinearity\n",
        "\n",
        "* We want high correlation with target\n",
        "* We don't want high correlation between features\n",
        "* Drop correlated features\n",
        "* Combine correlated features"
      ],
      "metadata": {
        "id": "O_iCw05QUmzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate dropping features with high vif\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "removed=[]\n",
        "x_copy1 = x_copy.copy()\n",
        "max_vif = thresh = 10\n",
        "while max_vif >= thresh:\n",
        "  my_list = [variance_inflation_factor(x_copy1, i) for i in range(x_copy1.shape[1])]\n",
        "  max_vif = max(my_list)\n",
        "  if max_vif > thresh:\n",
        "    max_index = my_list.index(max_vif)\n",
        "    removed.append(x_copy1.columns[max_index])\n",
        "    print(x_copy1.columns[max_index], variance_inflation_factor(x_copy1, max_index))\n",
        "    x_copy1.drop(x_copy1.columns[max_index], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Calculate VIF\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = x_copy1.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(x_copy1, i) for i in range(x_copy1.shape[1])]\n",
        "print(vif)\n",
        "\n",
        "# Create a variable to pickle\n",
        "data = {'vif': vif}\n"
      ],
      "metadata": {
        "id": "QswUrQZ2amYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(removed)"
      ],
      "metadata": {
        "id": "TJOdfdcPfTBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers"
      ],
      "metadata": {
        "id": "qzuW_Dp-h0hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code along\n",
        "df.boxplot(column=['outliers 1']);"
      ],
      "metadata": {
        "id": "Wvt9mNhQvX2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code along\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "ZwA0vezIyGsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def count_outliers_iqr(df, column):\n",
        "    \"\"\"Counts the number of outliers in a DataFrame column using the IQR method.\"\"\"\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    return len(outliers)\n",
        "\n",
        "def detect_and_print_numerical_outliers_iqr(df):\n",
        "    \"\"\"\n",
        "    Iterates through numerical columns in a DataFrame and prints the\n",
        "    variable name with the number of outliers based on the IQR method.\n",
        "    \"\"\"\n",
        "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
        "    for col in numerical_cols:\n",
        "        num_outliers = count_outliers_iqr(df, col)\n",
        "        print(f\"Variable: {col}, Number of outliers (IQR): {num_outliers}\")\n",
        "\n",
        "\n",
        "detect_and_print_numerical_outliers_iqr(df[df_numerical])"
      ],
      "metadata": {
        "id": "0g5SjPuDxKrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data science fiction ii pt 2.csv', index=False)"
      ],
      "metadata": {
        "id": "ZboKhr4UiOkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Data Prep\n",
        "\n",
        "https://www.udemy.com/course/feature-engineering-for-machine-learning\n",
        "\n",
        "* Types and characteristics of data\n",
        "* Missing data imputation\n",
        "* Categorical encoding\n",
        "* Variable transformation\n",
        "* Discretization\n",
        "* Outliers\n",
        "* Datetime\n",
        "* Scaling\n",
        "* Feature creation"
      ],
      "metadata": {
        "id": "p43IcKZ2H4TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "nWWZXj_t7wkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data science fiction ii pt 2.csv')\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GER6wToI7wkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean the Data"
      ],
      "metadata": {
        "id": "3ndBWnDAinv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# constants"
      ],
      "metadata": {
        "id": "VVb32G3MTSVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quasi constants"
      ],
      "metadata": {
        "id": "xzA7hw8-igFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# duplicate rows"
      ],
      "metadata": {
        "id": "-a0gef4YijmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# duplicate features"
      ],
      "metadata": {
        "id": "9KyxklSLpfmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# missing data"
      ],
      "metadata": {
        "id": "j73nQK6OiygU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling"
      ],
      "metadata": {
        "id": "s7Sf-awwpZAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outliers"
      ],
      "metadata": {
        "id": "7MLtqwF0pjMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify Variable Types for Encoding"
      ],
      "metadata": {
        "id": "gXOrOSfDpQje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_numerical = df.select_dtypes(include='number').columns\n",
        "# df_object = df.select_dtypes(include=['object']).columns\n",
        "# df_discreet = df.select_dtypes(include=['category']).columns\n",
        "# df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n",
        "# print(df_numerical)\n",
        "# print(df_object)\n",
        "# print(df_discreet)\n",
        "# print(df_categorical_features)"
      ],
      "metadata": {
        "id": "_qoBXoPjRUEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 - Feature Engineering"
      ],
      "metadata": {
        "id": "EI54gfICj3Tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derived Variables"
      ],
      "metadata": {
        "id": "3ZaEp01sx2-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# derived variables coding"
      ],
      "metadata": {
        "id": "l4TygZ-hx6PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Encoding"
      ],
      "metadata": {
        "id": "43tZx7khof9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical encoding"
      ],
      "metadata": {
        "id": "hjKC1_6U9Vvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that everything is numerical"
      ],
      "metadata": {
        "id": "xL7aAxnXpFMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv('data science fiction ii pt 3.csv', index=False)"
      ],
      "metadata": {
        "id": "495vI4Zmfyym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5 - Feature Selection"
      ],
      "metadata": {
        "id": "3zeC15Wmj7co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # get data\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('data science fiction ii pt 3.csv')\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "P3PeZvm2g5Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Split\n",
        "\n",
        "random_state was initialized in the first code cell"
      ],
      "metadata": {
        "id": "Oc6VU37OoCyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop('class', axis=1), df['class'], test_size=0.3, random_state=random_state)\n",
        "# X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "RcrWm0I4oEy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mutual Information"
      ],
      "metadata": {
        "id": "rb9tIdNp8J8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # mutual information\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# mi = mutual_info_classif(X_train, y_train)\n",
        "# mi = pd.Series(mi)\n",
        "# mi.index = X_train.columns\n",
        "# mi.sort_values(ascending=False).plot.bar()\n",
        "# plt.ylabel('Mutual Information');"
      ],
      "metadata": {
        "id": "SKFraXeRtr44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mi_keepers = mi.sort_values(ascending=False).index[:5]\n",
        "# print(mi_keepers)"
      ],
      "metadata": {
        "id": "f-UYVaIuwb6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SelectKBest"
      ],
      "metadata": {
        "id": "vRo1hgSE8Qy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # SelectKBest\n",
        "# from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
        "\n",
        "# selector = SelectKBest(f_classif, k=5) # Select the top 5 features\n",
        "# X_new = selector.fit(X_train, y_train)\n",
        "\n",
        "# kb_keepers = X_train.columns.values[selector.get_support()]\n",
        "# print(kb_keepers)"
      ],
      "metadata": {
        "id": "PExIjwjzuw_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select From Model"
      ],
      "metadata": {
        "id": "fjR1U4GM8Tyw"
      }
    },
    {
      "source": [
        "# # Select from model\n",
        "# import numpy as np\n",
        "# from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "# from sklearn.feature_selection import SelectFromModel\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(X_train)\n",
        "# X_scaled = scaler.transform(X_train)\n",
        "\n",
        "# selections = SelectFromModel(estimator=LogisticRegression()).fit(X_scaled, y_train)\n",
        "# mt_keepers = X_train.columns.values[selections.get_support()]\n",
        "# print(mt_keepers)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DkRDmJWchp0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursive Feature Elmination"
      ],
      "metadata": {
        "id": "Hum_dCtv8XfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_selection import RFE\n",
        "# from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# estimator = LogisticRegression()\n",
        "# selector = RFE(estimator, n_features_to_select=5) # Select the top 5 features\n",
        "# X_new = selector.fit_transform(X_scaled, y_train)\n",
        "# rf_keepers = X_train.columns.values[selections.get_support()]\n",
        "# print(rf_keepers)"
      ],
      "metadata": {
        "id": "epn6N1WTvyNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Importance\n"
      ],
      "metadata": {
        "id": "wPvl0kTMe3e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # random forest importance\n",
        "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "# from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# selects = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=random_state), max_features=4)\n",
        "# selects.fit(X_train, y_train)\n",
        "# rfi = X_train.columns[(selects.get_support())]\n",
        "# rfi.tolist()"
      ],
      "metadata": {
        "id": "VaMYAQmnfGRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review Previous Variables\n",
        "\n",
        "* Correlated features\n",
        "* VIF\n",
        "* Outliers"
      ],
      "metadata": {
        "id": "CQ7VwY3ABUsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # make a list of features you have selected and use it as a filter\n",
        "# features_to_model = []"
      ],
      "metadata": {
        "id": "VFzff6aCfefd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train[features_to_model]\n",
        "# X_test = X_test[features_to_model]"
      ],
      "metadata": {
        "id": "2PpQyIprnD6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 6 - Data Modeling and Evaluation"
      ],
      "metadata": {
        "id": "lJQ7-Uvaj-eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "JzLn_P9TsL8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # model, predict, evaluate, and plot\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# model = LogisticRegression(solver='liblinear', random_state=random_state)\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "\n",
        "# train_accuracy = model.score(X_train, y_train)\n",
        "# print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "# test_accuracy = model.score(X_test, y_test)\n",
        "# print(f\"Testing Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "4dtvvMWZsOVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "P3s_9zNAkBvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
        "# print('accuracy:', accuracy_score(y_test, predictions))\n",
        "# # compare with other metrics"
      ],
      "metadata": {
        "id": "u2_ZmOlSu3v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The order of `y_test` (true labels) and `predictions` (predicted labels) matters significantly for `confusion_matrix` and `classification_report` in scikit-learn.**\n",
        "\n",
        "However, for **`accuracy_score`**, the order does **not** matter because it simply calculates the proportion of correctly classified instances, regardless of which is considered the \"true\" and which is the \"predicted\" set in the function call.\n",
        "\n",
        "Let's break down why the order is crucial for `confusion_matrix` and `classification_report`:\n",
        "\n",
        "**1. `confusion_matrix(y_test, predictions)`:**\n",
        "\n",
        "* The first argument (`y_test`) should always be the **true labels** (the actual values).\n",
        "* The second argument (`predictions`) should always be the **predicted labels** (the values your model has outputted).\n",
        "\n",
        "The output of `confusion_matrix` is a 2x2 (for binary classification) or NxN (for multi-class classification) array where:\n",
        "\n",
        "* The rows correspond to the **true classes**.\n",
        "* The columns correspond to the **predicted classes**.\n",
        "\n",
        "Therefore, `confusion_matrix(y_test, predictions)` will produce a matrix where:\n",
        "\n",
        "* `TN` (True Negative) is the count of instances where the true label was negative and the prediction was negative.\n",
        "* `FP` (False Positive) is the count of instances where the true label was negative and the prediction was positive.\n",
        "* `FN` (False Negative) is the count of instances where the true label was positive and the prediction was negative.\n",
        "* `TP` (True Positive) is the count of instances where the true label was positive and the prediction was positive.\n",
        "\n",
        "If you reverse the order and do `confusion_matrix(predictions, y_test)`, the rows and columns would effectively be swapped in terms of what they represent (true vs. predicted), leading to an incorrect interpretation of `TN`, `FP`, `FN`, and `TP`.\n",
        "\n",
        "**2. `classification_report(y_test, predictions)`:**\n",
        "\n",
        "* Similar to `confusion_matrix`, the first argument (`y_test`) must be the **true labels**, and the second argument (`predictions`) must be the **predicted labels**.\n",
        "\n",
        "The `classification_report` provides a text summary of the precision, recall, F1-score, and support for each class. These metrics are calculated based on the true positives, true negatives, false positives, and false negatives, which are directly derived from the correct alignment of true and predicted labels. Swapping the order would lead to incorrect calculations of these metrics for each class.\n",
        "\n",
        "**3. `accuracy_score(y_test, predictions)`:**\n",
        "\n",
        "* For `accuracy_score`, the order does **not** matter. Accuracy is calculated as the number of correct predictions divided by the total number of predictions:\n",
        "\n",
        "    `Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)`\n",
        "\n",
        "    Whether you compare `y_test` against `predictions` or `predictions` against `y_test`, the set of correctly matched instances will be the same, and the total number of instances remains the same. Therefore, the accuracy score will be identical regardless of the order of the arguments.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "* **`confusion_matrix`:** **Order matters.** Always use `confusion_matrix(y_test, predictions)`.\n",
        "* **`classification_report`:** **Order matters.** Always use `classification_report(y_test, predictions)`.\n",
        "* **`accuracy_score`:** **Order does not matter.** `accuracy_score(y_test, predictions)` will yield the same result as `accuracy_score(predictions, y_test)`.\n",
        "\n",
        "It's crucial to maintain the correct order of true labels and predicted labels when using `confusion_matrix` and `classification_report` to ensure accurate evaluation of your classification model."
      ],
      "metadata": {
        "id": "V-TKDHexnjMJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11e9c355"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "* tn = pred 0 actual 0\n",
        "* fp = pred 1 actual 0\n",
        "* fn = pred 0 actual 1\n",
        "* tp = pred 1 actual 1\n",
        "* acc(uracy) = $\\frac{tn + tp}{total}$\n",
        "* error = $\\frac{fp + fn}{total}$\n",
        "* prev(alence) = $\\frac{fn + tp}{total}$\n",
        "* queue = $\\frac{fp + tp}{total}$\n",
        "* tpr = $\\frac{tp}{tp + fn}$\n",
        "    * true positive rate\n",
        "    * recall\n",
        "    * sensitivity\n",
        "    * prob of detection\n",
        "    * 1 - fnr\n",
        "* fnr = $\\frac{fn}{tp + fn}$\n",
        "    * false negative rate\n",
        "    * type II error\n",
        "    * 1 - tpr\n",
        "* tnr = $\\frac{tn}{tn + fp}$\n",
        "    * true negative rate\n",
        "    * specificity\n",
        "    * 1 - fpr\n",
        "* fpr = $\\frac{fp}{tn + fp}$\n",
        "    * false positive rate\n",
        "    * type I error\n",
        "    * fall out\n",
        "    * prob of false claim\n",
        "    * 1 - tnr\n",
        "* ppv = $\\frac{tp}{tp + fp}$\n",
        "    * positive predicted value\n",
        "    * precision\n",
        "    * 1 - fdr\n",
        "* fdr = $\\frac{fp}{tp + fp}$\n",
        "    * false discovery rate\n",
        "    * 1 - ppv\n",
        "* npv = $\\frac{tn}{tn + fn}$\n",
        "    * negative predicted value\n",
        "    * 1 - for\n",
        "* for = $\\frac{fn}{tn + fn}$\n",
        "    * false omission rate\n",
        "    * 1 - npv\n",
        "* liklihood ratio+ (lr+) = $\\frac{tpr}{fpr}$\n",
        "    * roc\n",
        "* liklihood ratio- (lr-) = $\\frac{fnr}{tnr}$\n",
        "* diagnostic odds ratio = $\\frac{lr+}{lr-}$\n",
        "* f1 score = 2 * $\\frac{precision-recall}{precision+recall}$\n",
        "* Youden's J = sensitivity + specificity - 1 = tpr - fpr\n",
        "* Matthew's Correlation Coefficient = $\\frac{(tp*tn)-(fp*tp)}{\\sqrt{(tp+fp)(tp+fn)(tn+fp)(tn+fn)}}$\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "YMQq51GE5XJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(confusion_matrix(y_test, predictions))"
      ],
      "metadata": {
        "id": "kUSZa-4N5cgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "Please explain what the Confusion Matrix is telling you"
      ],
      "metadata": {
        "id": "B_Ad4Q6FodI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision Recall"
      ],
      "metadata": {
        "id": "uaRRL0lf5Zv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "bqdrP2BE5dEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "Please explain what precision and recall are telling you in the classification report"
      ],
      "metadata": {
        "id": "AUxqdti9orI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias Variance"
      ],
      "metadata": {
        "id": "Xa9kbxryvx4d"
      }
    },
    {
      "source": [
        "# from mlxtend.evaluate import bias_variance_decomp\n",
        "\n",
        "# avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
        "#     model,\n",
        "#     X_train.values, # Convert X_train to NumPy array\n",
        "#     y_train.values, # Convert y_train to NumPy array\n",
        "#     X_test.values, # Convert X_test to NumPy array\n",
        "#     y_test.values, # Convert y_test to NumPy array\n",
        "#     loss='0-1_loss',\n",
        "#     random_seed=random_state)\n",
        "\n",
        "# print('Average expected loss: %.3f' % avg_expected_loss)\n",
        "# print('Average bias: %.3f' % avg_bias)\n",
        "# print('Average variance: %.3f' % avg_var)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QXvTKg1egT9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "Please explain how to interpret bias variance and what it means to your model"
      ],
      "metadata": {
        "id": "Hw2tjvt1o1D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coming Soon - Making Predictions and Gradio"
      ],
      "metadata": {
        "id": "mjHdMmRDl-2Y"
      }
    }
  ]
}