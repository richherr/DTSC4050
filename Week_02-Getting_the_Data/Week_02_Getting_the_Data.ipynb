{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC4050/blob/main/Week_02-Getting_the_Data/Week_02_Getting_the_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c9a5d1",
      "metadata": {
        "id": "41c9a5d1"
      },
      "source": [
        "# Week 02 - Getting the Data\n",
        "\n",
        "Your Name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC4050 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit your name\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link"
      ],
      "metadata": {
        "id": "tv86CANY3_Xy"
      },
      "id": "tv86CANY3_Xy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some Readings\n",
        "\n",
        "* Data Science Interviews - https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html\n",
        "* How to Use Google Colab for Python - https://www.jcchouinard.com/google-colab-with-python/\n",
        "* A Day in the Life of a Data Scientist - https://towardsdatascience.com/a-day-in-the-life-of-a-data-scientist-938d917370b9\n",
        "* https://towardsdatascience.com/the-inequality-in-the-data-science-industry-937992032851"
      ],
      "metadata": {
        "id": "74WqHLAqLXiY"
      },
      "id": "74WqHLAqLXiY"
    },
    {
      "cell_type": "markdown",
      "id": "87f8c9ee",
      "metadata": {
        "id": "87f8c9ee"
      },
      "source": [
        "## Data Science Process\n",
        "\n",
        "https://www.springboard.com/blog/wp-content/uploads/2022/05/data-science-life-cycle.png"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Life Cycle\n",
        "\n",
        "The data lifecycle is like a journey that data takes from its creation to its retirement. It's a continuous process with several stages, each with its own purpose and challenges. Here's a breakdown of the typical stages:\n",
        "\n",
        "**1. Data Creation/Capture**\n",
        "\n",
        "* This is where data is born! It can be generated from various sources:\n",
        "    * **Directly:** User input (forms, surveys), sensors (IoT devices), transactions (online purchases).\n",
        "    * **Indirectly:**  Social media activity, web scraping, machine-generated data (logs).\n",
        "\n",
        "**2. Data Collection**\n",
        "\n",
        "* Gathering the raw data from different sources and bringing it together.\n",
        "* This might involve:\n",
        "    * APIs to connect to databases or online services\n",
        "    * Sensors transmitting data wirelessly\n",
        "    * Manual data entry\n",
        "\n",
        "**3. Data Processing**\n",
        "\n",
        "* Cleaning, transforming, and preparing the data for analysis.\n",
        "* This is often the most time-consuming step and can involve:\n",
        "    * Removing errors and inconsistencies\n",
        "    * Handling missing values\n",
        "    * Converting data types\n",
        "    * Aggregating or summarizing data\n",
        "\n",
        "**4. Data Storage**\n",
        "\n",
        "* Storing the processed data in a secure and accessible way.\n",
        "* Storage options include:\n",
        "    * Databases (relational, NoSQL)\n",
        "    * Data warehouses\n",
        "    * Cloud storage (AWS S3, Google Cloud Storage)\n",
        "\n",
        "**5. Data Analysis**\n",
        "\n",
        "* Extracting insights and knowledge from the data.\n",
        "* This involves:\n",
        "    * Statistical analysis\n",
        "    * Machine learning\n",
        "    * Data visualization\n",
        "\n",
        "**6. Data Visualization**\n",
        "\n",
        "* Representing the data and insights in a visual format (charts, graphs, dashboards).\n",
        "* Makes it easier to understand trends, patterns, and outliers.\n",
        "\n",
        "**7. Data Interpretation**\n",
        "\n",
        "* Drawing conclusions and making decisions based on the data analysis and visualization.\n",
        "* This is where data turns into actionable knowledge.\n",
        "\n",
        "**8. Data Archiving/Deletion**\n",
        "\n",
        "* Deciding what to do with data that's no longer actively used.\n",
        "* Options include:\n",
        "    * Archiving for long-term storage\n",
        "    * Deleting data securely to comply with regulations or free up space\n",
        "\n",
        "**Why is the data lifecycle important?**\n",
        "\n",
        "* **Efficiency:**  It provides a structured approach to managing data, ensuring that it's used effectively throughout its lifespan.\n",
        "* **Data quality:** Helps maintain data accuracy, consistency, and completeness.\n",
        "* **Security and compliance:**  Ensures data is stored and handled securely, meeting regulatory requirements.\n",
        "* **Decision-making:** Provides a framework for turning raw data into valuable insights that drive informed decisions.\n",
        "\n",
        "Understanding the data lifecycle is crucial for anyone working with data, from data scientists and analysts to business leaders and policymakers. It helps organizations maximize the value of their data and make better decisions.\n"
      ],
      "metadata": {
        "id": "czRoYYVHCKFz"
      },
      "id": "czRoYYVHCKFz"
    },
    {
      "cell_type": "markdown",
      "id": "74b4d70e",
      "metadata": {
        "id": "74b4d70e"
      },
      "source": [
        "### Why Statistics\n",
        "\n",
        "Naked Statistics\n",
        "\n",
        "* To summarize huge quantities of data\n",
        "* To make better decisions\n",
        "* Answer important social questions\n",
        "* Recognize patterns\n",
        "* Evaluate effectiveness\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Data\n",
        "\n",
        "According to Wikipedia (2023):\n",
        "\n",
        ">  In the pursuit of knowledge, data is a collection of discrete values that convey information, describing quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted. A datum is an individual value in a collection of data. Data is usually organized into structures such as tables that provide additional context and meaning, and which may themselves be used as data in larger structures (para 1).\n",
        "\n",
        "Data. (2023, January 23). In *Wikipedia*. https://en.wikipedia.org/wiki/Data\n",
        "\n",
        "Data is dumb. Data needs context.\n",
        "\n",
        "* GPA, used a lot but doesn't count the difficulty of the courses taken\n",
        "\n",
        "* It was found that workers who take frequent breaks died earlier than others\n",
        "    * That's because they were out smoking\n",
        "\n",
        "### Data Needs to be Reliable\n",
        "\n",
        "* Consistency of a measure (whether the results can be reproduced under the same conditions)\n",
        "\n",
        "### Data Needs to be Valid\n",
        "\n",
        "* The accuracy of a measure (whether the results really do represent what they are supposed to measure)\n",
        "\n",
        "### Precision vs Accuracy\n",
        "\n",
        "* Precision: exactitude of describing something\n",
        "* Accuracy: how consistent to the truth\n",
        "* Where's the library? About 1.6 miles from here (precise but not to useful) vs. go down this street a couple of blocks till you pass a bank. It'll be to the right. If you pass the fire station, you've gone too far\n",
        "\n",
        "### Statistical Models for Data\n",
        "\n",
        "* A formal representation of relationships between variables we use to describe or explain something"
      ],
      "metadata": {
        "id": "G05A6eqOCkoV"
      },
      "id": "G05A6eqOCkoV"
    },
    {
      "cell_type": "markdown",
      "id": "0edbb01a",
      "metadata": {
        "id": "0edbb01a"
      },
      "source": [
        "### Data Starts with a Question\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  A research question is a question that a research project sets out to answer'. Choosing a research question is an essential element of both quantitative and qualitative research. Investigation will require data collection and analysis, and the methodology for this will vary widely. Good research questions seek to improve knowledge on an important topic, and are usually narrow and specific (para 1).\n",
        "\n",
        "Research Question. (2022, January 24). In *Wikipedia*. https://en.wikipedia.org/wiki/Research_question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e50a1e9",
      "metadata": {
        "id": "6e50a1e9"
      },
      "source": [
        "### Storytime: Jeff Seder\n",
        "\n",
        "Everybody Lies\n",
        "    \n",
        "* Jeff Seder was hired by an Egyptian Beer Magnate, named Ahmed Zayat, who was into horses and horse racing, as a consultant\n",
        "    * There was a sale of 152 one year old horses\n",
        "    * One of those horses, an average horse designated number 85, was owned by Zayat\n",
        "    * At the time Jeff Seder and his team were unable to recommend the purchase of any of the horses\n",
        "    * But was adamant about not selling No. 85 even though similar horses were going for 1 million dollars\n",
        "    * Three months later, Zayat named the horse American Pharoah\n",
        "    * 18 months later American Pharoah won the Triple Crown\n",
        "    \n",
        "* How did Seder know?\n",
        "    * Was it through pedigree? father, mother, grand parents, etc.\n",
        "    * Gaits (a manner of walk, trot, canter, and gallop)?\n",
        "    * Years of data collection resulted in frustration for Seder\n",
        "    * Measured the size of nostrils (produced a book on horse nostrils)\n",
        "    * EKGs\n",
        "    * Dug through horse poop\n",
        "    * Finally found that the size of the heart specifically the left ventricle and spleen mattered\n",
        "    \n",
        "* Data on American Pharoah:\n",
        "    * Height: 56 percentile\n",
        "    * Weight: 61 percentile\n",
        "    * Pedigree: 70 percentile\n",
        "    * Left Ventricle: 99.61 percentile\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Garbage In Garbage Out (GIGO)\n",
        "\n",
        "GI of GIGO\n",
        "\n",
        "* Wrong\n",
        "* Too Different\n",
        "* Too Similar\n",
        "* Missing\n",
        "* Terrible statistical assertions result from good statistical methods applied to bad samples\n",
        "\n",
        "### Good Data - Data Needs Three Things\n",
        "\n",
        "1. The sample is representative of the population\n",
        "    * One way to achieve this is through simple random sample\n",
        "    * Larger the better\n",
        "    * A representative sample opens possibilities from a hundred years of statistical methods\n",
        "    * Getting a good sample isn't easy\n",
        "2. Data should show comparison\n",
        "    * Differences between groups\n",
        "    * Does a method improve something\n",
        "    * In the movie Caddyshack, golf related comedy, Chevy Chase is asked how his scores compare with others\n",
        "        * I really don't keep score\n",
        "        * But then how do you compare with others\n",
        "        * By height\n",
        "3. Sometimes we collect data just because\n",
        "    * Like a crime scene, collect all the clues you can find\n",
        "    * Data are like clues\n"
      ],
      "metadata": {
        "id": "3TLHybIJ8SCR"
      },
      "id": "3TLHybIJ8SCR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Science for Business\n",
        "\n",
        "by Foster Provost and Tom Fawcett\n",
        "\n",
        "A critical skill in data science is the ability to decompose a problem into a known solution and work the solution\n",
        "\n",
        "* Data Analysis\n",
        "* Machine Learning\n",
        "* Artificial Intelligence\n",
        "* Math\n",
        "* Statistics\n",
        "* Computer Science\n",
        "* Information Technology\n",
        "* Business Knowledge\n",
        "* Classification: predicts yes or no\n",
        "* Regression: predicts how much\n",
        "* Profiling"
      ],
      "metadata": {
        "id": "v95shbr4IQdn"
      },
      "id": "v95shbr4IQdn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised vs Unsupervised Learning Review\n",
        "\n",
        "**Supervised learning** uses labeled data to train algorithms to predict outcomes or classify data. Think of it like learning with a teacher and answer key.\n",
        "\n",
        "**Unsupervised learning** uses unlabeled data to discover patterns, relationships, and structures in data. It's like exploring a new city without a map, letting the data guide you.\n"
      ],
      "metadata": {
        "id": "JsU4qcE65MWh"
      },
      "id": "JsU4qcE65MWh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature and Target Names\n",
        "<table align=\"left\">\n",
        "    <tr>\n",
        "        <th>Independent Variables (X)</th>\n",
        "        <th>Dependent Variable (y)</th>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td>Features</td>\n",
        "        <td>Target</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Regressors</td>\n",
        "        <td>Regressand</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Predictors</td>\n",
        "        <td>Response</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Observations</td>\n",
        "        <td>Outcomes</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Explanatory</td>\n",
        "        <td>Labels</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Input</td>\n",
        "        <td>Output</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>exog (exogenous)</td>\n",
        "        <td>endog (endogenous)</td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "zwcNIpgWE4ZE"
      },
      "id": "zwcNIpgWE4ZE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linearity vs Nonlinearity\n",
        "\n",
        "**Linearity**\n",
        "\n",
        "* **Relationship:**  A straight-line relationship between variables.  If you plot the data, it forms a line (or close to it).\n",
        "* **Models:** Linear regression, logistic regression (kind of), support vector machines (with linear kernels)\n",
        "* **Interpretation:**  Easy to understand.  A change in one variable leads to a proportional change in another.\n",
        "* **Examples:** Predicting house prices based on size, classifying emails as spam or not.\n",
        "\n",
        "**Nonlinearity**\n",
        "\n",
        "* **Relationship:** A curved or complex relationship between variables. The data doesn't form a straight line.\n",
        "* **Models:** Decision trees, random forests, neural networks\n",
        "* **Interpretation:** Can be more difficult to understand, but often captures more complex patterns in data.\n",
        "* **Examples:** Image recognition, natural language processing, predicting stock prices.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "Linear models are simpler and more interpretable, while nonlinear models can capture more complexity but are often harder to explain. The choice depends on the data and the problem you're trying to solve.\n"
      ],
      "metadata": {
        "id": "mY8IJ1Oq5_Ds"
      },
      "id": "mY8IJ1Oq5_Ds"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Causation vs Correlation\n",
        "\n",
        "**Correlation**\n",
        "\n",
        "* **What it is:**  A statistical relationship between two or more variables. They tend to change together.\n",
        "* **Types:** Positive (both increase together), negative (one increases, the other decreases), or zero (no relationship).\n",
        "* **Example:** Ice cream sales and crime rates are positively correlated (both tend to go up in the summer), but one doesn't cause the other.\n",
        "* **Techniques:**\n",
        "    * **Correlation coefficient:** Measures the strength and direction of the linear relationship between two variables (e.g., Pearson's correlation).\n",
        "    * **Scatter plots:** Visualize the relationship between two variables.\n",
        "\n",
        "**Causation**\n",
        "\n",
        "* **What it is:** One variable directly causes a change in another variable.\n",
        "* **Example:** Smoking causes an increased risk of lung cancer.\n",
        "* **Important:** Correlation does NOT imply causation! There might be a third, unseen factor (a \"confounder\") influencing both.\n",
        "* **Techniques:**\n",
        "    * **Randomized controlled trials:** The gold standard. Participants are randomly assigned to groups to isolate the effect of the variable being studied.\n",
        "    * **Causal inference:**  A branch of statistics that uses various techniques (e.g., instrumental variables, regression discontinuity) to try to identify causal relationships from observational data.\n",
        "\n",
        "**In data science:**\n",
        "\n",
        "* **Prediction:** Correlation is often enough for prediction (e.g., recommending products based on past purchases).\n",
        "* **Understanding:** Causation is crucial for understanding why things happen and making informed decisions (e.g., developing public health interventions).\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "Always be cautious about inferring causation from correlation alone. Dig deeper, consider confounding factors, and use appropriate techniques to explore potential causal relationships.\n"
      ],
      "metadata": {
        "id": "2b1E4_Ki6M0A"
      },
      "id": "2b1E4_Ki6M0A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "sAoSps7MDm81"
      },
      "id": "sAoSps7MDm81"
    },
    {
      "cell_type": "markdown",
      "id": "259d6803",
      "metadata": {
        "id": "259d6803"
      },
      "source": [
        "### Parameter vs Statistic\n",
        "\n",
        "* https://www.statisticshowto.com/statistics-basics/how-to-tell-the-difference-between-a-statistic-and-a-parameter/\n",
        "* Parameter describes population, statistic describes sample\n",
        "\n",
        "### Statistical Inference\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  the process of using data analysis to infer properties of an underlying distribution of probability (para 1).\n",
        "\n",
        "Statistical Inference. (2022, January 24). In *Wikipedia*. https://en.wikipedia.org/wiki/Statistical_inference.\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2022/03/an-overview-of-data-collection-data-sources-and-data-mining/\n",
        "\n",
        "* Mean and standard deviation of a sample should be similar to the mean and standard deviation of the population\n",
        "* Mean: where most of the observations lie with a distribution variance that is symmetrical\n",
        "* Standard Deviation: a way to measure how far observations are from the mean, on the x axis, in terms similar to the data it represents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling Types\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  Sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population [similar elements] to estimate characteristics of the whole population (para 1).\n",
        "\n",
        "Sampling. (2022, January 24). In *Wikipedia*.https://en.wikipedia.org/wiki/Sampling_(statistics)\n",
        "\n",
        "### Probabilistic Sampling\n",
        "\n",
        "* Simple random sampling: https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/simple-random-sample/\n",
        "* Systematic sampling: https://www.open.edu/openlearncreate/mod/oucontent/view.php?id=233&section=1.5.2\n",
        "* Stratified sampling: https://www.geeksforgeeks.org/stratified-sampling-in-pandas/\n",
        "* Clustering: https://www.voxco.com/blog/stratified-sampling-vs-cluster-sampling/\n",
        "\n",
        "### Non-Probabilistic Sampling Methods\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2019/09/data-scientists-guide-8-types-of-sampling-techniques/\n",
        "* Convenience sampling\n",
        "* Quota sampling\n",
        "* Snowball sampling\n"
      ],
      "metadata": {
        "id": "FAAleOeJ8gfW"
      },
      "id": "FAAleOeJ8gfW"
    },
    {
      "cell_type": "code",
      "source": [
        "# # numpy's random module\n",
        "# import numpy as np\n",
        "\n",
        "# print(np.random.rand())\n",
        "# print(np.random.randint(0, 9))"
      ],
      "metadata": {
        "id": "jJuaqE2dHx7S"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jJuaqE2dHx7S"
    },
    {
      "cell_type": "code",
      "source": [
        "# # create population\n",
        "# # https://stats.stackexchange.com/questions/560281/what-is-the-meaning-of-loc-and-scale-for-the-distributions-in-scipy-stats\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# mu, std, n = (0, 1, 10000) # unpacking a tuple\n",
        "# pop = np.random.normal(loc=mu, scale=std, size=n)\n",
        "# plt.hist(ndist);"
      ],
      "metadata": {
        "id": "3dJZH-zN_aL3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3dJZH-zN_aL3"
    },
    {
      "cell_type": "code",
      "source": [
        "# # random sample\n",
        "# np.random.choice(pop, replace=False, size=50)"
      ],
      "metadata": {
        "id": "1QcGSMjqBn1L"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1QcGSMjqBn1L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Population and Sample"
      ],
      "metadata": {
        "id": "VzdBmkg6ByW_"
      },
      "id": "VzdBmkg6ByW_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb0548c",
      "metadata": {
        "id": "5eb0548c"
      },
      "outputs": [],
      "source": [
        "# # normal distribution - what mean and standard deviation look like for the population\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# ndist = np.random.normal(loc=0.0, scale=1.0, size=50000)\n",
        "# plt.hist(ndist)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d27c59",
      "metadata": {
        "id": "a7d27c59"
      },
      "outputs": [],
      "source": [
        "# # what the mean and standard deviation looks like for the sample\n",
        "# plt.hist(np.random.choice(ndist, size=5000));"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precision vs Accuracy\n",
        "\n",
        "**Accuracy** is how close your measurements are to the true value. Think of it like hitting a bullseye on a dartboard.\n",
        "\n",
        "**Precision** is how close your measurements are to each other, regardless of whether they're close to the true value.  Imagine throwing darts that all land very close together, even if they're far from the bullseye.\n",
        "\n",
        "You can have high precision and low accuracy, high accuracy and low precision, both, or neither. The ideal is to have both high precision and high accuracy.\n",
        "\n",
        "I pull into a gas station and ask directions to a place to eat. One attendant says the restaurant is 1.5 miles north, another attendant gives streets directions.\n",
        "\n",
        "* **Attendant 1: \"1.5 miles north\"**\n",
        "\n",
        "    * **Precision:** This is **high precision**. The attendant gives you a specific distance (1.5 miles) and a specific direction (north). It's a very exact measurement.\n",
        "    * **Accuracy:** This is **unknown accuracy**.  While precise, it might be completely wrong! Maybe the restaurant is actually 1.8 miles north, or 1.5 miles northeast. Without further information, we don't know how accurate the directions are. And it doesn't help since I can't drive directly north.\n",
        "\n",
        "* **Attendant 2: Street directions**\n",
        "\n",
        "    * **Precision:** This is **lower precision**.  Street directions are less exact. They might involve turns, landmarks, and estimations (\"go down this road for a bit, then turn left at the big oak tree\").\n",
        "    * **Accuracy:** This is **potentially higher accuracy**. If the attendant knows the area well and gives you correct turns and landmarks, you're more likely to reach your destination, even if the directions are a bit vague.\n",
        "\n",
        "**Think of it like this:**\n",
        "\n",
        "* **Attendant 1** gave you coordinates on a map. Very precise, but if the map is wrong, you'll be lost.\n",
        "* **Attendant 2** drew you a rough sketch of the route. Less precise, but if the landmarks are correct, you'll get there.\n",
        "\n",
        "**In this scenario:**\n",
        "\n",
        "It's likely that the street directions (Attendant 2) are more helpful, even if they are less precise.  Knowing the general route and landmarks is usually more valuable than having a very specific distance and direction that might be inaccurate.\n",
        "\n",
        "This illustrates that in real-world situations, **accuracy is often more important than precision**, especially when getting directions!\n",
        "\n",
        "**However** In classification projects we will sometimes need a comprehensive evaluation of our model's performance in categorizing data and will need to introduce a new term called Recall.\n",
        "\n"
      ],
      "metadata": {
        "id": "X30JQ9_M7RQ4"
      },
      "id": "X30JQ9_M7RQ4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy and Precision of a Sample\n",
        "\n",
        "**Accuracy of a Sample**\n",
        "\n",
        "* **Parameter:** The true value of a characteristic in the population (e.g., the average height of all adults in a country).\n",
        "* **Statistic:** The estimate of that characteristic calculated from a sample (e.g., the average height of 1,000 adults in that country).\n",
        "* **Accuracy:** How close the *statistic* is to the true *parameter*.  A highly accurate sample will produce a statistic that's very close to the actual population value.\n",
        "\n",
        "**Precision of a Sample**\n",
        "\n",
        "* **Precision:**  How close the estimates from *different samples* are to each other. If you took multiple samples and calculated the statistic each time, high precision means those statistics would be very similar.\n",
        "* **Relationship to variability:** Precision is related to the variability or spread of the data in your sample. A low-variability sample (data points clustered closely together) will generally lead to higher precision.\n",
        "\n",
        "**How to assess accuracy and precision of a sample**\n",
        "\n",
        "* **Accuracy:**\n",
        "    * **Difficult to measure directly:**  You usually don't know the true population parameter (that's why you're sampling!).\n",
        "    * **Techniques:**\n",
        "        * **Confidence intervals:** Provide a range of values where the true parameter is likely to lie. A narrower confidence interval suggests higher accuracy.\n",
        "        * **Comparing to known benchmarks or previous studies:** If you have some external information about the population, you can compare your sample statistic to it.\n",
        "\n",
        "* **Precision:**\n",
        "    * **Measure:**  Calculate the standard error of the statistic. A smaller standard error indicates higher precision.\n",
        "    * **Factors that influence precision:**\n",
        "        * **Sample size:** Larger samples generally lead to higher precision.\n",
        "        * **Variability of the population:**  More variability in the population makes it harder to get precise estimates.\n",
        "        * **Sampling method:**  Proper random sampling techniques improve precision.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Imagine you're estimating the average income of people in a city.\n",
        "\n",
        "* **Accurate sample:** The average income you calculate from your sample is close to the true average income of the entire city.\n",
        "* **Precise sample:** If you took multiple samples, the average income calculated from each sample would be very similar to each other.\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "Accuracy and precision are important considerations when evaluating the quality of a sample and the reliability of the statistics derived from it. By understanding these concepts, you can better assess how well your sample represents the population and how much confidence you can have in your results.\n"
      ],
      "metadata": {
        "id": "rK_72UjJ-dA7"
      },
      "id": "rK_72UjJ-dA7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wash, Rinse, Repeat\n",
        "\n",
        "The need for multiple samples\n",
        "\n",
        "**Precision:**\n",
        "\n",
        "* **Multiple samples reveal variability:** If you take multiple samples from the same population, you'll likely get slightly different estimates each time. This variation between the sample estimates gives you a sense of the precision.\n",
        "    * Low variability between samples = high precision\n",
        "    * High variability between samples = low precision\n",
        "\n",
        "* **Example:** Imagine trying to estimate the average weight of apples from an orchard. If you take three samples and get average weights of 150g, 152g, and 151g, that suggests higher precision than if the averages were 140g, 165g, and 155g.\n",
        "\n",
        "**Accuracy:**\n",
        "\n",
        "* **Multiple samples help identify bias:**  If you consistently get estimates that are off in the same direction (e.g., always overestimating), it suggests there might be a bias in your sampling method.\n",
        "* **Averaging can improve accuracy:**  Taking the average of multiple sample estimates can often give you a more accurate estimate of the population parameter than any single sample.\n",
        "\n",
        "**Think of it like this:**\n",
        "\n",
        "Imagine you're throwing darts at a dartboard.\n",
        "\n",
        "* **One dart:** You can't really tell much about your accuracy or precision.\n",
        "* **Multiple darts:**\n",
        "    * **High accuracy, high precision:** All darts clustered tightly around the bullseye.\n",
        "    * **High accuracy, low precision:** Darts scattered around the bullseye, but their average position is close to the center.\n",
        "    * **Low accuracy, high precision:** Darts clustered tightly together, but far from the bullseye.\n",
        "    * **Low accuracy, low precision:** Darts scattered all over the dartboard.\n",
        "\n",
        "**In the context of sampling:**\n",
        "\n",
        "* Multiple samples are like multiple darts. They give you more information to assess both the accuracy (how close you are to the true population value) and the precision (how consistent your estimates are).\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "While a single sample can provide some information, multiple samples are essential for a more complete understanding of accuracy and precision in the context of sampling and statistical estimation.\n"
      ],
      "metadata": {
        "id": "1xojcohA-w1Q"
      },
      "id": "1xojcohA-w1Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Law of Large Numbers\n",
        "\n",
        "**Law of Large Numbers**\n",
        "\n",
        "* **What it is:**  A fundamental principle in probability and statistics stating that as the size of a sample increases, the sample average gets closer and closer to the true average of the entire population.\n",
        "* **In simpler terms:**  If you flip a coin a few times, you might get a lot of heads or tails in a row. But if you flip it thousands of times, the number of heads and tails will be much closer to 50/50.\n",
        "* **Why it matters:**  It's crucial for understanding how random events behave over time. It's the reason insurance companies can reliably predict risks, pollsters can estimate election results, and casinos can make a profit.\n",
        "\n",
        "**Law of Small Numbers**\n",
        "\n",
        "* **What it is:** This is more of a cognitive bias than a real law. It refers to people's tendency to incorrectly believe that small samples accurately represent the larger population.\n",
        "* **In simpler terms:**  If you meet two unfriendly people from a city, you might assume everyone in that city is unfriendly. This is the law of small numbers in action—drawing broad conclusions from limited evidence.\n",
        "* **Why it matters:**  This bias can lead to faulty decision-making, stereotypes, and inaccurate generalizations. It's important to be aware of it to avoid making these mistakes.\n",
        "\n",
        "**Here's an analogy:**\n",
        "\n",
        "Imagine a giant jar filled with thousands of marbles, with 90% red and 10% blue.\n",
        "\n",
        "* **Law of Large Numbers:** If you draw a handful of marbles (small sample), you might get mostly red or even all blue. But if you draw hundreds of marbles (large sample), you'll likely get a distribution close to 90% red and 10% blue.\n",
        "* **Law of Small Numbers:** If you draw only a few marbles and they're all blue, you might mistakenly believe the jar is mostly blue.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "* The law of large numbers is a proven mathematical principle, while the law of small numbers is a psychological bias.\n",
        "* The law of large numbers describes how random events converge to their true probabilities with increasing sample size, while the law of small numbers describes our tendency to misinterpret small samples.\n",
        "\n",
        "Understanding both these concepts is important for critical thinking and sound decision-making, especially when dealing with statistics and probability in everyday life.\n"
      ],
      "metadata": {
        "id": "_jbJexaHATHF"
      },
      "id": "_jbJexaHATHF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHzRLrl3BGZX"
      },
      "outputs": [],
      "source": [
        "# # find the mean of a large distribution\n",
        "# import numpy as np\n",
        "\n",
        "# np.random.seed(42)\n",
        "\n",
        "# data = np.random.randint(0, 5000, 5000)\n",
        "# np.mean(data)"
      ],
      "id": "eHzRLrl3BGZX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hufg0rHjBGZZ"
      },
      "outputs": [],
      "source": [
        "# # find sample means from generated list\n",
        "# samples = np.random.choice(data, 5)\n",
        "# # print(samples)\n",
        "# print(np.mean(samples))\n",
        "# samples = np.random.choice(data, 50)\n",
        "# print(np.mean(samples))\n",
        "# samples = np.random.choice(data, 500)\n",
        "# print(np.mean(samples))\n",
        "# samples = np.random.choice(data, 1000)\n",
        "# print(np.mean(samples))\n",
        "# samples = np.random.choice(data, 10000)\n",
        "# print(np.mean(samples))\n",
        "# samples = np.random.choice(data, 100000)\n",
        "# print(np.mean(samples))"
      ],
      "id": "hufg0rHjBGZZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Central Limit Theorem\n",
        "\n",
        "The Central Limit Theorem (CLT) is a cornerstone of statistics. It's a bit complex, but incredibly powerful in its implications. Here's a breakdown:\n",
        "\n",
        "**What it says:**\n",
        "\n",
        "Imagine you have a population of anything – people's heights, test scores, the number of petals on flowers, whatever. This population has some overall average and some variability.\n",
        "\n",
        "Now, you repeatedly take random samples from this population (and importantly, each sample is independent of the others).  The CLT states that if you calculate the *average* of each of these samples, the distribution of those sample averages will tend towards a normal distribution, *regardless* of the shape of the original population's distribution.\n",
        "\n",
        "**Here's the kicker:**\n",
        "\n",
        "*   This works even if the original population isn't normally distributed. It could be skewed, uniform, bimodal, whatever!\n",
        "*   The larger your sample size (the number of data points in each sample), the closer the distribution of the sample averages will be to a normal distribution.\n",
        "\n",
        "**Why is this so important?**\n",
        "\n",
        "1. **Inference:** It allows us to make inferences about a population based on a sample. Even if we don't know the true population distribution, we can use the CLT to estimate population parameters (like the mean) with confidence intervals.\n",
        "\n",
        "2. **Hypothesis Testing:**  Many statistical tests rely on the assumption of normality. The CLT allows us to use these tests even when the underlying population isn't normal, as long as our sample size is large enough.\n",
        "\n",
        "3. **Real-world applications:** It's used everywhere! In quality control, finance, opinion polling, medical research – any field where you're trying to draw conclusions about a population from a sample.\n",
        "\n",
        "**Analogy time:**\n",
        "\n",
        "Imagine you have a bag of differently shaped candies. You take a handful (a sample), calculate the average weight of the candies in your hand, and record it. You do this many times. The CLT says that even though the candies have different individual weights, the distribution of those average weights you record will look like a bell curve.\n",
        "\n",
        "**Important note:**\n",
        "\n",
        "While the CLT is powerful, it does have some assumptions:\n",
        "\n",
        "*   The samples must be random and independent.\n",
        "*   The sample size should be sufficiently large (generally, a sample size of 30 or more is considered sufficient, but it can depend on the original population distribution).\n",
        "\n",
        "If these assumptions are violated, the CLT might not hold.\n",
        "\n",
        "Hopefully, this explanation helps you grasp the core idea of the Central Limit Theorem! It's a complex concept, but understanding its essence is crucial for anyone working with statistics.\n"
      ],
      "metadata": {
        "id": "YN2zE28zBOpk"
      },
      "id": "YN2zE28zBOpk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcrzRxFkiHs5"
      },
      "source": [
        "* Pierre-Simon Laplace 1749 - 1827 (also revived Bayes Theorem)\n",
        "* Central Limit Theorem, in probability theory, a theorem that establishes the normal distribution as the distribution to which the mean (average) of almost any set of **independent and randomly generated (identically distributed) variables** rapidly converges. The central limit theorem explains why the normal distribution arises so commonly and why it is generally an excellent approximation for the mean of a collection of data (often with as few as 10 variables).\n",
        "* The standard version of the central limit theorem, first proved by the French mathematician Pierre-Simon Laplace in 1810, states that the sum or average of an infinite sequence of independent and identically distributed random variables, when suitably rescaled, tends to a normal distribution.\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  In probability theory, the central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions (para 1).\n",
        "\n",
        "And according to Scribbr (2023):\n",
        "\n",
        "> The central limit theorem states that if you take sufficiently large samples from a population, the samples’ means will be normally distributed, even if the population isn’t normally distributed.\n",
        "\n",
        "And according to Towards Data Science (2023):\n",
        "\n",
        "> If we take many samples from a population, and calculate a mean for each sample, then the distribution of these means across the samples (that is, the sampling distribution of the mean) will be the well-known normal distribution with the mean same as the population mean and with the standard deviation equal to the population standard deviation divided by the number of observations per sample. That’s the case even when the population is not normally distributed.\n",
        "\n",
        "References<br />\n",
        "* Central limit theorem. (2023, May 30). In *Scribbr*. https://www.scribbr.com/statistics/central-limit-theorem/\n",
        "* Central limit theorem. (2022, February 1). In *Wikipedia*. https://en.wikipedia.org/wiki/Central_limit_theorem.\n",
        "* https://towardsdatascience.com/central-limit-theorem-70b12a0a68d8"
      ],
      "id": "LcrzRxFkiHs5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMTwB1q7BKbA"
      },
      "outputs": [],
      "source": [
        "# # https://levelup.gitconnected.com/large-numbers-and-central-limit-theorem-using-numpy-1c8199ef63b1\n",
        "# # create right skewed data\n",
        "# import numpy as np\n",
        "# import seaborn as sns\n",
        "\n",
        "# data = np.random.gamma(1, 1000, 5000)\n",
        "# sns.kdeplot(data)\n",
        "# print('mean: ', np.mean(data))"
      ],
      "id": "QMTwB1q7BKbA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAM5rFJBBKbJ"
      },
      "outputs": [],
      "source": [
        "# # what a normal curve (distribution) looks like\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# x = np.linspace(-3, 3, 100)\n",
        "# y = stats.norm.pdf(x, 0, 1)\n",
        "# plt.plot(x, y)\n",
        "# plt.show()"
      ],
      "id": "xAM5rFJBBKbJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsSB69u1BKbJ"
      },
      "outputs": [],
      "source": [
        "# # visualize 100 sample means and histogram\n",
        "# import numpy as np\n",
        "# import seaborn as sns\n",
        "\n",
        "# means = []\n",
        "# for i in range(100000):\n",
        "#     means.append(np.random.choice(data, 100).mean())\n",
        "\n",
        "# sns.kdeplot(means)\n",
        "# print('mean: ', np.mean(data))"
      ],
      "id": "CsSB69u1BKbJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy and Precision in Regression\n",
        "\n",
        "**Accuracy in Regression**\n",
        "\n",
        "* **Focus:** How close the predicted values are to the true values.\n",
        "* **Challenge:**  In regression, you're predicting a continuous value (e.g., house price, temperature), not just a category.  So, it's less about being exactly right and more about how close you are on average.\n",
        "* **Metrics:**\n",
        "    * **Mean Absolute Error (MAE):** The average absolute difference between predicted and true values.\n",
        "    * **Mean Squared Error (MSE):** The average squared difference between predicted and true values (penalizes larger errors more).\n",
        "    * **Root Mean Squared Error (RMSE):** The square root of MSE (in the same units as the target variable).\n",
        "    * **R-squared:**  Represents the proportion of variance in the target variable explained by the model.\n",
        "\n",
        "* **Interpretation:** Lower values for MAE, MSE, and RMSE indicate higher accuracy. Higher R-squared indicates a better fit. This is what this class is all about!\n",
        "\n",
        "**Precision in Regression**\n",
        "\n",
        "* **Focus:** How consistent the predictions are for different data points or different runs of the model.\n",
        "* **Relationship to variance:**  Precision in regression is closely tied to the variance of the model's predictions.\n",
        "    * Low variance = high precision (predictions are tightly clustered)\n",
        "    * High variance = low precision (predictions are spread out)\n",
        "\n",
        "* **Factors affecting precision:**\n",
        "    * **Model complexity:** More complex models can capture more intricate patterns but might also be more sensitive to small variations in the data, leading to lower precision.\n",
        "    * **Data quality:** Noisy or inconsistent data can reduce precision.\n",
        "    * **Regularization:** Techniques like L1 or L2 regularization can help improve precision by preventing overfitting.\n",
        "\n",
        "* **Visualization:**  Plotting predicted values against true values can help visualize both accuracy and precision. A narrow band of points around the ideal 45-degree line indicates high accuracy and precision.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Imagine a model predicting house prices.\n",
        "\n",
        "* **High accuracy:** The predicted prices are, on average, close to the actual sale prices.\n",
        "* **High precision:** The model's predictions are consistent and don't vary wildly for similar houses.\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "In regression, accuracy is about how close you get to the true values, while precision is about the consistency of your predictions. Both are important for building effective regression models. By using appropriate metrics and visualizations, you can assess and improve both the accuracy and precision of your models.\n"
      ],
      "metadata": {
        "id": "btPD4w21_xf9"
      },
      "id": "btPD4w21_xf9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy and Precision in Classification\n",
        "\n",
        "**Key Metrics in a Classification Report**\n",
        "\n",
        "* **Accuracy:** The overall correctness of the model.  \n",
        "    * It answers: \"Out of all the predictions, how many were correct?\"\n",
        "    * Can be misleading with imbalanced datasets (where one class has far more instances than another).\n",
        "\n",
        "* **Precision:**  Focuses on the accuracy of positive predictions.\n",
        "    *  It answers the question: \"Of all the instances the model predicted as positive, how many were actually correct?\"\n",
        "    *  Useful when the cost of false positives is high (e.g., diagnosing someone with a disease they don't have).\n",
        "\n",
        "* **Recall (Sensitivity):** Measures the model's ability to identify all actual positive instances.\n",
        "    * It answers: \"Of all the actual positive instances, how many did the model correctly identify?\"\n",
        "    * Crucial when the cost of false negatives is high (e.g., missing a critical medical diagnosis).\n",
        "\n",
        "**How they work together**\n",
        "\n",
        "The classification report uses these metrics to give you a more complete picture of your model's performance than accuracy alone.  \n",
        "\n",
        "* **Example:** Imagine a model detecting spam emails.\n",
        "    * High precision means it correctly labels most of the emails it flags as spam.\n",
        "    * High recall means it catches most of the actual spam emails.\n",
        "    * Accuracy might be high overall, but if it misses a lot of spam (low recall), it's not very useful.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "A classification report helps you understand not just if your model is right or wrong, but *how* it's right or wrong, and where it might need improvement. This is crucial for building effective and reliable classification models.#"
      ],
      "metadata": {
        "id": "O83soRxi-WoN"
      },
      "id": "O83soRxi-WoN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression and Classification Reminder\n",
        "\n",
        "**Regression**\n",
        "\n",
        "* **Goal:** Predict a continuous or numerical outcome. Think of it like estimating a value on a scale.\n",
        "* **Output:** A number (e.g., house price, temperature, stock price).\n",
        "* **Examples:**\n",
        "    * Predicting house prices based on size, location, etc.\n",
        "    * Forecasting sales for the next quarter\n",
        "    * Estimating the time it will take to complete a task\n",
        "\n",
        "**Classification**\n",
        "\n",
        "* **Goal:**  Predict a categorical or discrete outcome.  Think of it like putting things into buckets.\n",
        "* **Output:** A class label (e.g., \"spam\", \"not spam\", \"cat\", \"dog\", \"red\", \"green\", \"blue\").\n",
        "* **Examples:**\n",
        "    * Image recognition (classifying images as cat, dog, bird, etc.)\n",
        "    * Spam detection (classifying emails as spam or not spam)\n",
        "    * Medical diagnosis (classifying a tumor as benign or malignant)\n",
        "\n",
        "**Key Differences**\n",
        "\n",
        "| Feature | Classification | Regression |\n",
        "|---|---|---|\n",
        "| Output | Discrete (categories, labels) | Continuous (numbers) |\n",
        "| Algorithms | Logistic regression, decision trees, support vector machines, naive Bayes | Linear regression, polynomial regression, support vector regression, random forests |\n",
        "| Evaluation Metrics | Accuracy, precision, recall, F1-score | Mean squared error (MSE), R-squared |\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "Classification is about assigning things to categories, while regression is about predicting numerical values. Both are fundamental tasks in machine learning and data science, used in a wide range of applications."
      ],
      "metadata": {
        "id": "yuZm46WmAHk7"
      },
      "id": "yuZm46WmAHk7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Types\n",
        "\n",
        "* Numerical: has measurement\n",
        "    * Discrete: can be counted (number of people in a class)\n",
        "    * Continuous: described as a measurement (height, weight, temps), usually rounded off\n",
        "* Categorical: some type of group, category, that can be represented by a number (female: 1, male: 2) but have no mathematical meaning\n",
        "* Nominal: named or labeled (31 flavors)\n",
        "* Ordinal: ordered or scaled (first, second, etc, very happy, happy, meh, sad, very sad)\n",
        "* Cardinal: In set theory, cardinality refers to the number of elements in a set. Cardinal data typically represents counts of discrete items (1, 2, 3, 4, 5, etc.)\n",
        "* Interval: ordered numbers with equal distances (50 degrees, 60 degrees, etc)\n",
        "* Ratio: has a true zero (can't have height of -2 inches, distance between 1 and 2 is the same as 3 and 4 and 4 is twice 2)"
      ],
      "metadata": {
        "id": "njGuJh1JFTAp"
      },
      "id": "njGuJh1JFTAp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bias\n",
        "\n",
        "Data bias is a sneaky problem! It can creep into your data in many ways, often without you even realizing it. Here are some of the common types of bias to watch out for:\n",
        "\n",
        "**1. Sampling Bias**\n",
        "\n",
        "* **What it is:** Your sample doesn't accurately represent the population you're interested in. This can lead to inaccurate conclusions about the population.\n",
        "* **Examples:**\n",
        "    * **Convenience Sampling:**  Surveying only people who are easy to reach (e.g., your friends, people on the street).\n",
        "    * **Undercoverage:**  Some groups in the population are underrepresented in the sample (e.g., a survey about internet usage that only includes people with landline phones).\n",
        "\n",
        "**2. Measurement Bias**\n",
        "\n",
        "* **What it is:** The way you collect data influences the results.\n",
        "* **Examples:**\n",
        "    * **Leading Questions:**  Survey questions that are worded in a way that suggests a particular answer.\n",
        "    * **Social Desirability Bias:** People tend to answer questions in a way that makes them look good, even if it's not entirely truthful.\n",
        "    * **Instrument Bias:**  Problems with the measurement tool itself (e.g., a faulty scale).\n",
        "\n",
        "**3. Confirmation Bias**\n",
        "\n",
        "* **What it is:** You tend to favor information that confirms your existing beliefs and ignore information that contradicts them.\n",
        "* **Example:**  Only looking for research that supports your hypothesis and dismissing studies that don't.\n",
        "\n",
        "**4. Observer Bias**\n",
        "\n",
        "* **What it is:** The researcher's expectations or beliefs influence how they perceive or interpret the data.\n",
        "* **Example:**  A doctor might be more likely to diagnose a patient with a condition they're already familiar with, even if the symptoms could indicate something else.\n",
        "\n",
        "**5. Omitted Variable Bias**\n",
        "\n",
        "* **What it is:**  An important variable that influences the relationship between the variables you're studying is left out of your analysis.\n",
        "* **Example:**  Analyzing the relationship between exercise and heart health without considering factors like diet or genetics.\n",
        "\n",
        "**6. Survivorship Bias**\n",
        "\n",
        "* **What it is:**  You only focus on the \"survivors\" of a process and ignore those that didn't make it.\n",
        "* **Example:**  Studying the characteristics of successful businesses without considering the businesses that failed.\n",
        "\n",
        "**7.  Historical Bias**\n",
        "\n",
        "* **What it is:** Data from the past may reflect societal biases or norms that are no longer relevant or acceptable.\n",
        "* **Example:**  Analyzing historical hiring data might reveal discriminatory practices that were prevalent at the time.\n",
        "\n",
        "**8.  Recall Bias**\n",
        "\n",
        "* **What it is:**  People's memories of past events can be inaccurate or influenced by their current beliefs.\n",
        "* **Example:**  Asking people to recall their childhood eating habits might lead to inaccurate data.\n",
        "\n",
        "You're absolutely correct! Selection bias and publication bias are two very important types of bias that can significantly impact data analysis and interpretation. Let's add them to the list:\n",
        "\n",
        "**9. Selection Bias**\n",
        "\n",
        "* **What it is:** This occurs when the process of selecting individuals, groups, or data for analysis results in a sample that is not representative of the population of interest. This can lead to skewed results and inaccurate conclusions.\n",
        "* **Examples:**\n",
        "    * **Sampling Bias (as mentioned before):**  A specific type of selection bias where the sample isn't representative due to the method of selection.\n",
        "    * **Attrition Bias:** This happens when participants drop out of a study non-randomly, leading to a biased sample of those who remain.\n",
        "    * **Non-response Bias:** Occurs when people who choose to participate in a study differ systematically from those who don't.\n",
        "\n",
        "**10. Publication Bias**\n",
        "\n",
        "* **What it is:** This bias arises in academic research when studies with positive or significant results are more likely to be published than studies with negative or non-significant results. This can create a misleading impression of the true state of knowledge on a topic.\n",
        "* **Examples:**\n",
        "    * **File Drawer Problem:**  Studies with negative results are often \"filed away\" and never published, leading to an overrepresentation of positive findings in the literature.\n",
        "    * **Outcome Reporting Bias:** Researchers may selectively report outcomes that support their hypotheses, while downplaying or ignoring those that don't.\n",
        "\n",
        "**Why are selection and publication bias important?**\n",
        "\n",
        "* **Selection bias** can distort the results of any study, making it difficult to generalize findings to the broader population.\n",
        "* **Publication bias** can lead to a biased view of the evidence on a particular topic, potentially influencing decision-making in areas like healthcare, policy, and research.\n",
        "\n",
        "**How to mitigate these biases:**\n",
        "\n",
        "* **Selection Bias:**\n",
        "    * Use random sampling methods whenever possible.\n",
        "    * Carefully consider the inclusion and exclusion criteria for your study.\n",
        "    * Analyze the characteristics of participants who drop out or don't respond to identify potential biases.\n",
        "\n",
        "* **Publication Bias:**\n",
        "    * Encourage the publication of negative results through platforms like pre-print servers and journals that specifically publish null findings.\n",
        "    * Conduct systematic reviews and meta-analyses that include unpublished studies to get a more complete picture of the evidence.\n",
        "\n",
        "\n",
        "**Why is it important to be aware of bias?**\n",
        "\n",
        "Bias can lead to inaccurate conclusions, flawed models, and unfair or discriminatory outcomes.  By understanding the different types of bias, you can take steps to minimize their impact and ensure that your data analysis is as objective and reliable as possible.\n",
        "\n",
        "**Selection Bias, Measurement Bias, Outcome Report Bias Example**\n",
        "\n",
        "Story of a Texas school district needing good scores on a standard achievement test given in 10th grade so they held back poor performers in 9th grade and then bumped them up to 11th grade after a couple of years."
      ],
      "metadata": {
        "id": "lxDwuP4VNJyD"
      },
      "id": "lxDwuP4VNJyD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Big Data\n",
        "\n",
        "How Data Happened, Wiggins and Jones\n",
        "\n",
        "* Better tools, services, and public goods or privacy incursions and invasive marketing?\n",
        "* Understand online communities and political movements or track protesters and suppress speech?\n",
        "* Transform how we study human communication and culture or narrow research options and alter what research means?\n",
        "* Tendency with too much data is selecting what we like and ignoring the rest\n",
        "* Whiteout like a snowstorm blinds us from the signal, or presents to many signals, start seeing patterns in random noise\n",
        "* Noise is increasing faster than information\n",
        "* As Nate Silvers says in his book Signal in the Noise, noise distracts us from the truth (objective truth?)\n",
        "* Objective Truth is what exists and can be proved in this physicality. (The sun moves across the sky each day.)\n",
        "* Normative Truth is what we, as a group, agree is true. (English speakers agreed to use the word day to name that time when the sky is lit by the sun.)\n",
        "* Subjective Truth is how the individual sees or experiences the world. (Today is a good day for me.)\n",
        "* Complex Truth recognizes the validity of all those truths and allows you to focus on the one is most useful at any given time. (The sun is up; the day is bright. Today is a good day for MOM, so lets take advantage of that and ask for ice cream for dinner.)\n",
        "* An objective statement is factual; it has a definite correspondence to reality, independent of anyone's feelings or biases.\n",
        "* The distinction between subjectivity and objectivity is a basic idea of philosophy, particularly epistemology and metaphysics. It is often related to discussions of consciousness, agency, personhood, philosophy of mind, philosophy of language, reality, truth, and communication.\n",
        "\n",
        "**Chapter 1 Summary**:\n",
        "\n",
        "In this chapter, the authors explore the complex relationship between data, technology, and society, emphasizing the need for greater awareness of the potential biases and harms associated with data-driven decision-making. They also stress the importance of accountability and fairness in algorithmic systems, especially given the increasing prevalence of automated decision-making in various sectors.\n",
        "\n",
        "The authors argue that while data and technology offer promising advancements, they are not neutral and can perpetuate existing inequalities if not developed and deployed responsibly.  They highlight concerns about privacy, surveillance, and the potential for algorithmic bias to amplify social injustices.\n",
        "\n",
        "The chapter also touches on the historical context of data collection and analysis, emphasizing that the issues we face today are not entirely new. The authors point to past instances where the accumulation and analysis of data have posed threats to privacy and exacerbated social inequalities.\n",
        "\n",
        "Overall, this chapter serves as a call for greater critical engagement with data and technology, urging readers to consider the potential consequences of algorithmic systems and advocate for fairness, transparency, and accountability in their development and implementation.\n",
        "\n",
        "Resources:\n",
        "\n",
        "* How Data Happened: A History from the Age of Reason to the Age of Algorithms By Chris Wiggins, Matthew L. Jones 2023\n",
        "* https://www.hsdinstitute.org/resources/four-truths.html\n",
        "* https://www.gotquestions.org/objective-truth.html\n",
        "* https://en.wikipedia.org/wiki/Subjectivity_and_objectivity_(philosophy)\n"
      ],
      "metadata": {
        "id": "YYp3ofIBWNPu"
      },
      "id": "YYp3ofIBWNPu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Randomized Controlled Trials (RCTs or AB Testing), Multicollinearity, and Confounders\n",
        "\n",
        "Child's shoe size and reading level is correlated but consider the child's age\n",
        "\n",
        "* Randomized Controlled Trial: A study design that randomly assigns participants into an experimental group or a control group. As the study is conducted, the only expected difference between the control and experimental groups in a randomized controlled trial (RCT) is the outcome variable being studied.\n",
        "* Multicollinearity: two or more independent variables are correlated with each other\n",
        "    * Removing variables?\n",
        "* Confounders\n",
        "  * An extraneous variable whose presence affects the variables being studied so that the results do not reflect the actual relationship between the variables under study.\n",
        "  * A variable that influences both the dependent variable and independent variable, causing a spurious association.\n",
        "* Lurking variable\n",
        "  * A lurking variable is a variable that is not included in a statistical analysis but can still affect the outcome of that analysis\n",
        "* Statistical Control\n",
        "    * Variables may be controlled directly by holding them constant throughout a study\n",
        "    * This hurts confounders as they share a relationship for what's being controlled\n",
        "    * Confounders need to be considered in the big picture\n",
        "    * Sometimes you end up controlling the thing you are trying to measure (Ezra Klein)\n",
        "    \n",
        "Sources:\n",
        "* https://himmelfarb.gwu.edu/tutorials/studydesign101/rcts.cfm\n",
        "* https://harini.blog/2021/10/10/understanding-multicollinearity-and-condounding-variables-in-regression/\n",
        "* https://statisticsbyjim.com/basics/lurking-variable/"
      ],
      "metadata": {
        "id": "s7IcZ-57Vj7c"
      },
      "id": "s7IcZ-57Vj7c"
    },
    {
      "cell_type": "markdown",
      "id": "a3076611",
      "metadata": {
        "id": "a3076611"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b9c5e9",
      "metadata": {
        "id": "26b9c5e9"
      },
      "source": [
        "### Sklearn\n",
        "\n",
        "* https://scikit-learn.org/stable/datasets.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd40e354",
      "metadata": {
        "id": "dd40e354"
      },
      "outputs": [],
      "source": [
        "# # fetch california housing\n",
        "# from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# housing = fetch_california_housing()\n",
        "# housing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf385ea6",
      "metadata": {
        "id": "cf385ea6"
      },
      "outputs": [],
      "source": [
        "# # get keys\n",
        "# housing.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2816bb",
      "metadata": {
        "id": "5e2816bb"
      },
      "outputs": [],
      "source": [
        "# # get description\n",
        "# print(housing.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "639d8790",
      "metadata": {
        "id": "639d8790"
      },
      "outputs": [],
      "source": [
        "# # print feature and target names\n",
        "# print(housing.feature_names)\n",
        "# print(housing.target_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c62c6de",
      "metadata": {
        "id": "1c62c6de"
      },
      "outputs": [],
      "source": [
        "# # create housing dataframe\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "# df['MedHouseVal'] = housing.target\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e85a2e3",
      "metadata": {
        "id": "3e85a2e3"
      },
      "outputs": [],
      "source": [
        "# # alternative method\n",
        "# import pandas as pd\n",
        "# from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# X, y = fetch_california_housing(return_X_y=True)\n",
        "# # print(X[0:2])\n",
        "# housing = fetch_california_housing()\n",
        "# housing_df = pd.DataFrame(X, columns=housing.feature_names)\n",
        "# housing_df['MedHouseVal'] = housing.target\n",
        "# print(housing_df.shape)\n",
        "# print(housing_df.info())\n",
        "# housing_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "898e3324",
      "metadata": {
        "id": "898e3324"
      },
      "source": [
        "### Seaborn\n",
        "\n",
        "* https://seaborn.pydata.org/generated/seaborn.load_dataset.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "149d2500",
      "metadata": {
        "id": "149d2500"
      },
      "outputs": [],
      "source": [
        "# # get list of seaborn datasets\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.get_dataset_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ed687d",
      "metadata": {
        "id": "20ed687d"
      },
      "outputs": [],
      "source": [
        "# # loading seaborn dataset\n",
        "# iris = sns.load_dataset('iris')\n",
        "# print(iris.shape)\n",
        "# print(iris.info())\n",
        "# iris.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UC Irvine Machine Learning Repository"
      ],
      "metadata": {
        "id": "_-sg5z9CM_-G"
      },
      "id": "_-sg5z9CM_-G"
    },
    {
      "cell_type": "code",
      "source": [
        "# # UC Irvine Machine Learning Repository example (mpg)\n",
        "# # https://archive.ics.uci.edu/dataset/9/auto+mpg\n",
        "# !pip install ucimlrepo"
      ],
      "metadata": {
        "id": "l32Jruo-KYCf"
      },
      "id": "l32Jruo-KYCf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # copy uci's example\n",
        "# from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# # fetch dataset\n",
        "# mpg = fetch_ucirepo(id=9)\n",
        "\n",
        "# # data (as pandas dataframes)\n",
        "# X = mpg.data.features\n",
        "# y = mpg.data.targets\n",
        "\n",
        "# # metadata\n",
        "# print(mpg.metadata)\n",
        "\n",
        "# # variable information\n",
        "# print(mpg.variables)\n"
      ],
      "metadata": {
        "id": "lr0wLTGOLCcX"
      },
      "id": "lr0wLTGOLCcX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # create dataframe from uci url\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv(mpg.metadata.data_url)\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "ogIP0mEILlHk"
      },
      "id": "ogIP0mEILlHk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fc8f5538",
      "metadata": {
        "id": "fc8f5538"
      },
      "source": [
        "### Online Repositories\n",
        "* https://archive.ics.uci.edu/\n",
        "* http://lib.stat.cmu.edu/datasets/\n",
        "* https://www.kaggle.com/datasets\n",
        "\n",
        "See Public Data Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64377d9a",
      "metadata": {
        "id": "64377d9a"
      },
      "source": [
        "### .csv\n",
        "\n",
        "Get Advertising.csv from dataset repository and upload it into session storage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_slp_OS3UdB-"
      },
      "id": "_slp_OS3UdB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcab4d37",
      "metadata": {
        "id": "bcab4d37"
      },
      "outputs": [],
      "source": [
        "# # get a public dataset into session storage\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('public_data.csv')\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7207616",
      "metadata": {
        "id": "a7207616"
      },
      "outputs": [],
      "source": [
        "# # reading from url\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/INFO4050/main/Datasets/Advertising.csv', usecols=['TV', 'radio', 'newspaper', 'sales'])\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb57721",
      "metadata": {
        "id": "abb57721"
      },
      "source": [
        "### .data\n",
        "\n",
        "get auto-mpg.data from class repository\n",
        "\n",
        "['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc209c1",
      "metadata": {
        "id": "2bc209c1"
      },
      "outputs": [],
      "source": [
        "# # sep data example\n",
        "# import pandas as pd\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/gitmystuff/Datasets/main/auto-mpg.data'\n",
        "# cars = pd.read_csv(url, sep='\\s+', header=None)\n",
        "# cols = ['mpg', 'cylinders', 'displacement', 'horespower', 'weight', 'acceleration', 'year', 'origin', 'name']\n",
        "# cars.columns = cols\n",
        "# cars.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc32dfd",
      "metadata": {
        "id": "9dc32dfd"
      },
      "source": [
        "### Simulated Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa2a0947",
      "metadata": {
        "id": "aa2a0947"
      },
      "outputs": [],
      "source": [
        "# # example with make_regression with noise=0\n",
        "# from sklearn.datasets import make_regression\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# X, y = make_regression(n_samples=100, n_features=1, noise=0)\n",
        "# plt.scatter(X, y)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1af1f358",
      "metadata": {
        "id": "1af1f358"
      },
      "outputs": [],
      "source": [
        "# # example with make_regression with noise=50\n",
        "# X, y = make_regression(n_samples=100, n_features=1, noise=50)\n",
        "# plt.scatter(X, y)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf36cc1",
      "metadata": {
        "id": "cbf36cc1"
      },
      "source": [
        "### Data Codebook\n",
        "\n",
        "https://www.kaggle.com/rhuebner/human-resources-data-set<br />\n",
        "https://rpubs.com/rhuebner/hrd_cb_v14"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbdf963",
      "metadata": {
        "id": "cfbdf963"
      },
      "source": [
        "### Excel: Text to Column\n",
        "\n",
        "* Download the student.zip file from http://archive.ics.uci.edu/ml/machine-learning-databases/00320/\n",
        "* Unzip to an appropriate folder, and open student-mat.csv in Excel (HINT: Right-click on student-mat.csv and choose Open with Excel)\n",
        "* Save this file as student-mat.xlsx\n",
        "* Use Text to Column to format data\n",
        "* HINT: select all the data in the first column before using Text to Column and then use the semicolon as the Delimiter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07eee1a2",
      "metadata": {
        "id": "07eee1a2"
      },
      "source": [
        "## Some Types of Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78b44513",
      "metadata": {
        "id": "78b44513"
      },
      "source": [
        "### Univariate Analysis\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  Univariate analysis is the simplest form of analyzing data. Uni means one, so in other words the data has only one variable. Univariate data requires to analyze each variable separately. Data is gathered for the purpose of answering a question, or more specifically, a research question. Univariate data does not answer research questions about relationships between variables, but rather it is used to describe one characteristic or attribute that varies from observation to observation (para 4).\n",
        "\n",
        "Univariate (statistics). (2022, January 24). In *Wikipedia*. https://en.wikipedia.org/wiki/Univariate_(statistics)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19efe84f",
      "metadata": {
        "id": "19efe84f"
      },
      "source": [
        "Note About read_csv\n",
        "\n",
        "* https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
        "* ',' is default since csv is used in the method name\n",
        "* Defining our sep such as \\s+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd61fa9",
      "metadata": {
        "id": "efd61fa9"
      },
      "outputs": [],
      "source": [
        "# # auto-mpg.data from class repository\n",
        "# import pandas as pd\n",
        "\n",
        "# cars = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/auto-mpg.data', sep = '\\s+', header = None)\n",
        "# cars.columns=['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin',  'car name']\n",
        "# cars.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57808e3d",
      "metadata": {
        "id": "57808e3d"
      },
      "outputs": [],
      "source": [
        "# # shape and info\n",
        "# print(cars.shape)\n",
        "# print(cars.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c6dd7c9",
      "metadata": {
        "id": "9c6dd7c9"
      },
      "outputs": [],
      "source": [
        "# # example of univariate histogram\n",
        "# cars['acceleration'].hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c116d01",
      "metadata": {
        "id": "0c116d01"
      },
      "outputs": [],
      "source": [
        "# # example of univariate pie chart\n",
        "# print(cars.cylinders.value_counts())\n",
        "# cars.cylinders.value_counts().plot.pie();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e64c95b",
      "metadata": {
        "id": "5e64c95b"
      },
      "source": [
        "### Bivariate Analysis\n",
        "\n",
        "* Used to describe the relationship between two variables such as between a feature and the target\n",
        "* The green lines show the numeric value of the means of the two variables\n",
        "* We lose confidence the further we get from where the two means cross\n",
        "\n",
        "Confidence intervals are about parameters not samples; e.g. the majority of the points in the OP's image are not within the shaded area (also seaborn uses 95% CIs by default). a better intuition in this example is that the data is consistent with any slope that passes through the CI. The reason it expands at the ends is because the data gives \"less information\" there.\n",
        "\n",
        "https://stackoverflow.com/questions/62167783/what-does-the-background-area-mean-in-seaborn-regression-plot\n",
        "\n",
        "If I construct a CI at some point x (slope), then I'm 95% confident that the regression line (not necessarily the mean) will be in the interval, vs the true population mean which is a fixed unknown value that is either inside or outside the CI with 100% certainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26751986",
      "metadata": {
        "id": "26751986"
      },
      "outputs": [],
      "source": [
        "# # bivariate scatter plot\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.regplot(data=cars, x='acceleration', y='mpg', line_kws={'color': 'red'}); # recommended to not use ci with large data\n",
        "# plt.axvline(x=cars['acceleration'].mean(), color='green')\n",
        "# plt.axhline(y=cars['mpg'].mean(), color='green');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2571c261",
      "metadata": {
        "id": "2571c261"
      },
      "source": [
        "### Multivariate Analysis\n",
        "\n",
        "Analysis using more than two variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8fcb34d",
      "metadata": {
        "id": "f8fcb34d"
      },
      "outputs": [],
      "source": [
        "# # showing correlation of multiple features with one target\n",
        "# cars[['displacement', 'horsepower', 'weight', 'acceleration']].corrwith(cars['mpg']).plot.bar(\n",
        "#         title = \"Correlation with MPG Target\", fontsize = 15,\n",
        "#         rot = 45, grid = True);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee1b272",
      "metadata": {
        "id": "cee1b272"
      },
      "source": [
        "### Predictive Multivariate Analysis Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e59a858",
      "metadata": {
        "id": "7e59a858"
      },
      "outputs": [],
      "source": [
        "# # complete a bare bone predictive model for the auto_mpg dataset using only the numeric data and identify problems\n",
        "# import pandas as pd\n",
        "\n",
        "# cars = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/auto-mpg.data', sep = '\\s+', header = None)\n",
        "# cars.columns=['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin',  'car name']\n",
        "# cars.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe327ae",
      "metadata": {
        "id": "1fe327ae"
      },
      "outputs": [],
      "source": [
        "# # description\n",
        "# print(cars.shape)\n",
        "# print(cars.info()) # why is horsepower object"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # check for non-numeric values\n",
        "# cars[pd.to_numeric(cars['horsepower'], errors='coerce').isnull()]"
      ],
      "metadata": {
        "id": "usByco-35f5S"
      },
      "id": "usByco-35f5S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # what is a mask\n",
        "# mask = (cars['origin'] == 1) & (cars['mpg'] > 35)\n",
        "# cars[mask]"
      ],
      "metadata": {
        "id": "ZcDfK0bR5i-r"
      },
      "id": "ZcDfK0bR5i-r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "385f94d8",
      "metadata": {
        "id": "385f94d8"
      },
      "outputs": [],
      "source": [
        "# # correct horsepower\n",
        "# hp_avg = cars['horsepower'][cars['horsepower'] != '?'].astype(float).mean()\n",
        "# cars['horsepower'] = cars['horsepower'].replace('?', int(hp_avg))\n",
        "# cars['horsepower'] = cars['horsepower'].astype('float')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a5b3d5",
      "metadata": {
        "id": "c0a5b3d5"
      },
      "outputs": [],
      "source": [
        "# # just use ['displacement', 'horsepower', 'weight', 'acceleration']\n",
        "# cars = cars[['displacement', 'horsepower', 'weight', 'acceleration', 'mpg']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdd8777",
      "metadata": {
        "id": "ccdd8777"
      },
      "outputs": [],
      "source": [
        "# # some descriptives\n",
        "# print(cars.shape)\n",
        "# print(cars.info())\n",
        "# print(cars.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd134eb",
      "metadata": {
        "id": "6fd134eb"
      },
      "outputs": [],
      "source": [
        "# # show histograms\n",
        "# cars.hist()\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c78376",
      "metadata": {
        "id": "f6c78376"
      },
      "outputs": [],
      "source": [
        "# # train test split (next week)\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(cars.drop(['mpg'], axis=1),\n",
        "#                                                     cars.mpg,\n",
        "#                                                     test_size=0.20,\n",
        "#                                                     random_state=42)\n",
        "# print(X_train.shape)\n",
        "# print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb00325c",
      "metadata": {
        "id": "eb00325c"
      },
      "outputs": [],
      "source": [
        "# # train and test model\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# # R-squared is a statistical measure that represents\n",
        "# # the proportion of the variance for a dependent variable\n",
        "# # that's explained by an independent variable.\n",
        "\n",
        "# model = LinearRegression()\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(f'MSE: {mean_squared_error(y_true=y_test, y_pred=predictions)}')\n",
        "# print(f'R-Squared: {r2_score(y_test, predictions)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Fun"
      ],
      "metadata": {
        "id": "W6XQZoypaQey"
      },
      "id": "W6XQZoypaQey"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Slicing"
      ],
      "metadata": {
        "id": "0xHFF_mKceHA"
      },
      "id": "0xHFF_mKceHA"
    },
    {
      "cell_type": "code",
      "source": [
        "# # matrix of rows and columns\n",
        "# mat = np.random.randint(10, size=(5, 5))\n",
        "# print(mat)"
      ],
      "metadata": {
        "id": "DMa7xaH8HjPK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DMa7xaH8HjPK"
    },
    {
      "cell_type": "code",
      "source": [
        "# # slicing col [row, col]\n",
        "# print(mat[0:1])"
      ],
      "metadata": {
        "id": "Fm-Kh5CFHQDj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Fm-Kh5CFHQDj"
    },
    {
      "cell_type": "code",
      "source": [
        "# # slicing row [row start: end, col]\n",
        "# print(mat[0:,0])"
      ],
      "metadata": {
        "id": "zUvADxyPIT3r"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zUvADxyPIT3r"
    },
    {
      "cell_type": "code",
      "source": [
        "# # slicing [row start: end, col start: end]\n",
        "# print(mat[0:3,3:4])"
      ],
      "metadata": {
        "id": "CqOYV7gRIrbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CqOYV7gRIrbs"
    },
    {
      "cell_type": "code",
      "source": [
        "# # slicing [row start: end, col start: end]\n",
        "# print(mat[:3,3:])"
      ],
      "metadata": {
        "id": "U1bgmCmBI6Gq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "U1bgmCmBI6Gq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Systematic Sampling"
      ],
      "metadata": {
        "id": "5ngCMIB_cozO"
      },
      "id": "5ngCMIB_cozO"
    },
    {
      "cell_type": "code",
      "source": [
        "# # systematic sampling [start, end, interval]\n",
        "# print(pop[0:7])\n",
        "# pop[0:25:3]"
      ],
      "metadata": {
        "id": "BAZjFL2mJc1W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BAZjFL2mJc1W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified Sampling"
      ],
      "metadata": {
        "id": "MWppQem9cko2"
      },
      "id": "MWppQem9cko2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. Clustering by Country Origin**\n",
        "\n",
        "*   **Purpose:** You're creating strata (subgroups) within your dataset based on the country of origin (e.g., USA, Japan, Europe). This ensures that each origin group is represented equally in your final sample.\n",
        "*   **Why this matters:** Stratified sampling helps prevent imbalances in your sample that could lead to biased results. If one origin group is overrepresented, your analysis might not accurately reflect the overall patterns in the data.\n",
        "\n",
        "**2. Creating a New DataFrame**\n",
        "\n",
        "*   **Purpose:** You're forming a new dataframe to hold your stratified sample. This keeps your original dataset intact while allowing you to work with the specific sample you've created.\n",
        "*   **Why this matters:**  Organization and clarity. It's good practice to keep your raw data separate from your processed or sampled data.\n",
        "\n",
        "**3. Value Count with Normalization**\n",
        "\n",
        "*   **Purpose:** You're calculating the proportion of each origin group in your stratified sample. Normalization ensures these proportions sum up to 1 (or 100%).\n",
        "*   **Why this matters:**  This gives you a clear picture of the distribution of origin groups in your sample and confirms that you've achieved equal representation.\n",
        "\n",
        "**4. Assigning Weights**\n",
        "\n",
        "*   **Purpose:** You're likely assigning weights to each observation in your sample to adjust for any differences in the size of the origin groups in the original population.\n",
        "*   **Why this matters:**  Weighting ensures that your sample accurately reflects the proportions of each origin group in the population you're trying to represent. If one origin group is smaller in the population, its observations in the sample will be weighted more heavily to compensate.\n",
        "\n",
        "**5. Group By with Origin Count**\n",
        "\n",
        "*   **Purpose:** You're likely grouping your stratified sample by origin and counting the number of observations in each group.\n",
        "*   **Why this matters:**  This is a final check to confirm that you have equal representation of each origin group in your stratified sample.\n",
        "\n",
        "**In essence, you're demonstrating the key principles of stratified sampling:**\n",
        "\n",
        "*   Divide the population into meaningful subgroups (strata).\n",
        "*   Sample proportionally from each stratum to ensure representation.\n",
        "*   Adjust for any differences in population proportions through weighting.\n",
        "\n",
        "By following these steps, you're creating a sample that is more likely to accurately reflect the characteristics of the overall population, leading to more reliable and generalizable results in your analysis."
      ],
      "metadata": {
        "id": "Lym0bxS1ctmy"
      },
      "id": "Lym0bxS1ctmy"
    },
    {
      "cell_type": "code",
      "source": [
        "# # clustering\n",
        "# import seaborn as sns\n",
        "\n",
        "# mpg = sns.load_dataset('mpg')\n",
        "# print(mpg.shape)\n",
        "# mpg.head()"
      ],
      "metadata": {
        "id": "UwgPh7LkTvAU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UwgPh7LkTvAU"
    },
    {
      "cell_type": "code",
      "source": [
        "# # get value counts for origin to determine cluster size\n",
        "# mpg['origin'].value_counts()"
      ],
      "metadata": {
        "id": "2CTVQUqR6uw_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2CTVQUqR6uw_"
    },
    {
      "cell_type": "code",
      "source": [
        "# # stratified sampling\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# # shift + alt + arrow down or up\n",
        "# clst1 = mpg.where(mpg['origin'] == 'usa').sample(50, replace=False) # False is default\n",
        "# clst2 = mpg.where(mpg['origin'] == 'japan').sample(50)\n",
        "# clst3 = mpg.where(mpg['origin'] == 'europe').sample(50)\n",
        "\n",
        "\n",
        "# # merge into one dataframe (concat, merge, join)\n",
        "# # https://pandas.pydata.org/docs/user_guide/merging.html\n",
        "# clst_df = pd.concat([clst1, clst2, clst3])\n",
        "# print(clst_df.shape)\n",
        "# clst_df.head()"
      ],
      "metadata": {
        "id": "PLBbTZKV7AtB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PLBbTZKV7AtB"
    },
    {
      "cell_type": "code",
      "source": [
        "# # weighted - using normal represents precentages of original population\n",
        "# mpg['origin'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "4yOYSUtenEaw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4yOYSUtenEaw"
    },
    {
      "cell_type": "code",
      "source": [
        "# # weighted possible solution based on percentage of each label for origin\n",
        "# weights = {'usa': mpg['origin'].value_counts(normalize=True)[0],\n",
        "#           'japan': mpg['origin'].value_counts(normalize=True)[1],\n",
        "#            'europe': mpg['origin'].value_counts(normalize=True)[2]}\n",
        "# mpg['weights'] = mpg['origin'].apply(lambda x: weights[x])\n",
        "# mpg.sample(n=300, weights='weights')['origin'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "gkaTey4m7P-7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gkaTey4m7P-7"
    },
    {
      "cell_type": "code",
      "source": [
        "# # stratified sampling by cylinder\n",
        "# clst_df.groupby(['cylinders', 'origin'])['origin'].count()"
      ],
      "metadata": {
        "id": "m7vWFDGNnGaI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "m7vWFDGNnGaI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}