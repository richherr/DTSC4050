{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMW+wiK/1hVOvF3MzgSUafK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC4050/blob/main/Week_11-Regression_II/Week_11_Assignment_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 11 Assignment Example\n",
        "\n",
        "Your Name\n"
      ],
      "metadata": {
        "id": "m0V2hcg6xygs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "This assignment is a continuation of the Week 11 discussion on data preparation and ways to use Python to automate redundant tasks.\n",
        "\n",
        "* Edit your name\n",
        "* Replace the Story with your story\n",
        "* Replace the variable names to fit the story\n",
        "* Review the notebook\n",
        "* Try to get everything to run\n",
        "* Troubleshoot if needed\n",
        "* Submit the shared link"
      ],
      "metadata": {
        "id": "IB7XJ0EePLDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - The Story and the Data"
      ],
      "metadata": {
        "id": "7TLB2JnjPaju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Data"
      ],
      "metadata": {
        "id": "WBewK3LoolEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# library to create fake data\n",
        "!pip install Faker -q"
      ],
      "metadata": {
        "id": "hWAgPcHYVrWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create fake demographics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.datasets import make_regression\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "output = []\n",
        "for x in range(100):\n",
        "    sex = np.random.choice(['egg', 'seed'], p=[0.5, 0.5])\n",
        "    output.append({\n",
        "        'categorical_1': sex,\n",
        "        'categorical_2': np.random.choice(['A', 'B', 'C']),\n",
        "        'name_1': fake.first_name_female() if sex == 'egg' else fake.first_name_male(),\n",
        "        'name_2': fake.last_name(),\n",
        "        'zip_code': fake.zipcode(),\n",
        "        'date': fake.date_of_birth(),\n",
        "        'location': fake.state_abbr()\n",
        "    })\n",
        "\n",
        "demographics = pd.DataFrame(output)\n",
        "\n",
        "def make_null(r, w):\n",
        "    if random.randint(0, 99) < w:\n",
        "        return np.nan\n",
        "    else:\n",
        "        return r\n",
        "\n",
        "# Generating features for linear regression with generic names\n",
        "features, target = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "generic_cols = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5']\n",
        "random.shuffle(generic_cols)\n",
        "df = pd.DataFrame(data=features, columns=generic_cols)\n",
        "df['target_variable'] = target\n",
        "\n",
        "# Introduce non-linearities and interactions\n",
        "df['feature_1_squared'] = df['feature_1']**2\n",
        "df['interaction_1_2'] = df['feature_1'] * df['feature_2']\n",
        "\n",
        "# Apply transformations and add noise\n",
        "df['target_variable'] = df['target_variable'] + np.random.normal(0, 5, 100)\n",
        "df['feature_4'] = df['feature_4'].apply(lambda x: abs(x) if x < 0 else x)\n",
        "\n",
        "# Add missing values\n",
        "for col in generic_cols:\n",
        "    df[col] = df[col].apply(make_null, args=(2,))\n",
        "\n",
        "df = pd.concat([df, demographics], axis=1)\n",
        "\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "w7Uw8hjrVfaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Story\n",
        "\n",
        "**The Great Martian Population Mystery**\n",
        "\n",
        "In the year 2342, humanity has established thriving colonies across Mars. However, the Martian Central Command has noticed a perplexing trend: the population growth rate of these colonies varies wildly. They have collected data on several factors, including:\n",
        "\n",
        "* **asteroid_impact:** The frequency and severity of nearby asteroid impacts.\n",
        "* **solar_flare_intensity:** The intensity of solar flares affecting the colony.\n",
        "* **alien_signal_strength:** The strength of mysterious alien signals detected.\n",
        "* **temporal_anomaly_index:** A measure of temporal anomalies observed in the region.\n",
        "* **cybernetic_enhancement_level:** The average level of cybernetic enhancements among the colonists.\n",
        "* **colony_population_growth:** The observed population growth rate.\n",
        "\n",
        "Additionally, they have demographic data on the colonists, including sex, brain wave patterns, names, zipcodes, birthdates, and state of origin (from Earth).\n",
        "\n",
        "Martian Central Command needs your help to build a linear regression model that can predict the colony population growth rate based on these factors. They suspect that some of the factors may have non-linear relationships or interact with each other. They also know that some data is missing.\n",
        "\n",
        "**Your Task:**\n",
        "\n",
        "1.  Analyze the provided `data_science_fiction.csv` dataset.\n",
        "2.  Clean and preprocess the data, handling missing values and potential outliers.\n",
        "3.  Build a linear regression model to predict `colony_population_growth`.\n",
        "4.  Evaluate the model's performance and interpret the coefficients.\n",
        "5.  Write a report explaining your findings and providing insights into the factors that influence Martian colony population growth.\n",
        "6. Create visualizations that help to explain your findings."
      ],
      "metadata": {
        "id": "YNFlpc-RTaWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Creative\n",
        "\n",
        "My story talks about colonies so the following code snippet creates some colony names. This is unique to my story. Do you need to create unique names for something? Ask your assistant for help. You have an example below."
      ],
      "metadata": {
        "id": "czo_I9P9brHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_colony_name(): # only relevant to example story\n",
        "    \"\"\"Generates a fake colony name with a sci-fi feel.\"\"\"\n",
        "    name_formats = [\n",
        "        \"Colony \" + fake.city_suffix() + \" \" + fake.word().capitalize(),\n",
        "        fake.word().capitalize() + \" \" + fake.word().capitalize() + \" Outpost\",\n",
        "        \"Sector \" + str(fake.random_int(min=1, max=100)) + \" \" + fake.word().capitalize(), # Convert the integer to a string using str()\n",
        "        fake.word().capitalize() + \" \" + \"Station \" + str(fake.random_int(min=1, max=50)), # Convert the integer to a string using str()\n",
        "        \"Terra \" + fake.word().capitalize(),\n",
        "        fake.word().capitalize() + \"-\" + fake.word().capitalize() + \" Base\",\n",
        "        fake.word().capitalize() + \" \" + \"Settlement\"\n",
        "    ]\n",
        "    return random.choice(name_formats)\n",
        "\n",
        "def add_colony_names_to_dataframe(df, num_colonies): # only relevant to example story\n",
        "    \"\"\"Adds a 'colony_name' column to a DataFrame.\"\"\"\n",
        "    colony_names = [fake_colony_name() for _ in range(num_colonies)]\n",
        "    df['colony_name'] = colony_names\n",
        "    return df\n",
        "\n",
        "# Add colony names to the DataFrame\n",
        "df = add_colony_names_to_dataframe(df, len(df)) # only relevant to example story"
      ],
      "metadata": {
        "id": "biKEzQdEbpJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename Columns to Fit Story"
      ],
      "metadata": {
        "id": "u3W3RXpHT95a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_columns(df, name_mapping):\n",
        "    \"\"\"\n",
        "    Renames columns in a DataFrame based on a provided mapping.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to rename columns in.\n",
        "        name_mapping (dict): A dictionary where keys are generic column names\n",
        "                              and values are the desired new column names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with renamed columns.\n",
        "    \"\"\"\n",
        "    return df.rename(columns=name_mapping)\n",
        "\n",
        "# Example Usage (Students would create their own name_mapping)\n",
        "example_name_mapping = {\n",
        "    'feature_1': 'asteroid_impact',\n",
        "    'feature_2': 'solar_flare_intensity',\n",
        "    'feature_3': 'alien_signal_strength',\n",
        "    'feature_4': 'temporal_anomaly_index',\n",
        "    'feature_5': 'cybernetic_enhancement_level',\n",
        "    'target_variable': 'colony_population_growth',\n",
        "    'feature_1_squared' : 'asteroid_impact_squared',\n",
        "    'interaction_1_2' : 'solar_flare_interaction',\n",
        "    'categorical_1' : 'sex',\n",
        "    'categorical_2' : 'brain_wave',\n",
        "    'name_1': 'given_name',\n",
        "    'name_2': 'surname',\n",
        "    'zip_code': 'zipcode',\n",
        "    'date': 'date_of_birth',\n",
        "    'location': 'state_of_origin'\n",
        "}\n",
        "\n",
        "# Add missing values to demographic data\n",
        "df['categorical_1'] = df['categorical_1'].apply(make_null, args=(5,))\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "df = rename_columns(df, example_name_mapping)\n",
        "\n",
        "df.to_csv('data_science_fiction.csv')\n",
        "\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uYj7pmkVTNMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - The Analysis"
      ],
      "metadata": {
        "id": "LcRZgw2mUOHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Prep Ideas\n",
        "\n",
        "* Missing Values\n",
        "* Categorical Encodeing\n",
        "* Duplicates\n",
        "* Scaling"
      ],
      "metadata": {
        "id": "-pOov8ycAk8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanations for Cleaning Techniques\n",
        "\n",
        "* Check out https://github.com/gitmystuff/DSChunks/blob/main/PrePy.ipynb for explanations"
      ],
      "metadata": {
        "id": "XnPaOYGZHUCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate X from y\n",
        "# REPLACE YOUR TARGET VARIABLE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Lasso, LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "X = df.drop(['colony_population_growth'], axis=1)\n",
        "y = df['colony_population_growth']\n",
        "\n",
        "X.info()\n"
      ],
      "metadata": {
        "id": "vS2nOB8WWKKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# missing values using bulky code example\n",
        "df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n",
        "dfx = X.copy()\n",
        "for feat in df.columns[df.isnull().sum() > 1]:\n",
        "  if feat in df_categorical_features:\n",
        "    dfx[feat] = df[feat].fillna(df[feat].mode()[0])\n",
        "  else:\n",
        "    if abs(df[feat].skew()) < .8:\n",
        "      dfx[feat] = df[feat].fillna(round(df[feat].mean(), 2))\n",
        "    else:\n",
        "      dfx[feat] = df[feat].fillna(df[feat].median())\n",
        "\n",
        "X = dfx.copy()\n",
        "# precaution for glitches in code example\n",
        "for col in X.select_dtypes(include=np.number).columns:\n",
        "    X[col] = X[col].fillna(X[col].mean())\n",
        "\n",
        "X.info()"
      ],
      "metadata": {
        "id": "e5JLUVaW17hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PrePy\n",
        "\n",
        "PrePy is an example of storing reusable chunks of code rather than copying and pasting chunks of bulky code. Check out the README to see some of the functions used for this example\n",
        "\n",
        "https://github.com/gitmystuff/preppy/tree/main"
      ],
      "metadata": {
        "id": "lhE79hmPK6gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/gitmystuff/preppy.git"
      ],
      "metadata": {
        "id": "UphxjdyeUNk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out your session storage folder to see that preppy has been cloned and is now usable."
      ],
      "metadata": {
        "id": "geeaQcW1KKPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preppy.version import __version__\n",
        "print(__version__)"
      ],
      "metadata": {
        "id": "VQ2lHgvlUNS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical encoding with less bulky code\n",
        "import preppy.utils as preppy\n",
        "\n",
        "X = preppy.functions.do_OHE(X)\n",
        "\n",
        "# adding duplicated for example\n",
        "dupes = X.loc[:7]\n",
        "X = pd.concat([X, dupes], axis=0)\n",
        "\n",
        "X.info()"
      ],
      "metadata": {
        "id": "Ha1QHYyQ11-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines\n",
        "\n",
        "Scikit-learn's pipeline is designed to streamline and automate machine learning workflows. It expects each step in the pipeline to adhere to a specific interface, ensuring consistent behavior and compatibility. Here's why custom classes are often needed:\n",
        "\n",
        "**1. Standardization:**\n",
        "\n",
        "* Scikit-learn operates on the principle of standardized transformers and estimators. This means that all components in a pipeline should have `fit()` and `transform()` methods (for transformers) or a `fit()` and `predict()` method (for estimators).\n",
        "* These methods provide a predictable way to interact with each component, regardless of its underlying functionality.\n",
        "* When you create a custom class that inherits from `BaseEstimator` and `TransformerMixin` (or `ClassifierMixin` or `RegressorMixin` for estimators), you are telling scikit-learn that your class follows this standard.\n",
        "\n",
        "**2. Seamless Integration:**\n",
        "\n",
        "* The pipeline's strength lies in its ability to chain together multiple steps, like data preprocessing, feature selection, and model training.\n",
        "* For this chaining to work smoothly, each step must provide a consistent input and output format.\n",
        "* Custom classes, when properly implemented, ensure that the data passed between pipeline steps is in the expected format (e.g., NumPy arrays or Pandas DataFrames).\n",
        "\n",
        "**3. Preventing Errors:**\n",
        "\n",
        "* Scikit-learn's pipeline handles details like cross-validation, grid search, and model persistence.\n",
        "* If you were to use functions or arbitrary code snippets directly within a pipeline, scikit-learn wouldn't be able to properly manage these operations.\n",
        "* Custom classes, with their defined methods, allow scikit-learn to correctly handle data transformations and model fitting across different pipeline stages.\n",
        "\n",
        "**4. Maintaining State:**\n",
        "\n",
        "* Some transformations or model fitting procedures require storing information learned from the training data (e.g., mean and standard deviation for scaling, learned model coefficients).\n",
        "* Classes can maintain this \"state\" as attributes (e.g., `self.mean_`, `self.coef_`).\n",
        "* This is important, so the transform method can use the data learned in the fit method. Functions, by default, do not retain state between calls.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "* Scikit-learn's pipeline relies on objects that have consistent methods and data formats.\n",
        "* Custom classes are used to wrap your custom operations, so they can interact with the pipeline in a way that scikit-learn understands. This makes the pipeline more robust, and less prone to errors.\n"
      ],
      "metadata": {
        "id": "dW3yAtZ3bJ4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of custom class to remove duplicate rows and columns\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class DuplicateRemover(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A simple transformer that removes duplicate rows and columns from a Pandas DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fits the transformer (no fitting needed in this case).\"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transforms the input data by removing duplicate rows and columns.\"\"\"\n",
        "        X = self.check_row_duplicates(X)\n",
        "        X = self.check_col_duplicates(X)\n",
        "        return X\n",
        "\n",
        "    def check_row_duplicates(self, X):\n",
        "        \"\"\"Removes duplicate rows.\"\"\"\n",
        "        return X.drop_duplicates()\n",
        "\n",
        "    def check_col_duplicates(self, X):\n",
        "        \"\"\"Removes duplicate columns.\"\"\"\n",
        "        duplicate_features = []\n",
        "        for i in range(0, len(X.columns)):\n",
        "            orig = X.columns[i]\n",
        "            for dupe in X.columns[i + 1:]:\n",
        "                if X[orig].equals(X[dupe]):\n",
        "                    duplicate_features.append(dupe)\n",
        "        if duplicate_features:\n",
        "            X = X.drop(duplicate_features, axis=1)\n",
        "        return X"
      ],
      "metadata": {
        "id": "D0QiLRh_5aTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. `BaseEstimator`**\n",
        "\n",
        "* **Purpose:**\n",
        "    * `BaseEstimator` is a base class in scikit-learn that provides a standard interface for all estimators (including transformers and models).\n",
        "    * It primarily ensures that your custom classes adhere to scikit-learn's conventions and can be used seamlessly within its ecosystem.\n",
        "* **Key Features:**\n",
        "    * **`get_params()`:** This method allows scikit-learn to retrieve the parameters of your estimator. It's crucial for functions like `GridSearchCV` and `Pipeline`, which need to inspect and manipulate the parameters.\n",
        "    * **`set_params()`:** This method allows scikit-learn to set the parameters of your estimator. It's used by `GridSearchCV` and `Pipeline` to configure the estimator during the search or pipeline execution.\n",
        "    * By inheriting from `BaseEstimator`, you gain these core functionalities without having to implement them yourself.\n",
        "* **Why It's Important:**\n",
        "    * Consistency: It enforces a consistent API, making your custom estimators compatible with scikit-learn's tools.\n",
        "    * Integration: It enables your estimators to work smoothly with pipelines, cross-validation, and hyperparameter tuning.\n",
        "\n",
        "**2. `TransformerMixin`**\n",
        "\n",
        "* **Purpose:**\n",
        "    * `TransformerMixin` is a mixin class specifically designed for transformers.\n",
        "    * It provides a default implementation of the `fit_transform()` method.\n",
        "* **Key Features:**\n",
        "    * **`fit_transform(X, y=None, **fit_params)`:** This method combines the `fit()` and `transform()` steps into a single call.\n",
        "        * The default implementation simply calls `self.fit(X, y, **fit_params)` followed by `self.transform(X)`.\n",
        "        * You can override `fit_transform()` if you need a more efficient or customized implementation.\n",
        "* **Why It's Important:**\n",
        "    * Convenience: It saves you from having to write the `fit_transform()` method explicitly in every transformer.\n",
        "    * Efficiency: While the default implementation works, you can optimize it for your specific transformer if needed.\n",
        "    * By inheriting from TransformerMixin, you are stating that your class will be able to transform data.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "* When you create a custom estimator (like a transformer or a model), you should inherit from `BaseEstimator` to ensure it integrates well with scikit-learn.\n",
        "* If your estimator is a transformer (i.e., it transforms data), you should also inherit from `TransformerMixin` to get the default `fit_transform()` implementation.\n",
        "\n",
        "By using these mixin classes, you make your custom code more robust, maintainable, and compatible with the scikit-learn ecosystem.\n"
      ],
      "metadata": {
        "id": "hlIAWS8BacHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the class without including it in a pipeline\n",
        "duplicate_remover = DuplicateRemover()\n",
        "X = duplicate_remover.fit_transform(X)\n",
        "\n",
        "X.info()"
      ],
      "metadata": {
        "id": "s0J5UOTw3i_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an example of how we could list several functions into a pipeline. Do not uncomment.\n",
        "pipeline = Pipeline([\n",
        "    # ('missingValues', preppy.classes.MissingValueImputer()),\n",
        "    # ('categoricalEncoding', preppy.functions.do_OHE()),\n",
        "    # ('duplicates', DuplicateRemover()),\n",
        "    ('constants', preppy.classes.ConstantAndSemiConstantRemover()),\n",
        "])\n",
        "\n",
        "X = pipeline.fit_transform(X)\n",
        "X.info()"
      ],
      "metadata": {
        "id": "h3dF5Et17Bu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Model Using Lasso\n",
        "\n",
        "Lasso (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that performs both variable selection and regularization. Here's a breakdown of what it does and why it's useful:\n",
        "\n",
        "**1. Linear Regression Foundation:**\n",
        "\n",
        "* Like standard linear regression, Lasso aims to find a linear relationship between a dependent variable (the target) and one or more independent variables (features).\n",
        "* It does this by estimating coefficients for each feature, which represent the strength and direction of the relationship with the target.\n",
        "\n",
        "**2. Regularization (Shrinkage):**\n",
        "\n",
        "* Lasso adds a penalty term to the standard linear regression cost function. This penalty is based on the *absolute values* of the coefficients.\n",
        "* The effect of this penalty is to \"shrink\" the coefficients towards zero.\n",
        "* The strength of this shrinkage is controlled by a parameter called \"alpha\" (or lambda). A higher alpha value leads to more shrinkage.\n",
        "\n",
        "**3. Variable Selection:**\n",
        "\n",
        "* A key feature of Lasso is that it can force some coefficients to become *exactly zero*.\n",
        "* When a coefficient is zero, it effectively removes the corresponding feature from the model.\n",
        "* This makes Lasso a powerful tool for feature selection, as it can automatically identify and discard irrelevant or redundant features.\n",
        "\n",
        "**4. How it Works (Simplified):**\n",
        "\n",
        "* Lasso minimizes a modified cost function:\n",
        "\n",
        "    ```\n",
        "    Cost = (Sum of Squared Errors) + alpha * (Sum of Absolute Values of Coefficients)\n",
        "    ```\n",
        "\n",
        "* The first part (Sum of Squared Errors) is the standard linear regression cost.\n",
        "* The second part is the L1 regularization term, which is the sum of the absolute values of the coefficients, multiplied by the alpha parameter.\n",
        "* Because of the nature of the absolute value, and the minimization process, the lasso can force some of the coefficient values to be exactly 0.\n",
        "\n",
        "**5. Why Lasso is Useful:**\n",
        "\n",
        "* **Feature Selection:** It automatically identifies and removes irrelevant features, leading to simpler and more interpretable models.\n",
        "* **Reduces Overfitting:** By shrinking coefficients, Lasso can prevent the model from overfitting the training data, especially when dealing with high-dimensional datasets (many features).\n",
        "* **Improved Prediction Accuracy:** In some cases, removing irrelevant features can improve the model's prediction accuracy on unseen data.\n",
        "* **Sparse Models:** Lasso produces \"sparse\" models, meaning that they have few non-zero coefficients. This can be beneficial for efficiency and interpretability.\n",
        "\n",
        "**6. When to Use Lasso:**\n",
        "\n",
        "* When you suspect that many of your features are irrelevant.\n",
        "* When you want to build a model with fewer features for better interpretability.\n",
        "* When you want to prevent overfitting in high-dimensional datasets.\n",
        "\n",
        "**In summary:** Lasso is a linear regression technique that adds a penalty to the coefficients, shrinking them towards zero and potentially setting some to exactly zero. This performs feature selection and regularization, leading to simpler, more interpretable, and potentially more accurate models.\n"
      ],
      "metadata": {
        "id": "SH8tzncLbW9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso, LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state=42)\n",
        "\n",
        "\n",
        "model = Lasso(alpha=1, fit_intercept=True)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "d = {'Feature': X_train.columns, 'Coef': model.coef_}\n",
        "pipe_df = pd.DataFrame(d)\n",
        "print(pipe_df)"
      ],
      "metadata": {
        "id": "9bDPkHEXLbmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove features with 0 coefficients for the heatmap\n",
        "mask = pd.Series(model.coef_ != 0, index=X_train.columns)\n",
        "\n",
        "# Apply the mask\n",
        "X_train_lassoed = X_train.loc[:, mask]\n",
        "X_test_lassoed = X_test.loc[:, mask]\n",
        "\n",
        "print(\"Original feature count:\", X_train.shape[1])\n",
        "print(\"Selected feature count:\", X_train_lassoed.shape[1])\n",
        "print(\"Selected features:\", X_train_lassoed.columns.tolist())"
      ],
      "metadata": {
        "id": "YNiglpWjEq76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation heat map\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# correlation matrix\n",
        "sns.set(style=\"white\")\n",
        "\n",
        "# compute the correlation matrix\n",
        "corr = X_train_lassoed.corr()\n",
        "\n",
        "# generate a mask for the upper triangle\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# set up the matplotlib figure\n",
        "# f, ax = plt.subplots()\n",
        "f = plt.figure(figsize=(16, 8))\n",
        "\n",
        "# generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
        "            annot=True, annot_kws={\"size\": 10});\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "xHc1JjdB8S7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "fQ6Yjml5baUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(f'Model Training Score (R^2): {model.score(X_train, y_train)}')\n",
        "print(f'Model Test Score (R^2): {model.score(X_test, y_test)}')\n",
        "print(f'Model Predictions Score: {r2_score(y_test, predictions)}')\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print()\n",
        "print(f'MAE: {mae}')\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "# Prediction DataFrame\n",
        "prediction_df = pd.DataFrame({'Actual': y_test, 'Predicted': predictions, 'Residuals': y_test - predictions})\n",
        "print('\\nPrediction DataFrame:')\n",
        "print(prediction_df.head())\n",
        "\n",
        "# Visualizations\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x='Actual', y='Predicted', data=prediction_df)\n",
        "plt.title('Actual vs. Predicted')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x='Predicted', y='Residuals', data=prediction_df)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title('Residual Plot')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZZ3De8HpHo_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**R-squared (Coefficient of Determination)**\n",
        "\n",
        "R-squared is a statistical measure that represents the proportion of the variance in the dependent variable (the variable you're trying to predict) that is predictable from the independent variables (the variables you're using to make the prediction). In simpler terms, it tells you how well your regression model \"fits\" the observed data.\n",
        "\n",
        "**Key Points:**\n",
        "\n",
        "* **Interpretation:**\n",
        "    * R-squared values range from 0 to 1.\n",
        "    * An R-squared of 1 indicates that the model perfectly predicts the dependent variable. All the variation in the dependent variable is explained by the independent variables.\n",
        "    * An R-squared of 0 indicates that the model does not explain any of the variation in the dependent variable. It's as bad as just using the mean of the dependent variable to make predictions.\n",
        "    * A value between 0 and 1 represents the proportion of the variance in the dependent variable that is explained by the model. For example, an R-squared of 0.75 means that 75% of the variance in the dependent variable is explained by the independent variables.\n",
        "* **Range:**\n",
        "    * The range of R-squared is generally **0 to 1**.\n",
        "    * In some rare cases where a model is very poorly fitted, it is possible to get a negative R-squared. This occurs when the model fits the data worse than a horizontal line.\n",
        "* **Limitations:**\n",
        "    * R-squared does not tell you whether the coefficients are statistically significant. It only measures how well the model fits the data.\n",
        "    * R-squared can increase simply by adding more independent variables to the model, even if those variables are not actually related to the dependent variable. This is why adjusted R-squared is often used.\n",
        "    * R-squared does not indicate if a model is biased.\n"
      ],
      "metadata": {
        "id": "xDcwonfINZSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`predictions`**:\n",
        "    * These are the values generated by the trained `model` when it's applied to the `X_test` dataset. They represent the model's estimations of the target variable for the test data.\n",
        "* **`model.score(X_train, y_train)`**:\n",
        "    * This calculates the coefficient of determination (R-squared, or R²) score for the model's performance on the training data. R-squared indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
        "* **`model.score(X_test, y_test)`**:\n",
        "    * This calculates the R-squared score for the model's performance on the testing data, providing an evaluation of how well the model generalizes to unseen data.\n",
        "* **`r2_score(y_test, predictions)`**:\n",
        "    * This function explicitly calculates the R-squared score by comparing the actual target values in `y_test` with the model's `predictions`.\n",
        "* **`mae` (mean_absolute_error)**:\n",
        "    * This is the average of the absolute differences between the predicted values and the actual values. It measures the average magnitude of the errors in the predictions, without considering their direction.\n",
        "* **`mse` (mean_squared_error)**:\n",
        "    * This is the average of the squared differences between the predicted values and the actual values. It gives more weight to larger errors due to the squaring.\n",
        "* **`rmse` (root mean squared error)**:\n",
        "    * This is the square root of the MSE. It provides an error metric that is in the same units as the target variable, making it easier to interpret.\n"
      ],
      "metadata": {
        "id": "ZkZUX5s2NXRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plots are designed to assess the performance of a regression model. Let's break down each subplot:\n",
        "\n",
        "**Left Subplot: Actual vs. Predicted**\n",
        "\n",
        "* **X-axis: Actual:** This axis represents the actual (or true) values of the target variable from your dataset.\n",
        "* **Y-axis: Predicted:** This axis represents the predicted values of the target variable as generated by your regression model.\n",
        "* **Scatter Plot:** Each point in the plot represents a data point from your dataset. The x-coordinate of the point is the actual value, and the y-coordinate is the predicted value.\n",
        "* **Interpretation:**\n",
        "    * **Ideal Scenario:** If the model were perfect, all points would fall exactly on a straight diagonal line (y = x), meaning the predicted values perfectly match the actual values.\n",
        "    * **Deviations from the Diagonal:** The further the points are from the diagonal line, the larger the prediction errors.\n",
        "    * **Patterns:** Look for any patterns in the scatter plot. For instance:\n",
        "        * **Curvature:** If the points form a curve instead of a straight line, it suggests that a linear model might not be appropriate.\n",
        "        * **Funnel Shape:** If the spread of the points increases or decreases as you move along the x-axis, it indicates heteroscedasticity (non-constant variance of errors).\n",
        "        * **Outliers:** Points that are far away from the general trend might be outliers, which could significantly affect the model's performance.\n",
        "\n",
        "**Right Subplot: Residual Plot**\n",
        "\n",
        "* **X-axis: Predicted:** This axis represents the predicted values of the target variable.\n",
        "* **Y-axis: Residuals:** This axis represents the residuals, which are the differences between the actual values and the predicted values (Residual = Actual - Predicted).\n",
        "* **Horizontal Line at Y=0:** This line represents zero residuals.\n",
        "* **Interpretation:**\n",
        "    * **Ideal Scenario:** In a good model, the residuals should be randomly scattered around zero, with no discernible pattern.\n",
        "    * **Patterns:** Look for any patterns in the residual plot. For instance:\n",
        "        * **Curvature:** If the residuals form a curve, it suggests that a linear model might not be appropriate.\n",
        "        * **Funnel Shape:** If the spread of the residuals increases or decreases as you move along the x-axis, it indicates heteroscedasticity.\n",
        "        * **Outliers:** Points that are far away from the zero line might be outliers.\n",
        "        * **Non-Random Patterns:** Any systematic patterns, such as a U-shape or a V-shape, indicate that the model is not capturing some underlying structure in the data.\n",
        "\n",
        "**Overall Interpretation of the Given Plot:**\n",
        "\n",
        "Based on the provided plot, here's a general assessment:\n",
        "\n",
        "* **Left Subplot (Actual vs. Predicted):** The points are somewhat scattered around the diagonal line, indicating that the model's predictions are not perfect. There are some noticeable deviations, suggesting that the model has room for improvement.\n",
        "* **Right Subplot (Residual Plot):** The residuals appear to be somewhat randomly scattered around zero, but there might be a slight tendency for the residuals to be more spread out at lower predicted values. This could indicate mild heteroscedasticity. There are also a couple of points that are relatively far from the zero line, which might be outliers.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The plots suggest that the regression model is capturing some of the underlying relationships in the data, but it's not a perfect fit. There are some deviations in the predicted values and potential issues with heteroscedasticity. Further analysis and potential model refinement might be necessary to improve the model's performance.\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "* **Check for Heteroscedasticity:** Use statistical tests (e.g., Breusch-Pagan test) to confirm the presence of heteroscedasticity. If confirmed, consider using transformations (e.g., logarithmic) or weighted least squares regression.\n",
        "* **Address Outliers:** Investigate the potential outliers to determine if they are genuine data points or errors. If they are errors, consider removing them. If they are genuine, consider using robust regression techniques.\n",
        "* **Consider Non-Linear Models:** If there are patterns in the residuals or the actual vs. predicted plot suggests non-linearity, explore non-linear regression models or feature engineering techniques.\n",
        "* **Evaluate Model Performance:** Use other evaluation metrics (e.g., R-squared, RMSE) to quantify the model's performance and compare it to other potential models.\n"
      ],
      "metadata": {
        "id": "dpH-VkpeGUDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OLS Model\n",
        "\n",
        "Statsmodels OLS (Ordinary Least Squares) is a statistical method used to estimate the unknown parameters in a linear regression model. It's a fundamental tool in econometrics and statistics, implemented in the Python statsmodels library.\n",
        "\n",
        "Here's a breakdown of what OLS is and how it works:\n",
        "\n",
        "**1. Linear Regression Model:**\n",
        "\n",
        "* OLS is designed for linear regression, which assumes a linear relationship between a dependent variable (the variable you're trying to predict) and one or more independent variables (the variables used to make the prediction).\n",
        "* The general form of a linear regression model is:\n",
        "\n",
        "    ```\n",
        "    y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ + ε\n",
        "    ```\n",
        "\n",
        "    * `y`: The dependent variable.\n",
        "    * `x₁, x₂, ..., xₚ`: The independent variables.\n",
        "    * `β₀, β₁, β₂, ..., βₚ`: The regression coefficients (parameters) that represent the relationship between the independent variables and the dependent variable.\n",
        "    * `ε`: The error term (or residual), which represents the difference between the actual value of `y` and the value predicted by the model.\n",
        "\n",
        "**2. Ordinary Least Squares (OLS) Estimation:**\n",
        "\n",
        "* OLS aims to find the values of the regression coefficients (βs) that minimize the sum of the squared residuals.\n",
        "* In other words, it finds the line (or hyperplane in multiple regression) that best fits the data points by minimizing the overall difference between the observed values and the predicted values.\n",
        "* \"Least squares\" refers to minimizing the sum of the squared errors:\n",
        "\n",
        "    ```\n",
        "    Σ(yᵢ - ŷᵢ)²\n",
        "    ```\n",
        "\n",
        "    * `yᵢ`: The actual value of the dependent variable for the i-th observation.\n",
        "    * `ŷᵢ`: The predicted value of the dependent variable for the i-th observation.\n",
        "\n",
        "**3. Assumptions of OLS:**\n",
        "\n",
        "* For OLS to provide unbiased and efficient estimates, several assumptions should be met:\n",
        "    * **Linearity:** The relationship between the dependent and independent variables is linear.\n",
        "    * **Independence:** The error terms are independent of each other.\n",
        "    * **Homoscedasticity:** The error terms have constant variance.\n",
        "    * **Normality:** The error terms are normally distributed.\n",
        "    * **No Multicollinearity:** The independent variables are not perfectly correlated with each other.\n",
        "\n",
        "**4. Statsmodels Implementation:**\n",
        "\n",
        "* The statsmodels library in Python provides a convenient way to perform OLS regression.\n",
        "* Key steps:\n",
        "    * Import the necessary libraries (`statsmodels.api` and `pandas`).\n",
        "    * Load your data into a Pandas DataFrame.\n",
        "    * Define the dependent and independent variables.\n",
        "    * Add a constant term to the independent variables using `statsmodels.api.add_constant()`.\n",
        "    * Create an OLS model using `statsmodels.api.OLS(y, X)`.\n",
        "    * Fit the model using `model.fit()`.\n",
        "    * Obtain the regression results using `results.summary()`.\n",
        "* The results summary provides valuable information, including:\n",
        "    * Coefficient estimates (βs).\n",
        "    * Standard errors of the coefficients.\n",
        "    * t-statistics and p-values for the coefficients.\n",
        "    * R-squared and adjusted R-squared.\n",
        "    * F-statistic and p-value for the overall model.\n",
        "    * Diagnostic information about the residuals.\n",
        "\n",
        "**5. Interpretation of Results:**\n",
        "\n",
        "* The coefficient estimates indicate the change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant.\n",
        "* The p-values help determine the statistical significance of the coefficients.\n",
        "* The R-squared value indicates the proportion of the variance in the dependent variable that is explained by the model.\n",
        "* The residual analysis is used to verify that the assumptions of OLS are met.\n",
        "\n",
        "In summary, Statsmodels OLS is a powerful tool for building and analyzing linear regression models in Python. It provides comprehensive statistical output and diagnostic information to help you understand the relationships between variables and assess the quality of your model.\n"
      ],
      "metadata": {
        "id": "269sbtWBbdcZ"
      }
    },
    {
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "X_train = X_train.copy()\n",
        "X_train.insert(0, 'const', 1)\n",
        "X_with_const = X_train\n",
        "\n",
        "X_with_const = X_with_const.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "ols_model = sm.OLS(y_train, X_with_const).fit()\n",
        "print(ols_model.summary())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fDMgVID_PqXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the p-values of the OLS summary with what Lasso selected\n",
        "X_train_lassoed.columns.tolist()"
      ],
      "metadata": {
        "id": "tWN3azf_FVRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R-Squared and Adj R-Squared\n",
        "\n",
        "Understanding the Relationship of R-Squared and Adjusted R-Square\n",
        "\n",
        "**R-Squared (Coefficient of Determination):**\n",
        "\n",
        "* **What it is:** R-squared measures the proportion of the variance in the dependent variable (the target) that is predictable from the independent variables (the features).\n",
        "* **How it's calculated:** It's essentially 1 minus the ratio of the residual sum of squares (the unexplained variance) to the total sum of squares (the total variance).\n",
        "* **What it tells you:** How well the model fits the data. A higher R-squared means the model explains a larger portion of the variance.\n",
        "* **Problem:** R-squared *always* increases (or at least stays the same) when you add more features to your model, even if those features don't actually improve the model's predictive power. This can lead to overfitting.\n",
        "\n",
        "**Adjusted R-Squared:**\n",
        "\n",
        "* **What it is:** Adjusted R-squared is a modified version of R-squared that penalizes you for adding unnecessary features to your model.\n",
        "* **How it's calculated:** It takes into account the number of features in the model and the number of data points. It essentially adjusts the R-squared value downward if you add features that don't significantly improve the model.\n",
        "* **What it tells you:** How well the model fits the data *while accounting for the complexity of the model*. It helps you determine if adding more features is actually improving the model or just overfitting.\n",
        "* **Why it's important:** It provides a more realistic measure of the model's performance, especially when you're dealing with models that have many features.\n",
        "\n",
        "**How They Are Used in Feature Selection:**\n",
        "\n",
        "1.  **R-squared as a Starting Point:**\n",
        "    * R-squared can give you a general idea of how well your model is performing. If it's very low, it might indicate that your features are not very predictive.\n",
        "\n",
        "2.  **Adjusted R-squared for Comparison:**\n",
        "    * When you're trying to select the best set of features, you should primarily rely on adjusted R-squared.\n",
        "    * As you add or remove features, compare the adjusted R-squared values of the resulting models.\n",
        "    * The model with the highest adjusted R-squared is generally considered the best, as it indicates the best balance between model fit and model complexity.\n",
        "\n",
        "3.  **Avoiding Overfitting:**\n",
        "    * Adjusted R-squared helps you avoid overfitting by penalizing the inclusion of irrelevant features.\n",
        "    * If you see a large increase in R-squared but only a small increase (or even a decrease) in adjusted R-squared when you add a feature, it's a sign that the feature is not adding much predictive power and might be leading to overfitting.\n",
        "\n",
        "4.  **Model Simplicity:**\n",
        "    * Adjusted R squared therefore encourages simpler models. If two models have similar R squared values, the model with the smaller number of features will generally have a higher adjusted R squared value.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "* Use R-squared for a basic assessment of model fit.\n",
        "* Use adjusted R-squared to compare models with different numbers of features and to prevent overfitting during feature selection. Adjusted R-squared is the more valuable metric when you are trying to select the most relevant features.\n"
      ],
      "metadata": {
        "id": "p9SAgscEOBjr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "009ce3f4"
      },
      "source": [
        "## OLS Regression Summary Explanation\n",
        "\n",
        "* Endog(enous): Similar to the dependent variable\n",
        "* Exog(enous): Similar to the independent variable\n",
        "* https://www.statisticshowto.com/endogenous-variable/\n",
        "* https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a\n",
        "\n",
        "### Model Info\n",
        "* Dep. Varialble: the response variable, dependent, outcome, etc.\n",
        "* Model: what model are we using (ordinary least squares) for the training\n",
        "* Method: how the parameters (coefficients) were calculated\n",
        "* No. Observations: the number of observations, rows... (n)\n",
        "* DF Residuals: degrees of freedom of the residuals\n",
        "* DF Model: number of parameters in the model excluding the constant if present\n",
        "* Covariance Type: deals with violations of assumptions\n",
        "\n",
        "### Goodness of Fit\n",
        "* R-Squared: coefficient of determination, how well the regression fits the data\n",
        "* Adj R-Squared: R-squared adjustment based on number of parameters and df residuals\n",
        "* F statistic: a measure of how significant the fit is\n",
        "* Prop F statistic: the probability that you would get the F stat given the null hypothesis\n",
        "* Log-Liklihood: can be used to compare the fit of different coefficients, the higher valur is better\n",
        "* AIC: Akaike Information Criterion is used to compare models, a lower score is better (doesn't address features, just the overall model)\n",
        "* BIC: Bayesian Information Criterion is similar to AIC but uses a higher penalty\n",
        "\n",
        "### Coefficients\n",
        "* coef: the estimated value of the coefficient\n",
        "* std error: the basic standard error of the estimate of the coefficient\n",
        "* t: the t-statistic value, how significant the coefficient is\n",
        "* P>|t|: the p-value, indicates a statistically significant relationship to the dependent variable if less than the confidence level, usually 0.05\n",
        "* 95% confidence interval: the lower and upper values\n",
        "\n",
        "### Statistical Tests\n",
        "* Skewness: A measure of the symmetry of the data about the mean\n",
        "* Kurtosis: A measure of the shape of the data\n",
        "* Omnibus: D'Angostino's test provides a combined test for the presence of skewness and kurtosis\n",
        "* Prob(Omnibus): probability of Omnibus\n",
        "* Jarque-Bera: Another test for skewness and kurtosis\n",
        "* Prob(Jarque-Bera): probability of Jarque-Bera\n",
        "* Durbin-Watson: A test for the presence of autocorrelation, if the errors aren't independent\n",
        "* Cond No: A test for multicollinearity"
      ]
    }
  ]
}