{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC4050/blob/main/Week_13-Trees/Week_13_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 13: Assignment\n",
        "\n",
        "Your Name"
      ],
      "metadata": {
        "id": "Ukv0gbj1f2R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "**Objective:** This assignment aims to deepen your understanding of tree-based methods for classification, specifically decision trees and random forests. You will gain practical experience in:\n",
        "\n",
        "* Loading and preparing a real-world dataset.\n",
        "* Building and evaluating decision tree models.\n",
        "* Understanding the impact of splitting criteria (entropy and Gini impurity).\n",
        "* Implementing and tuning random forest models using cross-validation and grid search.\n",
        "* Justifying hyperparameter choices based on theoretical understanding and empirical results.\n",
        "* Interpreting model performance.\n",
        "\n",
        "**Dataset:** You will be working with the dataset, a classic dataset for classification tasks. It contains measurements of three different penguin species (Adelie, Chinstrap, and Gentoo) on three islands in the Palmer Archipelago, Antarctica. The features include bill length, bill depth, flipper length, body mass, and sex. Your goal will be to build models that can accurately predict the penguin species based on these features.\n",
        "\n",
        "**Software:** You are expected to use Python with libraries such as `pandas` for data manipulation, `scikit-learn` for model building and evaluation, and potentially `matplotlib` or `seaborn` for visualization (optional)."
      ],
      "metadata": {
        "id": "aNPADVQJg9qW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Exploration and Preparation\n",
        "\n",
        "1.  **Load the Dataset:** Load the \"penguins\" dataset using a suitable library (e.g., `seaborn.load_dataset('penguins')` or by downloading it from an online repository).\n",
        "2.  **Data Inspection:**\n",
        "    * Display the first few rows of the dataset.\n",
        "    * Check for missing values and handle them appropriately (justify your chosen method).\n",
        "    * Get descriptive statistics of the numerical features.\n",
        "    * Identify the target variable (penguin species) and the feature variables.\n",
        "    * Check and delete variables that will lead to overfitting\n",
        "3.  **Feature Encoding:** If necessary, encode any categorical features (e.g., 'island', 'sex') into numerical representations suitable for the models. Explain the encoding method you chose.\n",
        "4.  **Data Splitting:** Split the dataset into training and testing sets (e.g., 80% train, 20% test) using a stratified split based on the penguin species. Explain why stratification is important here.\n",
        "\n"
      ],
      "metadata": {
        "id": "9UZr4PBNfvZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratification Explanation\n",
        "\n",
        "Do not delete the heading. Add your explanation below this line."
      ],
      "metadata": {
        "id": "UHW4tZXcheW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Decision Tree Classification\n",
        "\n",
        "1.  **Build Decision Tree Models:**\n",
        "    * Train two decision tree classifiers on the training data: one using the **entropy** criterion and the other using the **gini impurity** criterion. Use a `random_state` for reproducibility.\n",
        "    * Keep the initial trees relatively simple (e.g., limit `max_depth` to 5).\n",
        "2.  **Evaluate Performance:**\n",
        "    * Evaluate the performance of both decision tree models on the **test set** using appropriate classification metrics (e.g., accuracy, precision, recall, F1-score).\n",
        "    * Compare the performance of the two trees. Did the choice of splitting criterion significantly impact the results? Provide a brief explanation.\n",
        "3.  **Visualize the Decision Trees (Optional but Recommended):** Visualize the trained decision trees to understand the decision rules they have learned using the tree display functionality.\n",
        "4.  **Pruning (Optional):** Briefly explore the concept of pruning (e.g., using `ccp_alpha`). Train a pruned tree and compare its performance to the unpruned trees."
      ],
      "metadata": {
        "id": "TfIahl7zhcQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your Turn to Code"
      ],
      "metadata": {
        "id": "eiSCAoAdicD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here\n"
      ],
      "metadata": {
        "id": "o6JjajpQiKgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Default Random Forest Classification\n",
        "\n",
        "**Build a Random Forest Model:** Train a default Random Forest classifier on the training data using a `random_state`. Evaluate its performance on the test set using the same metrics as in Part 2.\n"
      ],
      "metadata": {
        "id": "rLefU96jiIy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your Turn to Code"
      ],
      "metadata": {
        "id": "cetogVd5kPud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "lPsVQ69PkKDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Random Forest Classification and Hyperparameter Tuning\n",
        "\n",
        "**Hyperparameter Tuning using Grid Search and Cross-Validation:**\n",
        "  * Define a hyperparameter grid for the Random Forest classifier. Use the following hyperparameters and the suggested limited values:\n",
        "  * Perform a **Grid Search** with **5-fold cross-validation** on the training data to find the best combination of hyperparameters. Explain the purpose of grid search and cross-validation in this context.\n",
        "  * Report the best hyperparameter settings found by the grid search.\n",
        "**Evaluate the Tuned Model:** Train a Random Forest classifier using the best hyperparameters found in the previous step and evaluate its performance on the **test set**. Compare its performance to the default Random Forest model from step 1. Did hyperparameter tuning improve the results?"
      ],
      "metadata": {
        "id": "l5_2INFGfvZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your Turn to Code"
      ],
      "metadata": {
        "id": "roQwUCcziyNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here\n",
        "hyperparameters = {\n",
        "        'n_estimators': [50, 200],\n",
        "        'criterion': ['entropy', 'gini'],\n",
        "        'max_depth': [3, 4],\n",
        "        'max_leaf_nodes': [7, 9],\n",
        "        'bootstrap': [True, False],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 3],\n",
        "        'max_features': ['sqrt', 0.5]\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "WPFUuJpCfvZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Justification of Hyperparameter Choices\n",
        "\n",
        "Provide a clear and concise justification for the range of values you included in your hyperparameter grid. Your justification should demonstrate your understanding of how these hyperparameters influence the model's behavior and potential for overfitting or underfitting.\n"
      ],
      "metadata": {
        "id": "gOhBv6wii-6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Justification\n",
        "\n",
        "Do not delete the heading. Add your explanation below this line."
      ],
      "metadata": {
        "id": "dCO6US16jLLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Submit Notebook\n",
        "\n",
        "Share and submit this notebook with appropriate permissions."
      ],
      "metadata": {
        "id": "8S9PFMsOlC82"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}