{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC4050/blob/main/Week_13-Trees/Tree_Based_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree Based Methods\n",
        "\n"
      ],
      "metadata": {
        "id": "nZ7FzC7CzDUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees\n",
        "\n",
        "A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Decision_tree\n",
        "\n",
        "                                      Chillin with some grillin?\n",
        "                                         Is it cloudy today?\n",
        "                                            (Root Node)\n",
        "                                                /\\\n",
        "                                           yes /  \\ no\n",
        "                                              /    \\\n",
        "                                         chance    grillin\n",
        "                                        of rain?      \n",
        "                                            /\\\n",
        "                                       yes /  \\ no\n",
        "                                          /    \\\n",
        "                                  no grillin     grillin\n",
        "                                   \n",
        "\n",
        "### Decision Tree Terms\n",
        "\n",
        "* Root Node: Represents our population from which two or more subsets are divided up\n",
        "* Decision Node: Sub-nodes divided into more sub-nodes based on a decision\n",
        "* Splitting: dividing a node into sub-nodes\n",
        "* Leaf / Terminal Node: Node with no children or sub-nodes\n",
        "* Branch: A section of a decision tree\n",
        "* Pruning: Eliminating branches and nodes\n",
        "* Parent / Children: The relationship between nodes and their sub-nodes\n",
        "\n",
        "**Decision Tree Classifier**\n",
        "\n",
        "* A   **Decision Tree Classifier** is a supervised machine learning algorithm used for classification tasks.\n",
        "* It works by recursively partitioning the dataset into smaller subsets based on a series of decision rules.\n",
        "* Each internal node in the tree represents a test on an attribute (feature).\n",
        "* Each branch represents the outcome of the test.\n",
        "* Each leaf node represents a class label (the prediction).\n",
        "* The algorithm learns these decision rules from the training data to predict the class of a new, unseen data point.\n",
        "* The goal is to create a tree that accurately classifies data points with minimal complexity.\n",
        "\n",
        "**CART (Classification and Regression Trees)**\n",
        "\n",
        "* **CART** is an algorithm that can be used for both classification and regression tasks.\n",
        "* For classification, it's a type of decision tree classifier.\n",
        "* CART uses the Gini Impurity as the criterion for splitting nodes. Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the subset.\n",
        "* CART produces binary trees (each node has at most two children).\n",
        "* It handles both categorical and numerical data.\n",
        "* CART is known for its ability to create complex trees that can overfit the training data, so pruning is often an important step.\n",
        "\n",
        "**ID3 (Iterative Dichotomiser 3)**\n",
        "\n",
        "* **ID3** is an earlier algorithm used for building decision tree classifiers.\n",
        "* ID3 uses Information Gain as the criterion for splitting nodes. Information Gain measures the reduction in entropy (uncertainty) about the target variable after splitting on an attribute.\n",
        "* ID3 typically works with categorical attributes.\n",
        "* It doesn't handle missing values directly.\n",
        "* ID3 can also produce trees that overfit, and pruning may be needed.\n",
        "\n",
        "**C4.5**\n",
        "\n",
        "* **C4.5** is an evolution of the ID3 algorithm.\n",
        "* It improves upon ID3 in several ways:\n",
        "    * C4.5 can handle both categorical and numerical data.\n",
        "    * It can deal with missing values.\n",
        "    * Instead of Information Gain, C4.5 uses Gain Ratio as the splitting criterion. Gain Ratio is a modification of Information Gain that reduces bias towards attributes with many values.\n",
        "    * C4.5 includes a pruning mechanism to reduce overfitting.\n",
        "\n",
        "**Key Differences Summarized**\n",
        "\n",
        "* **Splitting Criterion:** ID3 uses Information Gain, CART uses Gini Impurity, and C4.5 uses Gain Ratio.\n",
        "* **Data Types:** ID3 primarily handles categorical, while CART and C4.5 can handle both categorical and numerical.\n",
        "* **Missing Values:** C4.5 can handle missing values, while ID3 has no direct handling.\n",
        "* **Pruning:** C4.5 has a pruning mechanism, while ID3 and CART may require separate pruning.\n",
        "* **Tree Structure:** CART produces binary trees."
      ],
      "metadata": {
        "id": "ffVs7YehQ9tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Trees\n",
        "\n",
        "The idea behind building trees is, finding the best feature to split on, that generates the largest information gain or provides the least uncertainity in the following leafs. Think about a bag of mixed red and blue marbles and splitting the bag into two bags, one with red marbles and the other with blue marbles.\n",
        "\n",
        "https://www.kaggle.com/code/ma7555/decision-trees-information-gain-from-scratch\n",
        "\n",
        "Information gain is a decrease in entropy. Decision trees make use of information gain and entropy to determine which feature to split into nodes to get closer to predicting the target and also to determine when to stop splitting.\n",
        "\n",
        "https://www.askpython.com/python/examples/decision-trees\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/\n",
        "\n",
        "* Reduction in Variance\n",
        "* Gini Impurity\n",
        "* Entropy\n",
        "* Information Gain\n",
        "* Chi Square: See Week 11 - Chi Square"
      ],
      "metadata": {
        "id": "nAhciMXASABp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Functions"
      ],
      "metadata": {
        "id": "vvt52onUB8Ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entropy\n",
        "\n",
        "$H(S) = -p_1 \\log_2(p_1) - p_0 \\log_2(p_0)$\n",
        "\n",
        "The entropy of a set S, denoted as H(S), is calculated by taking the negative of the probability of the first class (p₁) multiplied by the base-2 logarithm of that probability, and then subtracting the probability of the second class (p₀) multiplied by the base-2 logarithm of that probability."
      ],
      "metadata": {
        "id": "HSIt1tmJCSoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np  # We'll need numpy for the logarithm base 2\n",
        "\n",
        "# def entropy(target_col):\n",
        "#     \"\"\"Calculates the entropy of a target variable with binary classes.\"\"\"\n",
        "#     elements, counts = np.unique(target_col, return_counts=True)\n",
        "#     probabilities = counts / len(target_col) # Calculates p1 and p0 (probabilities of each class)\n",
        "#     entropy_value = 0\n",
        "#     for prob in probabilities:\n",
        "#         if prob > 0:  # Avoid log(0) error\n",
        "#             entropy_value -= prob * np.log2(prob) # Calculates -p * log2(p) for each class\n",
        "\n",
        "#     return entropy_value # Returns H(S)"
      ],
      "metadata": {
        "id": "vJyu4w3wCG5_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Gain\n",
        "\n",
        "$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$\n",
        "\n",
        "The information gain of splitting a dataset S on an attribute A, denoted as IG(S, A), is determined by taking the initial entropy of the dataset S, represented as H(S), and subtracting the sum of the entropies of each subset Sv created by splitting on the different values (v) of attribute A. Each of these subset entropies, H(Sv), is weighted by the proportion of the number of elements in that subset (|Sv|) relative to the total number of elements in the original dataset (|S|)."
      ],
      "metadata": {
        "id": "bPE0s4iiCfAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def information_gain(data, split_attribute, target_attribute):\n",
        "#     \"\"\"Calculates the information gain of splitting on an attribute.\"\"\"\n",
        "#     # Total entropy of the target variable\n",
        "#     total_entropy = entropy(data[target_attribute]) # H(S)\n",
        "\n",
        "#     # Unique values in the split attribute\n",
        "#     values = data[split_attribute].unique()\n",
        "#     weighted_entropy = 0\n",
        "\n",
        "#     for value in values:\n",
        "#         subset = data[data[split_attribute] == value]\n",
        "#         subset_size = len(subset)\n",
        "#         if subset_size > 0:\n",
        "#             subset_entropy = entropy(subset[target_attribute]) # H(Sv)\n",
        "#             weighted_entropy += (subset_size / len(data)) * subset_entropy # (|Sv| / |S|) * H(Sv)\n",
        "\n",
        "#     # Information Gain\n",
        "#     gain = total_entropy - weighted_entropy # IG(S, A) = H(S) - Σ (|Sv| / |S|) * H(Sv)\n",
        "#     return gain"
      ],
      "metadata": {
        "id": "Xc5Mlb3hCQcG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gini Impurity\n",
        "\n",
        "$Gini(S) = 1 - (p_1^2 + p_0^2)$\n",
        "\n",
        "The Gini impurity of a set S, denoted as Gini(S), is calculated by taking one and subtracting the sum of the squared probabilities of each class. For a binary classification problem, this means subtracting the square of the probability of the first class (p₁) and the square of the probability of the second class (p₀) from one."
      ],
      "metadata": {
        "id": "uZC9mekzCwr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def gini(target_col):\n",
        "#     \"\"\"Calculates the Gini impurity of a target variable with binary classes.\"\"\"\n",
        "#     elements, counts = np.unique(target_col, return_counts=True)\n",
        "#     probabilities = counts / len(target_col) # Calculates p1 and p0 (probabilities of each class)\n",
        "#     gini_value = 1 - np.sum(probabilities**2) # Calculates 1 - (p1^2 + p0^2)\n",
        "#     return gini_value # Returns Gini(S)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "hxmIsLgSCwr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gini Gain (Reduction in Gini Impurity)\n",
        "\n",
        "$Gini\\_Gain(S, A) = Gini(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Gini(S_v)$\n",
        "\n",
        "The Gini gain achieved by splitting a dataset S on an attribute A, denoted as Gini\\_Gain(S, A), is calculated by taking the initial Gini impurity of the dataset S, represented as Gini(S), and subtracting the weighted sum of the Gini impurities of each subset Sv created by splitting on the different values (v) of attribute A. Each subset's Gini impurity, Gini(Sv), is weighted by the proportion of the number of elements in that subset (|Sv|) relative to the total number of elements in the original dataset (|S|)."
      ],
      "metadata": {
        "id": "f7Z-QMMhDHk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def gini_gain(data, split_attribute, target_attribute):\n",
        "#     \"\"\"Calculates the Gini gain of splitting on an attribute.\"\"\"\n",
        "#     # Total Gini impurity of the target variable\n",
        "#     total_gini = gini(data[target_attribute]) # Gini(S)\n",
        "\n",
        "#     # Unique values in the split attribute\n",
        "#     values = data[split_attribute].unique()\n",
        "#     weighted_gini = 0\n",
        "\n",
        "#     for value in values:\n",
        "#         subset = data[data[split_attribute] == value]\n",
        "#         subset_size = len(subset)\n",
        "#         if subset_size > 0:\n",
        "#             subset_gini = gini(subset[target_attribute]) # Gini(Sv)\n",
        "#             weighted_gini += (subset_size / len(data)) * subset_gini # (|Sv| / |S|) * Gini(Sv)\n",
        "\n",
        "#     # Gini Gain\n",
        "#     gain = total_gini - weighted_gini # Gini_Gain(S, A) = Gini(S) - Σ (|Sv| / |S|) * Gini(Sv)\n",
        "#     return gain"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "-6-SYI3TDHk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part One - Target with Two Classes"
      ],
      "metadata": {
        "id": "eSpGZb7LA_-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "**Our Goal:** To intuitively understand how Information Gain helps a decision tree choose the best feature to split on at a node, given a binary target variable.\n",
        "\n",
        "**Scenario:** Imagine we have a small dataset with one feature and a binary target variable (let's say, \"Buys Product\": Yes/No). We want to see how Information Gain helps us decide if this feature is useful for splitting."
      ],
      "metadata": {
        "id": "fvd8IV7KthQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setting up the Environment and Initial Data\n",
        "\n",
        "Let's start by importing the necessary library (which is just `pandas` for creating and manipulating our data) and creating our example dataset. I'll verbalize my thought process as I code."
      ],
      "metadata": {
        "id": "Zp20Ha2h3Sib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Goal: Create a small dataset to demonstrate Information Gain.\n",
        "# import pandas as pd\n",
        "\n",
        "# # Let's imagine we have data on whether a customer clicked on an ad ('Clicked')\n",
        "# # and whether they eventually bought the product ('Bought').\n",
        "# data = {'Clicked': ['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes'],\n",
        "#         'Bought': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes']}"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "45zjZIi5thQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #1"
      ],
      "metadata": {
        "id": "401H_rlNXSCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code along\n"
      ],
      "metadata": {
        "id": "pz3KVSRc3gBS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Any questions about the DataFrame.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* We import the `pandas` library, which is excellent for working with tabular data in Python. We give it the alias `pd` for convenience.\n",
        "* We create a dictionary called `data`  (key: value relationship) where the keys are our feature ('Clicked') and our target variable ('Bought'). The values are lists representing the observations for each.\n",
        "* We then use `pd.DataFrame(data)` to convert this dictionary into a DataFrame, which is a table-like structure.\n",
        "* Finally, we print the DataFrame to see our initial data."
      ],
      "metadata": {
        "id": "T0dGE6WKthQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Understanding Entropy\n",
        "\n",
        "Entropy measures the impurity or randomness in a set of data. For a binary target, it tells us how mixed the two classes are.\n",
        "\n",
        "The formula for Entropy (H) for a binary target with probabilities $p_1$ (probability of class 1) and $p_0$ (probability of class 0) is:\n",
        "\n",
        "$$H(S) = -p_1 \\log_2(p_1) - p_0 \\log_2(p_0)$$\n",
        "\n",
        "Let's write a Python function to calculate Entropy."
      ],
      "metadata": {
        "id": "KEUSVEh94Dlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #2"
      ],
      "metadata": {
        "id": "alUBdFIxXYLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recap of the data:**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {'Clicked': ['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes'],\n",
        "        'Bought': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes']}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "```\n",
        "\n",
        "**Calculating Entropy of 'Bought':**\n",
        "\n",
        "1.  **Count 'Yes' and 'No':**\n",
        "\n",
        "    * Number of 'Yes' (Bought): 4\n",
        "    * Number of 'No' (Not Bought): 4\n",
        "\n",
        "2.  **Calculate p1 and p0:**\n",
        "\n",
        "    * p1 (probability of 'Yes') = 4 / (4 + 4) = 0.5\n",
        "    * p0 (probability of 'No') = 4 / (4 + 4) = 0.5\n",
        "\n",
        "3.  **Calculate the logarithms:**\n",
        "\n",
        "    * log2(p1) = log2(0.5) = -1\n",
        "    * log2(p0) = log2(0.5) = -1\n",
        "\n",
        "4.  **Apply the Entropy formula:**\n",
        "\n",
        "    * H(S) = -p1 \\* log2(p1) - p0 \\* log2(p0)\n",
        "    * H(S) = -0.5 \\* (-1) - 0.5 \\* (-1)\n",
        "    * H(S) = 0.5 + 0.5\n",
        "    * H(S) = 1\n",
        "\n",
        "**Here's a summary:**\n",
        "\n",
        "* p1 (probability of 'Yes'): 0.5\n",
        "* p0 (probability of 'No'): 0.5\n",
        "* log2(p1): -1\n",
        "* log2(p0): -1\n",
        "* Entropy H(S): 1\n",
        "\n",
        "This shows that the 'Bought' column in our initial dataset has maximum entropy (1), indicating an equal proportion of 'Yes' and 'No' outcomes.': {initial_entropy:.4f}\")"
      ],
      "metadata": {
        "id": "pe_hqu9S6LnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Interpret the entropy value. A higher value indicates more impurity.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* We import the `numpy` library as `np` for numerical operations, specifically the base-2 logarithm (`np.log2`).\n",
        "* The `entropy` function takes a target column (a pandas Series) as input.\n",
        "* `np.unique(target_col, return_counts=True)` efficiently finds the unique values in the column and their counts.\n",
        "* We calculate the probabilities of each class by dividing the counts by the total number of observations.\n",
        "* We initialize `entropy_value` to 0 and then iterate through the probabilities. For each probability greater than 0, we add `-prob * np.log2(prob)` to the `entropy_value`. We handle the case where a probability is 0 to avoid a `log(0)` error.\n",
        "* Finally, we calculate and print the initial entropy of our 'Bought' column."
      ],
      "metadata": {
        "id": "aBOj-Rd0thQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **\"An entropy of 1 indicates a perfectly balanced or equally distributed class distribution in a binary target variable.\"**\n",
        "* **\"With an entropy of 1, there is maximum uncertainty about the outcome of the target variable.\"**\n",
        "* **\"An entropy of 1 signifies that the data is maximally impure with respect to the binary target, as both classes have an equal probability of occurrence.\"**\n",
        "* **\"In the context of decision trees, a node with an entropy of 1 provides the least information about which class a data point belongs to.\"**"
      ],
      "metadata": {
        "id": "nVtlJBx769_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Understanding Information Gain\n",
        "\n",
        "Information Gain (IG) measures the reduction in entropy achieved after splitting the dataset on a particular attribute. It tells us how much \"purity\" we gain by knowing the value of a feature.\n",
        "\n",
        "The formula for Information Gain (IG) of an attribute $A$ on a dataset $S$ is:\n",
        "\n",
        "$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $H(S)$ is the entropy of the original dataset $S$.\n",
        "* $Values(A)$ are the unique values of the attribute $A$.\n",
        "* $S_v$ is the subset of $S$ where attribute $A$ has the value $v$.\n",
        "* $|S_v|$ is the number of elements in $S_v$.\n",
        "* $|S|$ is the total number of elements in $S$.\n",
        "* $H(S_v)$ is the entropy of the target variable in the subset $S_v$.\n",
        "\n",
        "Let's calculate the Information Gain for our 'Clicked' feature."
      ],
      "metadata": {
        "id": "gU5XVK-P7E-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #3"
      ],
      "metadata": {
        "id": "mqC2HQT7Xg3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "metadata": {
        "id": "N0OdqLqu9Tyu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. A higher Information Gain suggests a better split.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* The `information_gain` function takes the data, the attribute to split on (`split_attribute`), and the target attribute (`target_attribute`) as input.\n",
        "* First, it calculates the `total_entropy` of the target variable in the original dataset using our `entropy` function.\n",
        "* It then gets the unique values of the `split_attribute`.\n",
        "* We iterate through each unique value of the split attribute:\n",
        "    * We create a `subset` of the data containing only the rows where the `split_attribute` equals the current `value`.\n",
        "    * We calculate the size of this `subset`.\n",
        "    * If the subset is not empty, we calculate the entropy of the `target_attribute` within this `subset`.\n",
        "    * We then add the weighted entropy of this subset to the `weighted_entropy`. The weight is the proportion of observations in the subset compared to the total dataset size.\n",
        "* Finally, the `information gain` is calculated as the difference between the `total_entropy` and the `weighted_entropy`."
      ],
      "metadata": {
        "id": "Y5CUQgeVthQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Formula:**\n",
        "\n",
        "$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $IG(S, A)$: Information Gain of splitting dataset $S$ on attribute $A$.\n",
        "* $H(S)$: Entropy of the target variable in the original dataset $S$.\n",
        "* $Values(A)$: The unique values of attribute $A$.\n",
        "* $v$: A specific value within $Values(A)$.\n",
        "* $S_v$: The subset of $S$ where attribute $A$ has the value $v$.\n",
        "* $|S_v|$: The number of instances in the subset $S_v$.\n",
        "* $|S|$: The total number of instances in the original dataset $S$.\n",
        "* $H(S_v)$: Entropy of the target variable in the subset $S_v$.\n",
        "\n",
        "**Our Dataset:**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {'Clicked': ['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes'],\n",
        "        'Bought': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes']}\n",
        "df = pd.DataFrame(data)\n",
        "```\n",
        "\n",
        "**Let's calculate the Information Gain for splitting on the attribute 'Clicked' (so, $A$ = 'Clicked'):**\n",
        "\n",
        "1.  **$H(S)$ (Entropy of the target 'Bought' in the original dataset):**\n",
        "    We calculated this earlier: $H(S) = 1$\n",
        "\n",
        "2.  **$Values(A)$ (Unique values of 'Clicked'):**\n",
        "    $Values(\\text{'Clicked'}) = \\{\\text{'Yes'}, \\text{'No'}\\}$\n",
        "\n",
        "3.  **For $v = \\text{'Yes'}$:**\n",
        "    * $S_{\\text{'Yes'}}$: The subset where 'Clicked' is 'Yes'.\n",
        "        ```\n",
        "          Clicked Bought\n",
        "        0     Yes    Yes\n",
        "        2     Yes    Yes\n",
        "        4     Yes    Yes\n",
        "        5     Yes     No\n",
        "        7     Yes    Yes\n",
        "        ```\n",
        "    * $|S_{\\text{'Yes'}}| = 5$\n",
        "    * $|S| = 8$\n",
        "    * $H(S_{\\text{'Yes'}})$: Entropy of 'Bought' where 'Clicked' is 'Yes'.\n",
        "        * 'Yes' count: 4\n",
        "        * 'No' count: 1\n",
        "        * Total: 5\n",
        "        * $p_{\\text{'Yes'}} = 4/5 = 0.8$\n",
        "        * $p_{\\text{'No'}} = 1/5 = 0.2$\n",
        "        * $H(S_{\\text{'Yes'}}) = - (0.8 \\log_2(0.8) + 0.2 \\log_2(0.2)) \\approx - (0.8 * -0.322 + 0.2 * -2.322) \\approx 0.258 + 0.464 \\approx 0.722$\n",
        "\n",
        "4.  **For $v = \\text{'No'}$:**\n",
        "    * $S_{\\text{'No'}}$: The subset where 'Clicked' is 'No'.\n",
        "        ```\n",
        "          Clicked Bought\n",
        "        1      No     No\n",
        "        3      No     No\n",
        "        6      No     No\n",
        "        ```\n",
        "    * $|S_{\\text{'No'}}| = 3$\n",
        "    * $|S| = 8$\n",
        "    * $H(S_{\\text{'No'}})$: Entropy of 'Bought' where 'Clicked' is 'No'.\n",
        "        * 'Yes' count: 0\n",
        "        * 'No' count: 3\n",
        "        * Total: 3\n",
        "        * $p_{\\text{'Yes'}} = 0/3 = 0$\n",
        "        * $p_{\\text{'No'}} = 3/3 = 1$\n",
        "        * $H(S_{\\text{'No'}}) = - (0 \\log_2(0) + 1 \\log_2(1))$\n",
        "            * Note: We typically treat $0 \\log_2(0)$ as 0 by convention in information theory.\n",
        "        * $H(S_{\\text{'No'}}) = - (0 + 1 * 0) = 0$\n",
        "\n",
        "5.  **Calculate the summation part:**\n",
        "    $$\\sum_{v \\in Values(\\text{'Clicked'})} \\frac{|S_v|}{|S|} H(S_v) = \\left( \\frac{|S_{\\text{'Yes'}}|}{8} H(S_{\\text{'Yes'}}) \\right) + \\left( \\frac{|S_{\\text{'No'}}|}{8} H(S_{\\text{'No'}}) \\right)$$\n",
        "    $$= \\left( \\frac{5}{8} * 0.722 \\right) + \\left( \\frac{3}{8} * 0 \\right)$$\n",
        "    $$\\approx 0.451 + 0 = 0.451$$\n",
        "\n",
        "6.  **Calculate Information Gain:**\n",
        "    $$IG(S, \\text{'Clicked'}) = H(S) - \\sum_{v \\in Values(\\text{'Clicked'})} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "    $$IG(S, \\text{'Clicked'}) \\approx 1 - 0.451 \\approx 0.549$$\n",
        "\n",
        "**Therefore, for splitting on the 'Clicked' attribute, the values in the formula are:**\n",
        "\n",
        "* $H(S) = 1$\n",
        "* $Values(A) = \\{\\text{'Yes'}, \\text{'No'}\\}$\n",
        "* For $v = \\text{'Yes'}$:\n",
        "    * $\\frac{|S_v|}{|S|} = \\frac{5}{8} = 0.625$\n",
        "    * $H(S_v) \\approx 0.722$\n",
        "* For $v = \\text{'No'}$:\n",
        "    * $\\frac{|S_v|}{|S|} = \\frac{3}{8} = 0.375$\n",
        "    * $H(S_v) = 0$\n",
        "* $\\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v) \\approx 0.451$\n",
        "* $IG(S, \\text{'Clicked'}) \\approx 0.549$\n",
        "\n",
        "By performing similar calculations for other features (if we had them), we could determine which feature provides the highest Information Gain and should be chosen for the first split in our decision tree."
      ],
      "metadata": {
        "id": "B6u6gpW59Pon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Interpreting the Result\n",
        "\n",
        "The Information Gain value tells us how much the entropy (impurity) of the 'Bought' variable is reduced when we split the data based on whether a customer 'Clicked' on the ad or not. A higher Information Gain indicates that this split is more effective in separating the classes of the target variable.\n",
        "\n",
        "**(Does 'Clicked' seem like a good predictor of 'Bought' based on Information Gain?)**\n",
        "\n"
      ],
      "metadata": {
        "id": "CC3VI3699gh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, based on the Information Gain of approximately 0.549, 'Clicked' seems like a reasonably good predictor of 'Bought' in this dataset.**\n",
        "\n",
        "Here's why:\n",
        "\n",
        "* **Reduction in Uncertainty:** The Information Gain value represents the reduction in the entropy (uncertainty) of the 'Bought' variable after splitting the dataset based on the 'Clicked' feature. An Information Gain of 0.549 means that by knowing whether a customer clicked, we reduce the uncertainty about whether they bought the product by about 0.549 bits.\n",
        "\n",
        "* **Moving Towards Purity:** The entropy of the 'Bought' variable in the original dataset was 1 (maximum impurity, even split). After splitting on 'Clicked', the weighted average entropy of the resulting subsets is approximately 0.451 (1 - 0.549). This decrease in entropy indicates that the subsets created by the split on 'Clicked' are more \"pure\" with respect to the 'Bought' variable than the original dataset.\n",
        "\n",
        "    * When 'Clicked' is 'Yes', the probability of 'Bought' being 'Yes' is 0.8 (80%), which is higher than the original 0.5.\n",
        "    * When 'Clicked' is 'No', the probability of 'Bought' being 'No' is 1.0 (100%), indicating a perfectly pure subset.\n",
        "\n",
        "* **Information Value:** A non-zero Information Gain implies that the 'Clicked' feature provides some information that helps in distinguishing between customers who bought and those who didn't. A higher Information Gain generally suggests a more informative feature for the split. While 0.549 isn't the absolute maximum possible (which is 1 for a binary target), it's a significant reduction from the initial entropy, suggesting that 'Clicked' is a useful feature for making this prediction.\n",
        "\n",
        "**In contrast:**\n",
        "\n",
        "* An Information Gain close to 0 would suggest that splitting on that feature doesn't significantly reduce the uncertainty about the target variable, making it a poor predictor for that split.\n",
        "* An Information Gain of 1 (in a binary case) would indicate a perfect separation of the target classes based on the feature.\n",
        "\n",
        "**Therefore, the Information Gain of 0.549 suggests that 'Clicked' is a valuable feature for the first split in a decision tree aimed at predicting 'Bought'. It helps to create more homogeneous groups with respect to the target variable.**"
      ],
      "metadata": {
        "id": "3QJVNgM2-aYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Gini Impurity\n",
        "\n",
        "Now let's move on to Gini Impurity. Gini Impurity is another measure of impurity used in decision trees, especially in algorithms like CART (Classification and Regression Trees). For a binary target with probabilities $p_1$ and $p_0$, the Gini Impurity is:\n",
        "\n",
        "$$Gini(S) = 1 - (p_1^2 + p_0^2)$$"
      ],
      "metadata": {
        "id": "oM7JB89Y-gEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #4"
      ],
      "metadata": {
        "id": "1OrJwFJJXvIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "metadata": {
        "id": "gIY8mtic-_WY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Compare the Gini value to the Entropy value – they both represent impurity.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* The `gini` function takes a target column as input.\n",
        "* Similar to the `entropy` function, it calculates the probabilities of each class.\n",
        "* The Gini Impurity is then calculated using the formula $1 - \\sum_{i} p_i^2$, where $p_i$ is the probability of class $i$. For our binary case, this simplifies to $1 - (p_1^2 + p_0^2)$."
      ],
      "metadata": {
        "id": "DkipM3vathQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Gini Gain (Reduction in Gini Impurity)\n",
        "\n",
        "Similar to Information Gain, we can calculate the reduction in Gini Impurity after a split. The feature with the largest Gini Gain would be chosen for the split.\n",
        "\n",
        "$$Gini\\_Gain(S, A) = Gini(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Gini(S_v)$$"
      ],
      "metadata": {
        "id": "IxxF3MdC_Os0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #5"
      ],
      "metadata": {
        "id": "UxjW_aLVX4D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "m0UFdk56thQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. How does the Gini Gain value relate to the Information Gain.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* The `gini_gain` function mirrors the structure of the `information_gain` function.\n",
        "* It calculates the initial Gini impurity of the target variable.\n",
        "* It then iterates through the unique values of the split attribute, calculates the Gini impurity of the target within each subset, and weights it by the subset size.\n",
        "* The Gini Gain is the difference between the initial Gini impurity and the weighted Gini impurity after the split.\n"
      ],
      "metadata": {
        "id": "e6ylTGXrthQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 7: Entropy (Revisited with a Slightly Different Perspective)\n",
        "\n",
        "Finally, let's briefly revisit Entropy. You've already seen the formula. The key takeaway for splitting is that we want to choose a split that *reduces* the entropy the most (which is what Information Gain measures). A node with low entropy (closer to 0) is more \"pure,\" meaning it predominantly contains instances of a single class.\n",
        "\n",
        "**(Reiterate the meaning of Entropy in terms of uncertainty or randomness.)**"
      ],
      "metadata": {
        "id": "YV0hKy10_t0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Demonstration of Choosing the First Split\n",
        "\n",
        "Let's imagine we had another feature in our dataset, say 'Age Group' (e.g., 'Young', 'Old'). We would calculate the Information Gain (or Gini Gain) for both 'Clicked' and 'Age Group' and choose the feature with the higher gain to be the root node of our decision tree."
      ],
      "metadata": {
        "id": "ipomGrIl_7UR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "                              Is Clicked?\n",
        "                                  / \\\n",
        "                             yes /   \\ no\n",
        "                                /     \\\n",
        "                              Age    No Buy\n",
        "                              / \\\n",
        "                       Young /   \\ Old\n",
        "                            /     \\\n",
        "                         Buy     No Buy\n",
        "</pre>"
      ],
      "metadata": {
        "id": "oFnnxf1W0wWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Let's add a hypothetical 'Age Group' feature\n",
        "# df['Age Group'] = ['Young', 'Old', 'Young', 'Young', 'Old', 'Young', 'Old', 'Old']\n",
        "# print(\"\\nDataset with 'Age Group':\")\n",
        "# print(df)\n",
        "\n",
        "# # Calculate Information Gain for 'Age Group'\n",
        "# gain_age_group = information_gain(df, 'Age Group', 'Bought')\n",
        "# print(f\"Information Gain of splitting on 'Age Group': {gain_age_group:.4f}\")\n",
        "\n",
        "# # Compare the Information Gains\n",
        "# print(\"\\nComparing Information Gains:\")\n",
        "# print(f\"Information Gain ('Clicked'): {gain_clicked:.4f}\")\n",
        "# print(f\"Information Gain ('Age Group'): {gain_age_group:.4f}\")\n",
        "\n",
        "# # Based on the higher Information Gain, the first split would be on the feature with the larger value.\n",
        "# if gain_clicked > gain_age_group:\n",
        "#     print(\"\\n'Clicked' would be chosen as the first splitting node based on Information Gain.\")\n",
        "# else:\n",
        "#     print(\"\\n'Age Group' would be chosen as the first splitting node based on Information Gain.\")"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "fLXpWkptthQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Why does one feature have a higher Information Gain than another based on how well it separates the target classes.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* We add a new feature 'Age Group' to our DataFrame.\n",
        "* We calculate the Information Gain for splitting on 'Age Group' using our `information_gain` function.\n",
        "* We then compare the Information Gain of 'Clicked' and 'Age Group' and print which feature would be chosen as the root node based on this metric."
      ],
      "metadata": {
        "id": "SeaWsu76thQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Pruning (Brief Introduction)\n",
        "\n",
        "After a tree is built, it might overfit the training data. Pruning is a technique to reduce the size of the tree by removing parts that do not provide significant predictive power on unseen data. This helps to improve the generalization ability of the tree.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "By following along with this code and the explanations, students should gain a more intuitive understanding of:\n",
        "\n",
        "* How Entropy measures the impurity of a node.\n",
        "* How Information Gain quantifies the reduction in entropy after a split.\n",
        "* How Gini Impurity provides another measure of impurity.\n",
        "* How Decision Trees use these metrics to decide which feature to split on at each node.\n",
        "\n",
        "Remember to encourage questions throughout the lesson and provide opportunities for students to experiment with the code (e.g., by changing the initial data). This active engagement will reinforce their learning."
      ],
      "metadata": {
        "id": "wDJS8Fk7AUzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Procedural Summary\n",
        "\n",
        "1.  **Start with the entropy of the target:**\n",
        "    * This is the initial step. You need to quantify the impurity or randomness of your target variable in the entire dataset *before* making any splits. This initial entropy serves as the baseline.\n",
        "\n",
        "2.  **With this entropy, find the information gain for each feature:**\n",
        "    * For each feature in your dataset, you consider it as a potential splitting attribute.\n",
        "    * For each feature, you calculate how much the entropy of the target variable would be *reduced* if you were to split the dataset based on the values of that feature.\n",
        "    * This reduction in entropy is what we call the **Information Gain**. It measures how much \"information\" a feature provides about the target variable.\n",
        "\n",
        "3.  **Split on the feature with the highest information gain:**\n",
        "    * The feature that yields the largest Information Gain is considered the most effective at separating the classes of the target variable.\n",
        "    * Therefore, the decision tree algorithm chooses this feature to perform the split at the current node (initially the root node).\n",
        "\n",
        "After the first split, the process is repeated recursively for each of the resulting subsets (branches):\n",
        "\n",
        "* For each subset, you recalculate the entropy of the target variable within that subset.\n",
        "* Then, you calculate the Information Gain for all the *remaining* features (those not already used for splitting in that branch).\n",
        "* You choose the feature with the highest Information Gain to split that subset further.\n",
        "* This process continues until a stopping condition is met (e.g., a node becomes pure (entropy is 0), a maximum tree depth is reached, or the Information Gain from further splitting is below a certain threshold).\n",
        "\n",
        "**In summary, this procedure describes the core logic of the greedy approach used by many decision tree algorithms (like ID3 and C4.5) that rely on Information Gain for feature selection.**"
      ],
      "metadata": {
        "id": "ygd4WqvK8BEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part Two - Target with Three Classes"
      ],
      "metadata": {
        "id": "fCvh3ujGzYGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "**Our Goal:** To understand how Information Gain, Gini Impurity, and Entropy guide the splitting of a decision tree when the target variable has three distinct classes.\n",
        "\n",
        "**Scenario:** Imagine we have a dataset about different types of fruits based on their color, size, shape, and texture, and our target variable is the fruit type (Apple, Banana, Orange)."
      ],
      "metadata": {
        "id": "EJaMs7jLxF-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setting up the Environment and Initial Data"
      ],
      "metadata": {
        "id": "_VfAMuWxBVn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Our fruit dataset\n",
        "# data = {'Color': ['Red', 'Yellow', 'Orange', 'Red', 'Yellow', 'Orange', 'Red', 'Yellow', 'Orange', 'Red', 'Yellow', 'Orange'],\n",
        "#         'Size': ['Large', 'Medium', 'Medium', 'Large', 'Small', 'Large', 'Medium', 'Small', 'Medium', 'Large', 'Medium', 'Small'],\n",
        "#         'Shape': ['Round', 'Elongated', 'Round', 'Round', 'Elongated', 'Round', 'Round', 'Elongated', 'Round', 'Round', 'Elongated', 'Round'],\n",
        "#         'Texture': ['Smooth', 'Smooth', 'Rough', 'Smooth', 'Smooth', 'Rough', 'Smooth', 'Smooth', 'Rough', 'Smooth', 'Smooth', 'Rough'],\n",
        "#         'Fruit': ['Apple', 'Banana', 'Orange', 'Apple', 'Banana', 'Orange', 'Apple', 'Banana', 'Orange', 'Apple', 'Banana', 'Orange']}"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "id": "nIfL1K8kxF-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #6"
      ],
      "metadata": {
        "id": "ypzEPTOTYCQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "metadata": {
        "id": "wk6n-wyyBwmR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Think about the features and the target variable.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* We import `pandas` for data manipulation and `numpy` for numerical operations.\n",
        "* We create a dictionary `data` representing our fruit dataset with four features ('Color', 'Size', 'Shape', 'Texture') and the target variable 'Fruit' (with three classes: 'Apple', 'Banana', 'Orange').\n",
        "* We convert this dictionary into a pandas DataFrame.\n",
        "* We print the DataFrame to see our initial data."
      ],
      "metadata": {
        "id": "TwFm78X0xF-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Understanding Entropy (Multi-Class)\n",
        "\n",
        "The formula for Entropy (H) for a multi-class target variable with probabilities $p_1, p_2, ..., p_n$ for each of the $n$ classes is:\n",
        "\n",
        "$$H(S) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)$$\n",
        "\n",
        "Our `entropy` function from before will actually work directly for the multi-class case! Let's recalculate the initial entropy of our 'Fruit' column."
      ],
      "metadata": {
        "id": "j8Yb2AGnBgSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #7"
      ],
      "metadata": {
        "id": "K4wGsdf9YH7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "id": "rySLWAo2xF-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Interpret the entropy value in the context of three classes. A higher value still indicates more impurity or a more even distribution of the fruit types.)**"
      ],
      "metadata": {
        "id": "wL3mBsfQxF-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Information Gain (Multi-Class)\n",
        "\n",
        "The formula for Information Gain remains the same, but now the entropy calculations within the subsets will also consider the three classes of the 'Fruit' variable.\n",
        "\n",
        "$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "\n",
        "Let's calculate the Information Gain for each of our features ('Color', 'Size', 'Shape', 'Texture')."
      ],
      "metadata": {
        "id": "8sjyoM5JEBrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #8"
      ],
      "metadata": {
        "id": "uGW_nkBNYNXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "id": "KGcy7zkAxF-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Which feature seems most promising for the first split based on these values.)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* We iterate through each of our features.\n",
        "* For each feature, we use our `information_gain` function (which we defined earlier and works for multi-class targets) to calculate how much the entropy of 'Fruit' is reduced by splitting on that feature.\n",
        "* The output shows the Information Gain for each potential first split. The feature with the highest Information Gain would be chosen as the root node."
      ],
      "metadata": {
        "id": "XvdXES0FxF-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Understanding Gini Impurity (Multi-Class)\n",
        "\n",
        "For a multi-class target with probabilities $p_1, p_2, ..., p_n$, the Gini Impurity is:\n",
        "\n",
        "$$Gini(S) = 1 - \\sum_{i=1}^{n} p_i^2$$\n",
        "\n",
        "Our `gini` function from before also works directly for the multi-class case! Let's calculate the initial Gini Impurity of our 'Fruit' column."
      ],
      "metadata": {
        "id": "4fkp-lHIEYDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #9"
      ],
      "metadata": {
        "id": "JH9HBie0YS72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "zAON9tAhxF-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Compare the Gini value to the entropy value. Both represent impurity, but they are calculated differently.)**"
      ],
      "metadata": {
        "id": "azjMJW8GxF-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Gini Gain (Multi-Class)\n",
        "\n",
        "The formula for Gini Gain remains the same:\n",
        "\n",
        "$$Gini\\_Gain(S, A) = Gini(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Gini(S_v)$$\n",
        "\n",
        "Let's calculate the Gini Gain for each of our features."
      ],
      "metadata": {
        "id": "TrHLVbInEpyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along #10"
      ],
      "metadata": {
        "id": "oET5_2GLYYmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code along\n"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "id": "cc1j4HJMxF-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Observe the Gini Gain for each feature. Which feature seems most promising for the first split based on these values?)**\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* We iterate through each of our features.\n",
        "* For each feature, we use our `gini_gain` function (which also works for multi-class targets) to calculate the reduction in Gini Impurity by splitting on that feature.\n",
        "* The output shows the Gini Gain for each potential first split. The feature with the highest Gini Gain would be chosen as the root node if we were using Gini Impurity as our splitting criterion."
      ],
      "metadata": {
        "id": "MRiC0McBxF-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Demonstration of Choosing the First Split (Multi-Class)\n",
        "\n",
        "By comparing the Information Gain (or Gini Gain) values for all the features, the decision tree algorithm will choose the feature that provides the highest gain as the first splitting node. This process will then be repeated recursively for each resulting subset of the data."
      ],
      "metadata": {
        "id": "ZmRDXvo3E_Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Comparing Information Gains to choose the first split\n",
        "# best_feature_ig = None\n",
        "# max_ig = -1\n",
        "# for feature in features:\n",
        "#     gain = information_gain(df, feature, 'Fruit')\n",
        "#     if gain > max_ig:\n",
        "#         max_ig = gain\n",
        "#         best_feature_ig = feature\n",
        "\n",
        "# print(f\"\\nBased on Information Gain, the first split would be on '{best_feature_ig}' with a gain of {max_ig:.4f}.\")\n",
        "\n",
        "# # Comparing Gini Gains to choose the first split\n",
        "# best_feature_gini = None\n",
        "# max_gini_gain = -1\n",
        "# for feature in features:\n",
        "#     gain_gini = gini_gain(df, feature, 'Fruit')\n",
        "#     if gain_gini > max_gini_gain:\n",
        "#         max_gini_gain = gain_gini\n",
        "#         best_feature_gini = feature\n",
        "\n",
        "# print(f\"Based on Gini Gain, the first split would be on '{best_feature_gini}' with a gain of {max_gini_gain:.4f}.\")"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "id": "ERUj8zUgxF-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Pause. Did Information Gain and Gini Gain suggest the same feature for the first split in this example? Why might they sometimes differ?)**\n",
        "\n",
        "After the first split, this process is repeated for each of the resulting branches. For example, if 'Color' was chosen as the first split, we would then consider the subsets of data where 'Color' is 'Red', 'Yellow', and 'Orange' separately. For each of these subsets, we would again calculate the Information Gain (or Gini Gain) for the remaining features ('Size', 'Shape', 'Texture') to decide the next split, and so on. This continues until a stopping criterion is met (e.g., a node becomes pure, a maximum depth is reached, or the gain from splitting is below a certain threshold).\n",
        "\n",
        "Pruning is still an important step in multi-class decision trees to avoid overfitting the training data and improve generalization to new, unseen data. The same principles of pre-pruning and post-pruning apply.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "By working through this example with a multi-class target, students should now understand how Entropy, Information Gain, and Gini Impurity extend to situations with more than two classes. The core principle remains the same: to choose splits that maximize the reduction in impurity at each node, leading to more homogeneous leaf nodes with respect to the target variable."
      ],
      "metadata": {
        "id": "ct9nJ2v_xF-m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2c8SLYkaoqu"
      },
      "source": [
        "## Iris"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Data"
      ],
      "metadata": {
        "id": "DX6wuQOKGwjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Tp0Dmkxtaoqu"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "\n",
        "# df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# df['species'] = y\n",
        "# df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop('species', axis=1),\n",
        "#                                                     df['species'],\n",
        "#                                                     test_size=0.20,\n",
        "#                                                     random_state=42)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier Model and Tree"
      ],
      "metadata": {
        "id": "IwO3nKeyGrVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7_SKH8naaoqv"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# print(X_train.head())\n",
        "\n",
        "# model = DecisionTreeClassifier(criterion='gini', random_state=42).fit(X_train, y_train)\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # example gini score on petal length\n",
        "# 1 - np.sum(np.square(y_train.value_counts(normalize=True)))"
      ],
      "metadata": {
        "id": "mqFuzxxwBxzD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natural Splits"
      ],
      "metadata": {
        "id": "_reav3APNZhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example = X_train.copy()\n",
        "# example['species'] = y_train\n",
        "\n",
        "# sns.pairplot(example, hue='species', palette=['red', 'blue', 'green']);"
      ],
      "metadata": {
        "id": "0qJ02zV6I5y3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pair Plot**\n",
        "\n",
        "* **Petal Length vs. Other Features:** In every scatter plot where petal length is involved (first row, third column), *setosa* (red) is clearly separated along the petal length axis.\n",
        "* **Petal Width vs. Other Features:** The same is true for petal width (fourth column, second row); *setosa* is well-separated.\n",
        "* **Petal Length vs. Petal Width:** This plot shows *setosa* forming a distinct cluster in the lower-left corner, reinforcing that both features together provide excellent separation."
      ],
      "metadata": {
        "id": "WjQndZ0DN0g2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xCZMsXr2aoqv"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X_train.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histograms**\n",
        "\n",
        "* **Petal Length Histogram:** The histogram for petal length shows a distinct group on the far left (around 1-2 cm) that corresponds to *setosa* (red in the pair plot). There's a clear separation from the other two species, which have petal lengths mostly greater than 3 cm.\n",
        "* **Petal Width Histogram:** Similarly, the petal width histogram shows *setosa* grouped on the far left (around 0-0.5 cm), well-separated from the *versicolor* and *virginica* distributions.\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "The visual separation in both the histograms and pair plots indicates that *setosa* has consistently smaller petal lengths and widths compared to *versicolor* and *virginica*. This \"natural\" separation means a decision tree can easily use these features to create simple rules (like \"petal length <= a threshold\") to isolate *setosa* with high accuracy, as seen in the decision tree where the first split is on petal length."
      ],
      "metadata": {
        "id": "RudmXpxlMjX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partitioning Examples"
      ],
      "metadata": {
        "id": "lhoZjoBEHBkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "metadata": {
        "id": "9YbGtuU1OnX4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "where does the decision tree model get the specific values it uses for splitting (like `petal length (cm) <= 2.45`). Let's clarify this:\n",
        "\n",
        "**The Model Learns the Split Values from the Training Data**\n",
        "\n",
        "The decision tree algorithm *learns* these threshold values (e.g., 2.45 for petal length, 1.65 or 1.75 for petal width) directly from the **training data** you provide it.  It doesn't pull them out of thin air or use some pre-defined list.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "1.  **Training Data:**\n",
        "    * You start with a dataset (like the Iris dataset) that contains measurements of features (petal length, petal width, sepal length, sepal width) for a set of samples, along with the known class or category (species) for each sample.\n",
        "    * This dataset is typically split into two parts:\n",
        "        * **Training set:** This is the portion of the data the model uses to learn the relationships between features and the target variable (species).\n",
        "        * **Testing set:** This is a separate portion used to evaluate how well the model generalizes to new, unseen data.\n",
        "\n",
        "2.  **Algorithm's Search for Optimal Splits:**\n",
        "    * The decision tree algorithm (like CART, ID3, or C4.5) examines the training data to find the best way to split the data at each node.\n",
        "    * \"Best\" is defined based on a criterion like:\n",
        "        * **Information Gain (for ID3, C4.5):** How much does this split reduce the entropy (uncertainty) about the target variable?\n",
        "        * **Gini Impurity (for CART):** How much does this split reduce the \"impurity\" (mixing of classes) in the resulting subsets?\n",
        "    * To find the best split, the algorithm essentially tries out different possible threshold values for each feature.\n",
        "    * For example, for 'petal length', it might try splitting at 2.0 cm, 2.1 cm, 2.2 cm, 2.3 cm, 2.4 cm, 2.45 cm, 2.5 cm, and so on, evaluating the Information Gain or Gini Impurity for each potential split.\n",
        "    * The algorithm then selects the threshold value that results in the greatest improvement in the chosen criterion (highest Information Gain or lowest Gini Impurity).\n",
        "\n",
        "3.  **Example with Petal Length <= 2.45:**\n",
        "    * In the tree you provided, the algorithm found that splitting the data at `petal length (cm) <= 2.45` produced the most significant reduction in impurity (or increase in Information Gain) at that stage of the tree building process.\n",
        "    * This means that, among all the possible petal length values, 2.45 cm was the best at separating the Iris species in the training data at that point.\n",
        "\n",
        "4.  **No Predefined Values:**\n",
        "    * It's crucial to understand that the algorithm is *calculating* and *selecting* these values based on the data. It's not using any pre-set list of petal length or width values.\n",
        "    * If you trained the same model on a slightly different training set (a different random sample of the Iris data), the exact split values might be slightly different, although the overall structure of the tree would likely be similar.\n",
        "\n",
        "**In essence:** The decision tree model learns the rules (including the threshold values for splitting) by analyzing patterns in the training data and finding the splits that best organize the data points according to the target variable."
      ],
      "metadata": {
        "id": "HoDRCtXyQt5_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Sg1bovfZaoqv"
      },
      "outputs": [],
      "source": [
        "# # plot using hue to show different classes\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.scatterplot(x=example['petal length (cm)'],\n",
        "#                 y=example['petal width (cm)'],\n",
        "#                 hue=example['species'],\n",
        "#                 palette=['red', 'blue', 'green'])\n",
        "# plt.axvline(x=2.45, color='black')\n",
        "# plt.axvline(x=4.75, color='black')\n",
        "# plt.hlines(y=1.75, xmin=2.45, xmax=8, color='black')\n",
        "\n",
        "# plt.xlim(0, 8)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YS63Jj0maoqw"
      },
      "outputs": [],
      "source": [
        "# # plot using hue to show different classes\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.scatterplot(x=example['petal length (cm)'][example['petal length (cm)']>2.45],\n",
        "#                 y=example['petal width (cm)'],\n",
        "#                 hue=example['species'],\n",
        "#                 palette=['red', 'blue', 'green'])\n",
        "# plt.axhline(y=1.75, color='black')\n",
        "# plt.vlines(x=4.95, ymin=0, ymax=1.75, color='black')\n",
        "# plt.vlines(x=5.45, ymin=0, ymax=1.75, color='black')\n",
        "# plt.axhline(y=1.55, color='black')\n",
        "\n",
        "# plt.xlim(2.45, 8)\n",
        "# plt.ylim(0, )\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Explanation**\n",
        "\n",
        "The decision tree algorithm aims to partition the Iris dataset by making a series of decisions based on the feature values, ultimately creating subsets that are as pure as possible with respect to the Iris species (setosa, versicolor, and virginica). It prioritizes the features that best separate the species.\n",
        "\n",
        "**Step-by-Step Breakdown with \"I\" Perspective**\n",
        "\n",
        "* **Initial State:** I start with the entire Iris dataset, which, as shown in the histograms (Image 2), has some overlap between species, particularly in petal length and width. This means the dataset isn't perfectly separated from the beginning.\n",
        "* **First Split (Root Node):**\n",
        "    * I observe in the tree (Image 1) that the first split occurs on `petal length (cm) <= 2.45`.\n",
        "    * Looking at the scatter plot (Image 3), I can see that this split effectively isolates the *setosa* species (red points) on the left.\n",
        "    * So, I'm essentially saying: \"If a flower has a petal length of 2.45 cm or less, I confidently classify it as *setosa*.\" This creates a pure *setosa* leaf node.\n",
        "* **Second Split (Right Branch):**\n",
        "    * For the remaining data points (which are *versicolor* and *virginica*), I see that the tree splits on `petal length (cm) <= 4.75`.\n",
        "    * In the scatter plot, this is the vertical line at around 4.75 cm.  This split further divides the dataset, though there's still some mixing of *versicolor* and *virginica*.\n",
        "    * I'm now saying: \"Of the flowers that are *not* *setosa*, if their petal length is 4.75 cm or less, they are *likely* *versicolor*.\"\n",
        "* **Third Split (Left Branch from Second):**\n",
        "    * The tree then splits the *likely versicolor* branch based on `petal width (cm) <= 1.65`.\n",
        "    * This is the horizontal line at 1.65 cm in the scatter plot (Image 3).\n",
        "    * I refine my classification: \"Of the non-*setosa* flowers with petal lengths less than or equal to 4.75 cm, if their petal width is also less than or equal to 1.65 cm, I classify them as *versicolor*.\"  There's still a tiny bit of misclassification here, as seen in the tree's Gini impurity.\n",
        "* **Fourth Split (Right Branch from Second):**\n",
        "    * Finally, the tree splits the other branch (petal length > 4.75 cm) based on `petal width (cm) <= 1.75`.\n",
        "    * This is the horizontal line at 1.75 cm in the scatter plot.\n",
        "    * I conclude: \"The remaining flowers (non-*setosa* with petal lengths greater than 4.75 cm) are classified based on petal width. If the petal width is greater than 1.75 cm, they are classified as *virginica*; otherwise, *versicolor*.\" Again, there's a small amount of misclassification.\n",
        "\n",
        "**Intuitive Summary**\n",
        "\n",
        "The tree essentially creates a series of if-else rules that carve up the data space. It starts with the most important feature (petal length) to make the biggest distinction (separating *setosa*). Then, it refines the classifications using other features (petal width) to separate the remaining species as cleanly as possible. The scatter plots visually show how these rules correspond to dividing the data points into rectangular regions."
      ],
      "metadata": {
        "id": "R2fDPwJsMpGh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwOrQrejaoq6"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "* https://www.analyticsvidhya.com/blog/2022/05/handling-missing-values-with-random-forest/\n",
        "\n",
        "The way to build a good model is through data collection, data cleaning, feature selection, and feature engineering. After we have done all of this, it is then time to optimize our model's performance through its hyperparameters.\n",
        "\n",
        "An ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Random_forest\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Learning\n",
        "\n",
        "In statistics and machine learning, **ensemble methods** use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Ensemble_learning\n",
        "\n",
        "See Ensemble Learning Notebook\n",
        "\n",
        "The traditional way of performing hyperparameter optimization has been **grid search**, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search"
      ],
      "metadata": {
        "id": "YzxoYTH_DMLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging and Boosting\n",
        "\n",
        "* Out of bag...\n",
        "* Bagging parallel, boosting sequential"
      ],
      "metadata": {
        "id": "L_8-zzBDn4MI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfXyPaw4aoq4"
      },
      "source": [
        "### Pruning - Hyperparameters\n",
        "\n",
        "A parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "* https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/\n",
        "\n",
        "Here are some default parameters:\n",
        "\n",
        "<pre>\n",
        "hyperparameters = {\n",
        "            'n_estimators': 100,\n",
        "            'criterion': 'gini',\n",
        "            'max_depth': None,\n",
        "            'max_leaf_nodes': None,\n",
        "            'bootstrap': True\n",
        "            }\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "</pre>\n",
        "\n",
        "**Parameters vs Hyperparameters**:\n",
        "* Parameter: Usually estimated or learned from data\n",
        "* Hyperparameter: Values that are tuned by the data scientist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "54PGK4F5aoq4"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data = load_iris()\n",
        "# X = data.data\n",
        "# y = data.target\n",
        "\n",
        "# iris = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# iris['species'] = y\n",
        "# X_train, X_test, y_train, y_test = train_test_split(iris.drop('species', axis=1),\n",
        "#                                                     iris['species'],\n",
        "#                                                     test_size=0.20,\n",
        "#                                                     random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "995HXwlUaoq5"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# hyperparameters = {\n",
        "#             'criterion': 'entropy'\n",
        "#             }\n",
        "\n",
        "# model = DecisionTreeClassifier(random_state=42).set_params(**hyperparameters)\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9PJck9Yaoq6"
      },
      "source": [
        "The above tree keeps splitting till all the nodes are pure and can lead to overfitting. The next cell introduces some (hyper)parameters that help avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Sa_H_MLvaoq6"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import tree\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# hyperparameters = {\n",
        "#             'criterion': 'entropy',\n",
        "#             'max_depth': 3,\n",
        "#             'max_leaf_nodes': 4\n",
        "#             }\n",
        "\n",
        "# model = DecisionTreeClassifier(random_state=42).set_params(**hyperparameters)\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))\n",
        "\n",
        "# plt.figure(figsize=(14, 14))\n",
        "# tree.plot_tree(model,\n",
        "#               feature_names=X_train.columns,\n",
        "#               class_names=['setosa', 'versicolor', 'virginica'],\n",
        "#               filled=False)\n",
        "\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgqHLJYAaoq6"
      },
      "source": [
        "### Tuning Random Forest (Hyper)Parameters\n",
        "\n",
        "**Focusing on Tree Structure and Complexity:**\n",
        "\n",
        "* **`min_samples_split`**: This controls the minimum number of samples required to split an internal node.\n",
        "    * **Suggested Values:** `[2, 5]`\n",
        "    * **Justification:** A smaller value (like 2) allows for more complex trees that might overfit, while a larger value (like 5) can help prevent overfitting by ensuring nodes only split if they contain a reasonable number of data points. Limiting to two values keeps the search space manageable.\n",
        "* **`min_samples_leaf`**: This controls the minimum number of samples required to be at a leaf node.\n",
        "    * **Suggested Values:** `[1, 3]`\n",
        "    * **Justification:** Similar to `min_samples_split`, a smaller value (like 1) can lead to more complex trees, while a larger value (like 3) can promote more generalized models by requiring a minimum number of samples in the final predictions.\n",
        "* **`max_features`**: This determines the number of features to consider when looking for the best split at each node.\n",
        "    * **Suggested Values:** `['sqrt', 0.5]`\n",
        "    * **Justification:**\n",
        "        * `'sqrt'` (or 'log2') considers the square root of the total number of features. This is a common and often effective default.\n",
        "        * `0.5` considers half of the total number of features. This provides a different level of randomness in feature selection compared to 'sqrt'. Limiting to two options keeps the search efficient.\n",
        "\n",
        "**Hyperparameter Related to Randomness:**\n",
        "\n",
        "* **`random_state`**: While not directly a tuning parameter for the model's complexity, it's crucial for reproducibility.\n",
        "    * **Suggested Value:** `[42]` (or any single integer)\n",
        "    * **Justification:** Setting a `random_state` ensures that the random processes within the Random Forest (like bootstrapping and feature selection) produce the same results each time the code is run. This is important for consistent evaluation and comparison of different hyperparameter settings. You might not include this in the *tuning* grid but should emphasize its importance for good practice.\n",
        "\n",
        "**Considerations for Keeping Execution Time Down:**\n",
        "\n",
        "* **Number of Hyperparameters:** The example you provided has 5 hyperparameters. Adding 3 more (`min_samples_split`, `min_samples_leaf`, `max_features`) will increase the size of the grid significantly (2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 = 256 combinations). Be mindful of this. You might suggest that students initially try a smaller grid and then potentially expand if time allows.\n",
        "* **Range of Values:** Keeping the number of values for each hyperparameter limited is key, as you've already done.\n",
        "\n",
        "```python\n",
        "hyperparameters = {\n",
        "    'n_estimators': [50, 150],  # Slightly reduced max for faster execution\n",
        "    'criterion': ['entropy', 'gini'],\n",
        "    'max_depth': [3, 5],      # Slightly increased max depth\n",
        "    'max_leaf_nodes': [6, 10], # Adjusted range\n",
        "    'bootstrap': [True, False],\n",
        "    'min_samples_split': [2, 4],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt', 0.6] # Adjusted value\n",
        "}\n",
        "```\n",
        "\n",
        "**Important Considerations**\n",
        "\n",
        "* **Justification is Key:** Explain *why* you chose the specific values for each hyperparameter. Your reasoning should be based on your understanding of how these parameters affect the model's bias-variance trade-off, complexity, and potential for overfitting or underfitting.\n",
        "* **Cross-Validation Strategy:** Use an appropriate cross-validation strategy (e.g., Stratified K-Fold since it's likely a classification task with potentially imbalanced classes) to get a reliable estimate of the model's performance for each hyperparameter combination.\n",
        "* **Computational Limits:** Keep in mind the constraints on their hyperparameter choices and grid size and computational time. You might need to start with a smaller grid and iterate if time permits.\n",
        "\n",
        "Random forests create many decision trees that sample data. The bootstrap hyperparameter sets sampling with or without replacement.\n",
        "\n",
        "**Rule**: Never make adjustments to your model based on test set results.\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search\n",
        "\n",
        "See Grid Search Notebook"
      ],
      "metadata": {
        "id": "3CAhF3MaDDFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Validation\n",
        "\n",
        "See Cross Validation Notebook\n",
        "\n",
        "* K-fold vs Group K-fold\n",
        "* Leave one out\n",
        "* Monte Carlo\n",
        "\n",
        "https://towardsdatascience.com/why-you-should-never-use-cross-validation-4360d42456ac"
      ],
      "metadata": {
        "id": "8ORlXkIZFK8Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60uMb6Eqaoq7"
      },
      "source": [
        "### Our Final Random Forest Model with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "920nHbfOaoq7"
      },
      "outputs": [],
      "source": [
        "# # create dataframe from sklearn iris dataset; print shape, info, and head\n",
        "# import pandas as pd\n",
        "# from sklearn.datasets import load_iris\n",
        "\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "\n",
        "# df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "# df['species'] = y\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0WjRu5NLaoq7"
      },
      "outputs": [],
      "source": [
        "# # train test split using 25% for test size; print X_train shape and head\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop('species', axis=1),\n",
        "#                                                     df['species'],\n",
        "#                                                     test_size=0.25,\n",
        "#                                                     random_state=42)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "298UVX_Taoq7"
      },
      "outputs": [],
      "source": [
        "# # implement a grid search using max_depth, min_samples_split, min_samples_leaf, bootstrap, and criterion (entropy and gini)\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# hyperparameters = {\n",
        "#     'n_estimators': [50, 150],\n",
        "#     'criterion': ['entropy', 'gini'],\n",
        "#     'max_depth': [3, 5],\n",
        "#     'max_leaf_nodes': [6, 10],\n",
        "#     'bootstrap': [True, False],\n",
        "#     'min_samples_split': [2, 4],\n",
        "#     'min_samples_leaf': [1, 2],\n",
        "#     'max_features': ['sqrt', 0.6]\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(estimator = RandomForestClassifier(),\n",
        "#                            param_grid = hyperparameters,\n",
        "#                            scoring = 'accuracy',\n",
        "#                            cv = 10)\n",
        "\n",
        "# grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# best_accuracy = grid_search.best_score_\n",
        "# best_parameters = grid_search.best_params_\n",
        "\n",
        "# print('best accuracy', best_accuracy)\n",
        "# print('best parameters', best_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qrmA7QA2aoq7"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # model = RandomForestClassifier(bootstrap = False,\n",
        "# #                                criterion = 'entropy',\n",
        "# #                                max_depth = 3,\n",
        "# #                                min_samples_leaf = 5,\n",
        "# #                                min_samples_split = 4,\n",
        "# #                                random_state = 42)\n",
        "# model = RandomForestClassifier(random_state = 42).set_params(**best_parameters) # * args, ** kwargs\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# print(accuracy_score(y_test, predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}