{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC4050/blob/main/Week_03-Data_Prep_and_EDA/Week_03_Data_Prep_and_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "268ad749",
      "metadata": {
        "id": "268ad749"
      },
      "source": [
        "# Week 03 - Data Prep and Exploratory Data Analysis (EDA)\n",
        "\n",
        "Your Name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC4050 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Take attendance\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link"
      ],
      "metadata": {
        "id": "ojyIDptwB7Iy"
      },
      "id": "ojyIDptwB7Iy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bell Curve Demonstration\n",
        "\n",
        "**1. Educational Attainment in the US**\n",
        "\n",
        "* **Bachelor's Degrees:** According to the U.S. Census Bureau, about 37.5% of Americans aged 25 and older have a bachelor's degree or higher (2021 data).\n",
        "* **Graduate Degrees:**  A smaller percentage, around 13.1%, hold a graduate or professional degree.\n",
        "* **\"Senior-Level College Class\":**  Defining this is tricky. It could mean students in their final year of a bachelor's program or those taking advanced courses.  There's no readily available statistic for this specific group. Let's say it's slightly higher than 37.5%.\n",
        "\n",
        "**2. GPA Range**\n",
        "\n",
        "* **Average College GPA:**  While it varies, the average college GPA in the US is often cited around 3.0 to 3.3.\n",
        "* **Senior-Level Students:** It's reasonable to assume that students who reach senior-level courses might have a slightly higher average GPA, perhaps in the 3.2 to 3.5 range. However, this is an estimate.\n",
        "* **Standard Deviation:**  The standard deviation of GPAs would depend on the specific institution and major.  A reasonable estimate might be around 0.3 to 0.5, but this is speculative.\n",
        "\n",
        "**3. Demonstrating Sampling**\n",
        "\n",
        "To illustrate sampling these students from the general population, you could use a simplified simulation:\n",
        "\n",
        "* **Population:**  Start with a hypothetical population of, say, 10,000 individuals representing the US adult population.\n",
        "* **Assign Education Levels:** Based on the Census data, assign educational attainment levels to these individuals (e.g., 37.5% with bachelor's or higher).\n",
        "* **Assign GPAs:**  For those with bachelor's or higher, randomly assign GPAs based on a normal distribution with a mean of 3.3 (or your chosen estimate) and a standard deviation of 0.4 (or your estimate).\n",
        "* **Sampling:**\n",
        "    * **Simple Random Sampling:** Randomly select a certain number of individuals from the population.\n",
        "    * **Stratified Sampling:** Divide the population into strata based on education level (e.g., bachelor's, master's, etc.) and sample proportionally from each stratum.\n",
        "* **Compare Samples:**  Compare the characteristics of your samples (education level, GPA distribution) to the overall population to illustrate how well they represent the population.\n",
        "\n",
        "**Important Notes:**\n",
        "\n",
        "* **Data Limitations:** The available data doesn't perfectly capture the specific group of \"senior-level college classes.\"\n",
        "* **Assumptions:** You'll need to make some assumptions about GPA distributions, which might not be entirely accurate.\n",
        "* **Focus on the Concepts:** The goal is to demonstrate sampling methods and how they can be used to estimate population characteristics, even if the data isn't perfect.\n",
        "\n",
        "By using this simplified simulation, you can illustrate the challenges and considerations involved in sampling a specific subgroup from a larger population and how different sampling methods can affect the accuracy and representativeness of your sample."
      ],
      "metadata": {
        "id": "pcv_-oD3uHDf"
      },
      "id": "pcv_-oD3uHDf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Driven Decision Making and Game theory\n",
        "\n",
        "* Von Neumann, Dr. Strangelove, and Game Theory\n",
        "* https://thecritic.co.uk/issues/february-2022/the-genius-that-was-dr-strangelove/\n",
        "* https://en.wikipedia.org/wiki/Theory_of_Games_and_Economic_Behavior\n",
        "* https://medium.com/@skueong/thinking-in-bets-making-smarter-decisions-when-you-dont-have-all-the-facts-book-notes-98f02db61c1a\n",
        "\n",
        "**Data-Driven Decision Making**\n",
        "\n",
        "* **The Core Idea:** Instead of relying on intuition, gut feelings, or tradition, data-driven decision-making (DDDM) uses facts, figures, and statistical analysis to guide choices. This leads to more objective, informed decisions.\n",
        "\n",
        "* **Von Neumann's Influence:** John von Neumann, a brilliant mathematician and game theorist, played a crucial role in developing the foundations of modern computing and data analysis. His work on game theory, particularly the concept of expected value, provided a framework for making rational decisions in complex situations.\n",
        "\n",
        "* **Dr. Strangelove and Game Theory:** The film \"Dr. Strangelove\" satirizes Cold War decision-making, highlighting the potential dangers of relying on flawed data and biased assumptions. It also illustrates game theory concepts like the \"prisoner's dilemma\" and the risks of escalation in strategic interactions. https://en.wikipedia.org/wiki/Prisoner%27s_dilemma\n",
        "\n",
        "* **Game Theory's Relevance:** Game theory provides a mathematical framework for analyzing strategic interactions, where the outcome of your decisions depends on the choices of others. In DDDM, game theory can help you:\n",
        "    * **Model scenarios:**  Analyze potential outcomes based on different choices and competitor actions.\n",
        "    * **Identify optimal strategies:** Determine the best course of action to maximize your chances of success.\n",
        "    * **Understand risk and uncertainty:**  Assess the potential downsides and uncertainties associated with different decisions.\n",
        "\n",
        "**How it all connects:**\n",
        "\n",
        "Von Neumann's contributions to computing and game theory laid the groundwork for modern data analysis and decision-making.  \"Dr. Strangelove\" serves as a cautionary tale about the dangers of flawed data and biased assumptions in high-stakes decisions. Game theory provides a powerful tool for analyzing strategic choices and optimizing outcomes in data-driven decision-making.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "Data-driven decision-making, built upon the foundations of figures like Von Neumann and informed by tools like game theory, empowers individuals and organizations to make more rational, informed choices in complex and uncertain environments.\n",
        "\n",
        "Versus\n",
        "\n",
        "* Westley vs Vizzini and the poison drinks (The Princess Bride)\n",
        "* https://www.cbr.com/battle-of-wits-scene-sham-theory-princess-bride/\n",
        "* A lethal battle of wits, Thinking in Bets\n",
        "\n",
        "This is a classic scene that cleverly illustrates game theory principles, although it's not a direct example of the prisoner's dilemma.\n",
        "\n",
        "**The Scenario**\n",
        "\n",
        "Westley and Vizzini face each other in a battle of wits to determine who will drink from the poisoned cup. Vizzini, convinced of his intellectual superiority, engages Westley in a game of deduction and logic.\n",
        "\n",
        "**Game Theory Elements**\n",
        "\n",
        "* **Incomplete Information:**  Neither Westley nor Vizzini knows which cup contains the poison. This creates a situation of incomplete information, where each player must make decisions based on their assumptions about the other player's knowledge and reasoning.\n",
        "* **Deception:** Westley uses deception to outsmart Vizzini. He implies that he has switched the cups, leading Vizzini to make a fatal miscalculation.\n",
        "* **Zero-Sum Game:** The battle of wits is a zero-sum game, where one player's gain is the other player's loss. Only one can survive.\n",
        "* **Psychological Warfare:** Westley engages in psychological manipulation, playing on Vizzini's arrogance and overconfidence to influence his decision-making.\n",
        "\n",
        "**Why it's not a Prisoner's Dilemma**\n",
        "\n",
        "While the scene involves strategic decision-making, it differs from the prisoner's dilemma in a few key ways:\n",
        "\n",
        "* **No dominant strategy:** In the prisoner's dilemma, confessing is always the dominant strategy, regardless of what the other player does. In the battle of wits, there is no single dominant strategy. The optimal choice depends on the assumptions each player makes about the other.\n",
        "* **Communication:**  The prisoner's dilemma involves no communication between players. In the battle of wits, there is a verbal exchange that allows for deception and psychological manipulation.\n",
        "* **Outcome:** The prisoner's dilemma typically leads to a suboptimal outcome for both players. In the battle of wits, one player wins and the other loses.\n",
        "\n",
        "**Key Takeaway**\n",
        "\n",
        "While not a direct example of the prisoner's dilemma, the battle of wits in \"The Princess Bride\" demonstrates the importance of strategic thinking, deception, and psychological factors in game theory and decision-making. It's a reminder that even in situations with incomplete information, careful analysis and clever maneuvering can lead to victory."
      ],
      "metadata": {
        "id": "l9c8XQVtqMEO"
      },
      "id": "l9c8XQVtqMEO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutoring Extra Credit\n",
        "\n",
        "* Please see Canvas for link"
      ],
      "metadata": {
        "id": "f_DAD3iYhpO3"
      },
      "id": "f_DAD3iYhpO3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Part I\n",
        "\n",
        "Presentations start next week\n",
        "\n",
        "* Think of a data science project you'd like to work on for the rest of the semester that uses a dataset\n",
        "* Think about a story you want to tell with this data\n",
        "* Introduce this dataset to the class and describe the dataset\n",
        "  * Shape, Info, Simple descriptives\n",
        "  * Missing values\n",
        "  * Histograms - Skewness Kurtosis\n",
        "  * Correlations\n",
        "\n",
        "Hints\n",
        "\n",
        "* Keep the dataset simple if possible\n",
        "* If you change your mind later, it's ok but a complete analysis as per the instructions in class will still be needed."
      ],
      "metadata": {
        "id": "sUkooZdnl7rp"
      },
      "id": "sUkooZdnl7rp"
    },
    {
      "cell_type": "markdown",
      "id": "c41cc0a6",
      "metadata": {
        "id": "c41cc0a6"
      },
      "source": [
        "## Functions, Methods, and Attributes\n",
        "\n",
        "* df.shape: attribute; values that are precomputed\n",
        "* df.head(): method; values are computed when called; belongs to a class, package, module, an object\n",
        "* my_func(): function; usually created by programmer; set of instructions that perform a task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7572c3",
      "metadata": {
        "id": "ec7572c3"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b50af6f",
      "metadata": {
        "id": "0b50af6f"
      },
      "source": [
        "### Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2078a7e2",
      "metadata": {
        "id": "2078a7e2"
      },
      "outputs": [],
      "source": [
        "# # download class-grades from github and upload it to session storage\n",
        "# # print shape, info, and first five rows\n",
        "# import pandas as pd\n",
        "\n",
        "# grades = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/class-grades2.csv')\n",
        "# grades = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/class-grades2.csv', on_bad_lines='warn')\n",
        "# print(grades.shape)\n",
        "# print(grades.info())\n",
        "# grades.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424776a2",
      "metadata": {
        "id": "424776a2"
      },
      "source": [
        "### Missing Values\n",
        "\n",
        "Missing data cause problems because most statistical procedures require a value for each variable. When a data set is incomplete, the data analyst has to decide how to deal with it.\n",
        "\n",
        "https://www.theanalysisfactor.com/causes-of-missing-data/<br />\n",
        "https://www.theanalysisfactor.com/when-listwise-deletion-works/\n",
        "\n",
        "* MCAR: Missing Completely at Random\n",
        "    * Probability of missing is same for all cases\n",
        "    * I tripped and broke the test tubes I was carrying\n",
        "    * Missing because not sampled\n",
        "* MNAR: Missing Not at Random\n",
        "    * Probability of missing is not the same for all cases\n",
        "    * Data are missing on IQ and only the people with low IQ values have missing observations for this variable\n",
        "    * Missing from public opinion because respondent maybe inhibited or have bias\n",
        "* MAR: Missing at Random\n",
        "    * Probability of missing is the same only within groups\n",
        "    * Probability of sample depends on some known property\n",
        "    * Only younger people have missing values for IQ\n",
        "\n",
        "https://stefvanbuuren.name/fimd/sec-MCAR.html<br />\n",
        "https://www.iriseekhout.com/post/2022-06-28-missingdatamechanisms/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you're conducting a survey where people have to fill in their IQ and years of experience in a particular field. However, not everyone answers both questions. Here's how MCAR, MAR, and MNAR could play out:\n",
        "\n",
        "**1. Missing Completely at Random (MCAR)**\n",
        "\n",
        "* **Scenario:** Some participants accidentally skip the IQ question or the experience question due to a misclick or oversight. The missingness occurs randomly and is unrelated to any other factors, including the values of IQ or experience themselves.\n",
        "* **Implication:**  The missing data doesn't introduce any systematic bias. You can still analyze the remaining data and draw valid conclusions, although with slightly reduced statistical power due to the smaller sample size.\n",
        "\n",
        "**2. Missing at Random (MAR)**\n",
        "\n",
        "* **Scenario:** People with very high IQs might be less likely to reveal their experience, feeling it's irrelevant to their intelligence.  In this case, the missingness of the experience variable depends on the IQ variable (which is observed).\n",
        "* **Implication:** There's a systematic pattern in the missing data, but it can be accounted for by using the observed information (IQ). You can use statistical techniques to adjust for this bias and still obtain relatively valid conclusions.\n",
        "\n",
        "**3. Missing Not at Random (MNAR)**\n",
        "\n",
        "* **Scenario:** Individuals with low IQs might be embarrassed to report their scores and are more likely to leave that question blank. The missingness of the IQ variable is directly related to its value (low IQ), which is unobserved.\n",
        "* **Implication:**  This is the most problematic scenario. The missing data is biased and cannot be fully accounted for using the available information. Analyzing only the reported IQs would lead to an overestimation of the average IQ in the population.\n",
        "\n",
        "**Key takeaways**\n",
        "\n",
        "* **MCAR:** The easiest to handle, as the missingness is truly random.\n",
        "* **MAR:**  Can be addressed by using the observed information to adjust for the bias.\n",
        "* **MNAR:** The most challenging, as the missingness is directly related to the unobserved values, creating a bias that's difficult to correct.\n",
        "\n",
        "Understanding these missing data mechanisms is crucial for interpreting survey results and drawing valid conclusions. It's important to consider the potential reasons for missing data and choose appropriate statistical techniques to handle them effectively."
      ],
      "metadata": {
        "id": "RzHvVTiW9CyR"
      },
      "id": "RzHvVTiW9CyR"
    },
    {
      "cell_type": "markdown",
      "id": "80e59473",
      "metadata": {
        "id": "80e59473"
      },
      "source": [
        "### Complete-Case Analysis (CCA)\n",
        "\n",
        "* Aka Listwise deletion\n",
        "* Reduces sample size\n",
        "* Can reduce the statistical efficiency of estimates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69119af1",
      "metadata": {
        "id": "69119af1"
      },
      "outputs": [],
      "source": [
        "# # delete rows with missing values example of cca\n",
        "# print(grades.shape)\n",
        "# print(grades.dropna().shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b3cf11",
      "metadata": {
        "id": "28b3cf11"
      },
      "source": [
        "### Cardinality of Features\n",
        "* Uneven distribution of labels between train and test sets (some may appear in one set and not in other)\n",
        "* Features with many labels dominate over those with fewer labels\n",
        "* Many labels introduces noise with little or no information\n",
        "* Reducing may help model performance\n",
        "* Removing features with low cardinality my help model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3e472c",
      "metadata": {
        "id": "fe3e472c"
      },
      "outputs": [],
      "source": [
        "# # features with only one value (constant)\n",
        "# constant_features = [\n",
        "#     feat for feat in grades.columns if len(grades[feat].unique()) == 1\n",
        "# ]\n",
        "\n",
        "# print(grades.shape)\n",
        "# constant_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # drop constant_features\n",
        "# grades.drop(constant_features, axis=1, inplace=True)\n",
        "# print(grades.shape)"
      ],
      "metadata": {
        "id": "wAGTtjJGRxWT"
      },
      "id": "wAGTtjJGRxWT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e349fbd6",
      "metadata": {
        "id": "e349fbd6"
      },
      "outputs": [],
      "source": [
        "# # identify quasi constant values (sometimes these may be boolean/binary features)\n",
        "# quasi_consts = []\n",
        "# for val in grades.columns.sort_values():\n",
        "#     if (len(grades[val].unique()) < 3):\n",
        "#         val_counts = grades[val].value_counts(normalize=True)\n",
        "#         print(val_counts)\n",
        "#         if list(val_counts)[0] > .98:\n",
        "#             quasi_consts.append(val)\n",
        "\n",
        "# print('quasi_consts', quasi_consts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # drop quasi_consts\n",
        "# grades.drop(quasi_consts, axis=1, inplace=True)\n",
        "# grades.shape"
      ],
      "metadata": {
        "id": "tNmlDtA8YVOu"
      },
      "id": "tNmlDtA8YVOu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f47e67d5",
      "metadata": {
        "id": "f47e67d5"
      },
      "source": [
        "### Duplications"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # duplicate rows\n",
        "# grades[grades.duplicated(keep=False)]"
      ],
      "metadata": {
        "id": "PQMmwlv9Lklw"
      },
      "id": "PQMmwlv9Lklw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # drop duplicate rows\n",
        "# print(grades.shape)\n",
        "# grades.drop_duplicates(inplace=True)\n",
        "# grades.shape"
      ],
      "metadata": {
        "id": "0GIAZy6ALgC9"
      },
      "id": "0GIAZy6ALgC9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a511cc25",
      "metadata": {
        "id": "a511cc25"
      },
      "outputs": [],
      "source": [
        "# # check of duplications\n",
        "# duplicated_feats = []\n",
        "# for i in range(0, len(grades.columns)):\n",
        "#     orig = grades.columns[i]\n",
        "\n",
        "#     for dupe in grades.columns[i + 1:]:\n",
        "#         if grades[orig].equals(grades[dupe]):\n",
        "#             duplicated_feats.append(dupe)\n",
        "\n",
        "# duplicated_feats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # drop duplicated_feats\n",
        "# print(grades.shape)\n",
        "# grades.drop(duplicated_feats, axis=1, inplace=True)\n",
        "# grades.shape"
      ],
      "metadata": {
        "id": "BTG-Owb7XtbN"
      },
      "id": "BTG-Owb7XtbN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "89aaa3dd",
      "metadata": {
        "id": "89aaa3dd"
      },
      "source": [
        "### Any vs All"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a61d39",
      "metadata": {
        "id": "08a61d39"
      },
      "outputs": [],
      "source": [
        "# # count nulls\n",
        "# grades.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d153c4e1",
      "metadata": {
        "id": "d153c4e1"
      },
      "outputs": [],
      "source": [
        "# # drop columns with null values https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\n",
        "# print(grades.shape)\n",
        "# grades.dropna(how='all', axis='columns', inplace=True) # 1\n",
        "# print(grades.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffce2927",
      "metadata": {
        "id": "ffce2927"
      },
      "outputs": [],
      "source": [
        "# # drop rows with null values\n",
        "# print(grades.shape)\n",
        "# print(grades.dropna(how='any', axis='index').shape) # information is lost when we drop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preppy Introduction\n",
        "\n",
        "* identify_consts\n",
        "* identify_quasi_consts\n",
        "* check_row_duplicates\n",
        "* check_col_duplicates\n",
        "* handle_missing_values\n",
        "* handle_standard_scaler\n",
        "* handle_minmax_scaler\n",
        "* handle_outliers\n",
        "* do_OHE\n",
        "\n",
        "Problems Solved\n",
        "* constants provide no variability, no informations\n",
        "* duplicates inflate statistics and bias models\n",
        "* scaling ensures that all features contribute equally to the model's training"
      ],
      "metadata": {
        "id": "99N7hlJb3agQ"
      },
      "id": "99N7hlJb3agQ"
    },
    {
      "cell_type": "markdown",
      "id": "7f8961e4",
      "metadata": {
        "id": "7f8961e4"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descriptive Statistics\n",
        "\n",
        "* Descriptive statistics: the numbers and calculations we use to summarize raw data\n",
        "* The mean is prone to distortion by outliers so we have the median\n",
        "* Elon Musk and Tiny Homes, 35000 till Elon joins and then average income jumps to $91 million\n",
        "* Absolute statistic vs relative statistic; actual values vs percentages\n",
        "* Standard deviation: how dispersed the data from the mean, how spread out\n",
        "* Descriptive statistics are often used to compare two quantities"
      ],
      "metadata": {
        "id": "2XRPz7hFQxFZ"
      },
      "id": "2XRPz7hFQxFZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anscombes Quartet\n",
        "Anscombe's quartet comprises four data sets that have nearly identical simple descriptive statistics, yet have very different distributions and appear very different when graphed.\n",
        "https://en.wikipedia.org/wiki/Anscombe%27s_quartet"
      ],
      "metadata": {
        "id": "5Coy4KF5p9ou"
      },
      "id": "5Coy4KF5p9ou"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deceptive Statitics\n",
        "\n",
        "* The use of statistics to describe complex phenomena is not exact\n",
        "* Again precision and accuracy\n",
        "* Precision can mask inaccuracy\n",
        "* In 1950, Joseph McCarthy waved a piece of paper in a speech and declared he had a list of 205 names known to the Secretary of State that were working in the State Department. The paper was blank and this was an outright lie, but the specificity gave the lie credibility\n",
        "* Measurements, or calculations, no matter how precise, need to be checked with common sense\n",
        "* Descriptive statistics may suffer from clarity over exactly what is being described\n",
        "* Be sure to present a range of statistics with a range of perspectives\n",
        "* Our schools are getting worse! 60% of our schools had lower test scores this year from last\n",
        "* Our schools are getting better! 80% of our students had higher test scores from last year\n",
        "* Not all schools/students are equal and it depends on the unit of analysis\n",
        "* One measured schools and the other measured students"
      ],
      "metadata": {
        "id": "5YNVj4ndqM8m"
      },
      "id": "5YNVj4ndqM8m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanatory Analysis vs Exploratory Analysis\n",
        "\n",
        "Exploratory analysis is the process of turning over 100 rocks to find perhaps 1 or 2 precious gemstones. Explanatory analysis is what happens when you have something specific you want to show an audience - probably about those 1 or 2 precious gemstones.\n",
        "\n",
        "https://www.storytellingwithdata.com/blog/2014/04/exploratory-vs-explanatory-analysis"
      ],
      "metadata": {
        "id": "Lflfw2QZZF_T"
      },
      "id": "Lflfw2QZZF_T"
    },
    {
      "cell_type": "markdown",
      "id": "11d57cbb",
      "metadata": {
        "id": "11d57cbb"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abb17ad",
      "metadata": {
        "id": "5abb17ad"
      },
      "outputs": [],
      "source": [
        "# # train test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(grades.drop('FinalGrade', axis=1), grades['FinalGrade'], test_size=.2, random_state=42)\n",
        "# print(X_train.shape)\n",
        "# print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f3e831",
      "metadata": {
        "id": "c1f3e831"
      },
      "outputs": [],
      "source": [
        "# # info\n",
        "# X_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709bae64",
      "metadata": {
        "id": "709bae64"
      },
      "outputs": [],
      "source": [
        "# # brief statistics\n",
        "# X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "126d9d4d",
      "metadata": {
        "id": "126d9d4d"
      },
      "outputs": [],
      "source": [
        "# # value counts\n",
        "# X_train['TakeHome'].str.lower().value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97707d53",
      "metadata": {
        "id": "97707d53"
      },
      "outputs": [],
      "source": [
        "# # pie chart\n",
        "# grades['TakeHome'].str.lower().value_counts(dropna=False).plot.pie()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541c8cbb",
      "metadata": {
        "id": "541c8cbb"
      },
      "outputs": [],
      "source": [
        "# # show histograms\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X_train.hist()\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d5eb588",
      "metadata": {
        "id": "6d5eb588"
      },
      "source": [
        "## Measures of Center\n",
        "* Mean\n",
        "* Median\n",
        "* Mode\n",
        "\n",
        "And the Normal Distribution: https://www.mathsisfun.com/data/standard-normal-distribution.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58dc35ef",
      "metadata": {
        "id": "58dc35ef"
      },
      "outputs": [],
      "source": [
        "# X_train['TakeHome'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f55c153",
      "metadata": {
        "id": "5f55c153"
      },
      "outputs": [],
      "source": [
        "# # mean, median, mode\n",
        "# import numpy as np\n",
        "\n",
        "# print('mean:', int(np.mean(X_train['Assignment1'])))\n",
        "# print('mean:', int(X_train['Assignment1'].mean()))\n",
        "# print('median:', int(X_train['Assignment1'].median()))\n",
        "# print('mode:', X_train['TakeHome'].mode())\n",
        "# print('mode:', X_train['TakeHome'].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # what happens to the mean and median with skewed data\n",
        "# import numpy as np\n",
        "# from scipy.stats import skewnorm, norm\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, ax = plt.subplots(1, 3, figsize=(14, 3))\n",
        "# skew = 20\n",
        "# n = 100000\n",
        "\n",
        "# r = skewnorm.rvs(skew, loc=0, scale=10, size=n)\n",
        "# ax[0].hist(r, bins=50, density=True, alpha=0.4)\n",
        "# ax[0].axvline(x=np.median(r), color='green', label='Median')\n",
        "# ax[0].axvline(x=np.mean(r).round(2), color='red', label='Mean')\n",
        "# ax[0].legend()\n",
        "# print(f'right skew data mean: {np.mean(r).round(2)}, median: {np.median(r).round(2)}')\n",
        "\n",
        "# l = skewnorm.rvs(-skew, loc=0, scale=10, size=n)\n",
        "# ax[2].hist(l, bins=50, density=True, alpha=0.4);\n",
        "# ax[2].axvline(x=np.mean(l).round(2), color='red', label='Mean')\n",
        "# ax[2].axvline(x=np.median(l), color='green', label='Median')\n",
        "# ax[2].legend()\n",
        "# print(f'left skew data mean: {np.mean(l).round(2)}, median: {np.median(l).round(2)}')\n",
        "\n",
        "# n = norm.rvs(loc=0, scale=1, size=n)\n",
        "# ax[1].hist(n, bins=50, density=True, alpha=0.4);\n",
        "# ax[1].axvline(x=np.mean(n).round(2), color='red', linewidth=3, label='Mean')\n",
        "# ax[1].axvline(x=np.median(n), color='green', label='Median')\n",
        "# ax[1].legend()\n",
        "# print(f'normal data mean: {np.mean(n).round(2)}, median: {np.median(n).round(2)}')"
      ],
      "metadata": {
        "id": "wy1B6u7voAHY"
      },
      "id": "wy1B6u7voAHY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d6ce6a16",
      "metadata": {
        "id": "d6ce6a16"
      },
      "source": [
        "### The Wisdom of the Crowd, The Median"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab67540",
      "metadata": {
        "id": "3ab67540"
      },
      "source": [
        "### Imputing Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b7e654",
      "metadata": {
        "id": "03b7e654"
      },
      "outputs": [],
      "source": [
        "# X_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0a59a6",
      "metadata": {
        "id": "ab0a59a6"
      },
      "outputs": [],
      "source": [
        "# # replace missing values with mean\n",
        "# X_train['Assignment1'].fillna(X_train['Assignment1'].round(decimals=2).mean(), inplace=True)\n",
        "# X_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837cf897",
      "metadata": {
        "id": "837cf897"
      },
      "outputs": [],
      "source": [
        "# replace missing values for Tutorial, Midterm, Quiz, and Final using the example above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0572cc",
      "metadata": {
        "id": "2b0572cc"
      },
      "source": [
        "## Measures of Spread"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e05e23",
      "metadata": {
        "id": "b6e05e23"
      },
      "source": [
        "### Variance\n",
        "\n",
        "Equation for mean:<br />\n",
        "$\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$\n",
        "\n",
        "https://www.statology.org/sample-variance-vs-population-variance/<br />\n",
        "Equation for population variance:<br />\n",
        "$\\sigma^2 = \\frac{1}{N}\\sum({x}-\\bar{x})^2$\n",
        "\n",
        "Equation for sample variance:<br />\n",
        "$s^2 = \\frac{1}{n-1}\\sum({x}-\\bar{x})^2$\n",
        "\n",
        "Equation for standard deviation:<br />\n",
        "$\\sigma = \\sqrt{\\frac{1}{N}\\sum(x-\\bar{x})^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "246a6e16",
      "metadata": {
        "id": "246a6e16"
      },
      "outputs": [],
      "source": [
        "# # Assignment variance and standard deviation\n",
        "# print('Population variance:', X_train['Final'].var(ddof=0))\n",
        "# print('Sample variance:', X_train['Final'].var(ddof=1))\n",
        "# print('Population std dev:', X_train['Final'].std(ddof=0))\n",
        "# print('Sample std dev:', X_train['Final'].std(ddof=1))\n",
        "# print('Square root of sample variance:', np.sqrt(X_train['Final'].var()))\n",
        "# print('Square root of sample variance:', X_train['Final'].var()**(1/2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747d0858",
      "metadata": {
        "id": "747d0858"
      },
      "source": [
        "### Quartiles\n",
        "\n",
        "https://en.wikipedia.org/wiki/Interquartile_range\n",
        "\n",
        "Interquartile range<br />\n",
        "Whiskers<br />\n",
        "\n",
        "Outliers<br />\n",
        "Fence<br />\n",
        "https://www.statisticshowto.com/upper-and-lower-fences/\n",
        "\n",
        "Boxplots<br />\n",
        "Violin plots"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Percentages\n",
        "\n",
        "* Naked Statistics - the dress and 25% markdown\n",
        "* Art of Statistics Chapter 2 - 5% mortality sounds much worse than 95% survival"
      ],
      "metadata": {
        "id": "RJr1D1crQ3cJ"
      },
      "id": "RJr1D1crQ3cJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa5b5b1",
      "metadata": {
        "id": "2aa5b5b1"
      },
      "outputs": [],
      "source": [
        "# # Assignment histogram\n",
        "# X_train['Final'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2163a715",
      "metadata": {
        "id": "2163a715"
      },
      "outputs": [],
      "source": [
        "# # Assignment boxplot\n",
        "# X_train.boxplot(column=['Final']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304660b6",
      "metadata": {
        "id": "304660b6"
      },
      "outputs": [],
      "source": [
        "# # Assignment violinplot\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.violinplot(x=X_train['Final']);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094c027a",
      "metadata": {
        "id": "094c027a"
      },
      "source": [
        "## Measures of Shape\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/05/shape-of-data-skewness-and-kurtosis/\n",
        "\n",
        "**Skewness**\n",
        "* Skewed right\n",
        "* Skewed left\n",
        "\n",
        "**Kurtosis**\n",
        "* Mesokurtic\n",
        "* Leptokurtic\n",
        "* Platykurtic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d3bd9c",
      "metadata": {
        "id": "59d3bd9c"
      },
      "outputs": [],
      "source": [
        "# # visualize outliers before and after\n",
        "# from random import random\n",
        "# from random import randint\n",
        "# from sklearn.datasets import make_regression\n",
        "\n",
        "# X, y = make_regression(n_samples=100, n_features=1, noise=50, random_state=42)\n",
        "# print('mean before outliers:', np.mean(X))\n",
        "# print('var before outliers:', np.var(X))\n",
        "# fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(10, 10))\n",
        "# ax1.scatter(X, y)\n",
        "# ax2.hist(X)\n",
        "# ax3.boxplot(X)\n",
        "# sns.violinplot(ax=ax4, data=X);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0b0a4e",
      "metadata": {
        "id": "7d0b0a4e"
      },
      "outputs": [],
      "source": [
        "# # create outliers\n",
        "# for i in range(30):\n",
        "#     factor = randint(-3, 3)\n",
        "#     if random() > 0.5:\n",
        "#         X[i] += factor * X.std()\n",
        "#     else:\n",
        "#         X[i] -= factor * X.std()\n",
        "\n",
        "# fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(10, 10))\n",
        "# ax1.scatter(X, y)\n",
        "# ax2.hist(X)\n",
        "# ax3.boxplot(X)\n",
        "# sns.violinplot(ax=ax4, data=X)\n",
        "# print('mean after outliers:', np.mean(X))\n",
        "# print('var after outliers:', np.var(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1cbf3d7",
      "metadata": {
        "id": "f1cbf3d7"
      },
      "source": [
        "### Skewed data\n",
        "\n",
        "https://www.itl.nist.gov/div898/handbook/eda/section3/eda33e6.htm\n",
        "\n",
        "Occur due to upper or lower bounds on the data<br />\n",
        "https://www.mathsisfun.com/definitions/upper-bound.html<br />\n",
        "Mean, Median, and Mode should be mentioned because there is no center in the usual sense<br />\n",
        "\n",
        "**Right Skewed**\n",
        "* Tail is on the right side\n",
        "* Mode Median Mean\n",
        "* Data have a lower bound\n",
        "\n",
        "**Left Skewed**\n",
        "* Tail is on the left\n",
        "* Mean Median Mode\n",
        "* Data have an upper bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61543d9b",
      "metadata": {
        "id": "61543d9b"
      },
      "outputs": [],
      "source": [
        "# # creating skewed data\n",
        "# # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skewnorm.html\n",
        "# from scipy.stats import skewnorm\n",
        "\n",
        "# a = 4 # skewness parameter: positive values are right skewed, negative values are left skewed\n",
        "# X = skewnorm.rvs(a, size=100)\n",
        "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 5))\n",
        "# ax1.hist(X)\n",
        "# ax2.boxplot(X)\n",
        "# sns.violinplot(ax=ax3, data=X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37d57f7f",
      "metadata": {
        "id": "37d57f7f"
      },
      "source": [
        "### Kurtosis\n",
        "\n",
        "https://www.investopedia.com/terms/p/platykurtic.asp\n",
        "\n",
        "* Mesokurtic: Extreme events are rare, resembles normal distribution\n",
        "* Platykurtic: Excess kurtosis is negative (< 3) and has thinner tails. Fewer extreme events. In finance, risk-averse investors might perfer platykurtic distributions\n",
        "* Leptokurtic: Excess kurtosis is greater than 3 and relates to the heaviness of the distribution tails caused by extreme events or outliers.\n",
        "\n",
        "According to Investopedia (2022):\n",
        "\n",
        "> Risk-seeking investors can focus on investments whose returns follow a leptokurtic distribution, to maximize the chances of rare events—both positive and negative (para 3).\n",
        "\n",
        "Sources:\n",
        "* Leptokurtic Definition. (2022, February 1). In *Investopedia*. https://www.investopedia.com/terms/l/leptokurtic.asp.\n",
        "* https://medium.com/@filip.sekan/4-ways-how-to-shape-histograms-appearance-a87764df1417\n",
        "* https://miro.medium.com/v2/resize:fit:1100/format:webp/1*m2X-C-IMcYORq7G5mzvAzg.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf37501c",
      "metadata": {
        "id": "bf37501c"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from scipy.stats import norm\n",
        "\n",
        "# # generate a normal distribution\n",
        "# normal_dist = norm(0, 1)\n",
        "# normal_samples = normal_dist.rvs(10000)\n",
        "\n",
        "# # generate a leptokurtic distribution\n",
        "# leptokurtic_dist = norm(loc=0, scale=0.5)\n",
        "# leptokurtic_samples = leptokurtic_dist.rvs(10000)\n",
        "\n",
        "# # generate a platykurtic distribution\n",
        "# platykurtic_samples = normal_samples + np.random.randn(10000)\n",
        "\n",
        "# # plot the distributions\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.hist(normal_samples, bins=100, alpha=0.5, label='Normal distribution')\n",
        "# plt.hist(leptokurtic_samples, bins=100, alpha=0.5, label='Leptokurtic distribution')\n",
        "# plt.hist(platykurtic_samples, bins=100, alpha=0.5, label='Platykurtic distribution')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c052ccd",
      "metadata": {
        "id": "4c052ccd"
      },
      "source": [
        "https://www.kaggle.com/getting-started/170781\n",
        "\n",
        "Skewness essentially measures the symmetry of the distribution, while kurtosis determines the heaviness of the distribution tails.\n",
        "\n",
        "The topic of Kurtosis has been controversial for decades now, the basis of kurtosis all these years has been linked with the peakedness but the ultimate verdict is that outliers (fatter tails) govern the kurtosis effect far more than the values near the mean (peak).\n",
        "\n",
        "https://towardsdatascience.com/skewness-kurtosis-simplified-1338e094fc85"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0327526",
      "metadata": {
        "id": "e0327526"
      },
      "source": [
        "### Moments\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  If the function is a probability distribution, then the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis (para 1).\n",
        "\n",
        "Moment (mathematics). (2022, January 31). In *Wikipedia*. https://en.wikipedia.org/wiki/Moment_(mathematics).\n",
        "\n",
        "Mean:<br />\n",
        "$\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$v\n",
        "\n",
        "Variance:<br />\n",
        "$s^2 = \\frac{\\sum(x-\\bar{x})^2}{n-1}$\n",
        "\n",
        "Skewness:<br />\n",
        "$\\frac{\\frac{1}{n}\\sum(x - \\mu)^3}{\\sigma^3}$\n",
        "\n",
        "Kurtosis:<br />\n",
        "$\\frac{\\frac{1}{n}\\sum(x - \\mu)^4}{\\sigma^4}$\n",
        "\n",
        "More reading: https://gregorygundersen.com/blog/2020/04/11/moments/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3713892",
      "metadata": {
        "id": "d3713892"
      },
      "source": [
        "## Correlation\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. In the broadest sense correlation is any statistical association, though it actually refers to the degree to which a pair of variables are linearly related. Familiar examples of dependent phenomena include the correlation between the height of parents and their offspring, and the correlation between the price of a good and the quantity the consumers are willing to purchase... Correlations - 2 are useful because they can indicate a predictive relationship that can be exploited in practice (paras. 1 - 2).\n",
        "\n",
        "Correlation. (2022, February 1). In *Wikipedia*. https://en.wikipedia.org/wiki/Correlation.\n",
        "\n",
        "Correlation does not cause causation. Warm days on the beach, ice cream, and shark bites.\n",
        "\n",
        "Covariance:<br />\n",
        "$cov(x, y) = \\frac{1}{N} \\sum_{i=1}^{N}(x_i - \\bar{x}) (y_i - \\bar{y})$\n",
        "\n",
        "* Shows how variables change together\n",
        "* A measure of correlation\n",
        "* Measures direction\n",
        "\n",
        "Pearson’s r (correlation coefficient):<br />\n",
        "$\\rho_{x,y} = \\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{\\frac{1}{N}\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt\\frac{\\sum(x-\\bar{x})^2}{N}\\sqrt\\frac{\\sum(y-\\bar{y})^2}{N}}  = \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt{\\sum(x-\\bar{x})^2}\\sqrt{\\sum(y-\\bar{y})^2}}$\n",
        "\n",
        "* Shows linear relationship between two continuous variables\n",
        "* How one variable changes as another variable changes\n",
        "* Measures both strength and direction\n",
        "\n",
        "Resources:\n",
        "\n",
        "* https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php\n",
        "* https://www.mygreatlearning.com/blog/covariance-vs-correlation/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # show correlation between the features\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # correlation matrix\n",
        "# sns.set(style=\"white\")\n",
        "\n",
        "# # compute the correlation matrix\n",
        "# corr = X_train._get_numeric_data().corr()\n",
        "\n",
        "# # generate a mask for the upper triangle\n",
        "# mask = np.zeros_like(corr, dtype=bool)\n",
        "# mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# # set up the matplotlib figure\n",
        "# # f, ax = plt.subplots()\n",
        "# f = plt.figure(figsize=(8, 8))\n",
        "\n",
        "# # generate a custom diverging colormap\n",
        "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# # draw the heatmap with the mask and correct aspect ratio\n",
        "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);\n",
        "\n",
        "# plt.tight_layout()"
      ],
      "metadata": {
        "id": "MR6JK8P3pdYi"
      },
      "id": "MR6JK8P3pdYi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c716c7d4",
      "metadata": {
        "id": "c716c7d4"
      },
      "outputs": [],
      "source": [
        "# # showing correlation of multiple features with one target\n",
        "# X_train.corrwith(y_train, numeric_only=True).plot.bar(\n",
        "#         title = \"Correlation with Target\", fontsize = 15,\n",
        "#         rot = 45, grid = True);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multicollinearity\n",
        "The need to reduce multicollinearity depends on its severity and your primary goal for your regression model. Keep the following three points in mind:\n",
        "* The severity of the problems increases with the degree of the multicollinearity. Therefore, if you have only moderate multicollinearity, you may not need to resolve it.\n",
        "* Multicollinearity affects only the specific independent variables that are correlated. Therefore, if multicollinearity is not present for the independent variables that you are particularly interested in, you may not need to resolve it. Suppose your model contains the experimental variables of interest and some control variables. If high multicollinearity exists for the control variables but not the experimental variables, then you can interpret the experimental variables without problems.\n",
        "* Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.\n",
        "https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/"
      ],
      "metadata": {
        "id": "2nlsDKEgpodc"
      },
      "id": "2nlsDKEgpodc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spurious Correlations\n",
        "\n",
        "* https://www.tylervigen.com/spurious-correlations"
      ],
      "metadata": {
        "id": "Z5qASGtTeEbJ"
      },
      "id": "Z5qASGtTeEbJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SweetViz\n",
        "\n",
        "* Pandas-Profiling\n",
        "* Autoviz\n",
        "* D-Tale\n",
        "* https://towardsdatascience.com/4-libraries-that-can-perform-eda-in-one-line-of-python-code-b13938a06ae\n",
        "* https://pypi.org/project/sweetviz/\n",
        "* https://colab.research.google.com/drive/1-md6YEwcVGWVnQWTBirQSYQYgdNoeSWg?usp=sharing"
      ],
      "metadata": {
        "id": "a7LoxB7pDknf"
      },
      "id": "a7LoxB7pDknf"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}