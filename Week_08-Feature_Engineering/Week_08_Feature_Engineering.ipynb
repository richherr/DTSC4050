{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC4050/blob/main/Week_08-Feature_Engineering/Week_08_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc4062f",
      "metadata": {
        "id": "7cc4062f"
      },
      "source": [
        "# Week 08 - Feature Engineering\n",
        "\n",
        "Name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC4050 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Take attendance\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link\n"
      ],
      "metadata": {
        "id": "bGezCn18xkLd"
      },
      "id": "bGezCn18xkLd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering Defined\n",
        "\n",
        "Feature engineering is the process of transforming raw data into meaningful features that can be used by machine learning models to improve their accuracy by selecting, manipulating, and creating relevant input variables from the original data, essentially making the data more suitable for prediction tasks; it's considered a crucial step in the machine learning workflow, often involving techniques like data scaling, encoding categorical variables, and combining features to reveal patterns that might not be readily apparent in the raw data. [1, 2, 3, 4, 5, 6]  \n",
        "\n",
        "Key points about feature engineering: [1, 2, 3, 6]  \n",
        "• Goal: To create features that best represent the underlying patterns in the data, allowing the machine learning model to learn more effectively and make better predictions. [1, 2, 3, 6]  \n",
        "* Process steps: [5, 7]  \n",
        "\t* Data cleaning: Handling missing values, outliers, and inconsistencies. [5, 7]  \n",
        "\t* Feature selection: Choosing the most relevant variables from the data. [2, 4, 5]  \n",
        "\t* Feature transformation: Applying mathematical operations like scaling, normalization, or log transformation to features. [1, 3, 5]  \n",
        "\t* Feature creation: Combining existing features to create new, potentially more predictive features. [1, 2, 3]  \n",
        "\t* Encoding: Transforming categorical data into numerical representations. [3, 5, 7]  \n",
        "\n",
        "* Importance: Feature engineering can significantly impact the performance of a machine learning model, often making the difference between good and poor predictions. [2, 3, 5]  \n",
        "\n",
        "Generative AI is experimental.\n",
        "\n",
        "* [1] https://www.ibm.com/think/topics/feature-engineering\n",
        "* [2] https://online.msoe.edu/engineering/blog/importance-of-feature-engineering-in-machine-learning\n",
        "* [3] https://www.databricks.com/glossary/feature-engineering\n",
        "* [4] https://builtin.com/articles/feature-engineering\n",
        "* [5] https://www.analyticsvidhya.com/blog/2021/10/a-beginners-guide-to-feature-engineering-everything-you-need-to-know/\n",
        "* [6] https://medium.com/@jaberi.mohamedhabib/a-comprehensive-guide-to-feature-engineering-definition-importance-and-example-ccab74a5f83a\n",
        "* [7] https://www.linkedin.com/pulse/feature-engineering-eda-shubhra-rana-yqhlc\n",
        "* [-] https://medium.com/@simranjeetsingh1497/the-ultimate-guide-to-machine-learning-from-eda-to-model-deployment-part-2-e56ac58785f8\n"
      ],
      "metadata": {
        "id": "3C8sU032WKG6"
      },
      "id": "3C8sU032WKG6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Historical Context"
      ],
      "metadata": {
        "id": "Lqmen7jhGw4t"
      },
      "id": "Lqmen7jhGw4t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Astronomy and the Law of Error\n",
        "\n",
        "In 18th century astronomy, the \"law of error\" was a developing concept that dealt with the inherent uncertainty in astronomical observations, where scientists like Tobias Mayer and Alexis-Claude Clairaut were grappling with the limitations of their instruments and the need to account for small discrepancies between predicted and observed planetary motions, particularly regarding the Moon's movement, which was crucial for navigation; this laid the groundwork for the later formalization of the \"law of errors\" by Carl Friedrich Gauss, which established that the most likely value of a measurement is the average of multiple observations, with smaller errors occurring more frequently than large ones. [1, 2, 3, 4, 5]  \n",
        "\n",
        "Key points about astronomy and the law of error in the 18th century: [3, 4, 5]  \n",
        "* Challenges with Lunar Motion: One major challenge in 18th century astronomy was accurately predicting the Moon's motion due to the gravitational perturbations from the Sun, leading scientists to try to refine Newton's law of gravity by adding small corrections to better explain the observed discrepancies. [3, 4, 5]  \n",
        "* Importance of Observation and Data Analysis: Astronomers like Mayer recognized the importance of careful observation and analyzing multiple data points to understand the inherent errors in their measurements. [3, 5, 6]  \n",
        "* Early Concepts of Error Distribution: While not explicitly formulated as a \"law of error\" at the time, the idea that smaller errors are more likely than larger ones was starting to be understood through analysis of observational data. [2, 4, 5]  \n",
        "* Impact on Navigation: Accurately determining longitude at sea relied on precise astronomical observations, particularly of the Moon, which made the study of lunar motion and associated errors a critical area of research. [3, 4, 5]  \n",
        "\n",
        "Generative AI is experimental.\n",
        "\n",
        "* [1] https://www.teachastronomy.com/textbook/How-Science-Works/Measurement-Errors/\n",
        "* [2] https://www.actuaries.digital/2021/03/31/gauss-least-squares-and-the-missing-planet/\n",
        "* [3] https://www.rmg.co.uk/stories/topics/longitude-found-nevil-maskelyne-lunar-method\n",
        "* [4] https://www.britannica.com/science/astronomy/Enlightenment\n",
        "* [5] https://pubs.aip.org/physicstoday/article/63/1/27/413282/The-18th-century-battle-over-lunar-motionIn-a\n",
        "* [6] https://www.sciencedirect.com/science/article/pii/S2211379722008282\n",
        "\n"
      ],
      "metadata": {
        "id": "e1caIeWYVJJZ"
      },
      "id": "e1caIeWYVJJZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adolphe Quetelet\n",
        "\n",
        "* 1796 - 1874\n",
        "*   **Application of probability to human affairs** Quetelet believed that probability profoundly influenced human affairs, even more so than his contemporaries believed.\n",
        "*   **Applying the law of error to human beings** Inspired by astronomy, Quetelet thought the law of error could be applied to human measurements.\n",
        "*   **The \"average man\"** Quetelet aimed to determine the average physical and intellectual traits of a population by gathering \"facts of life,\" which could then be graphically represented as bell-shaped curves. This \"average man\" served as a benchmark against which individual behavior could be assessed. However, some critics argued that an individual who is average in all dimensions might not be biologically feasible.\n",
        "*   **Social mechanics** Quetelet championed a new science, called social mechanics, focused on mapping normal physical and moral characteristics. In 1835, he published *A Treatise on Man, and the Development of His Faculties*, detailing the influence of probability on human affairs.\n",
        "*   **Normal distribution in human measurements** Quetelet demonstrated that diverse human measurements, such as the heights of French conscripts and the chest sizes of Scottish soldiers, followed a normal distribution.\n",
        "*   **Use of the normal curve beyond error law** Quetelet was the first to use the normal curve in contexts other than error laws.\n",
        "*   **Studies of crime and social determinism** His work on the consistency of crimes stimulated discussions about free will versus social determinism.\n",
        "*   **Data collection and census improvements** Quetelet collected and analyzed statistics on crime and mortality for the government and improved census methods.\n",
        "*   **Quetelet Index** He developed the Quetelet index, now used internationally as a measure of obesity. The Quetelet index is calculated as weight in kilograms divided by the square of height in meters; a value greater than 30 indicates obesity.\n",
        "*   **International Collaboration** Quetelet organized the first international statistics conference in 1853 and helped form the Statistical Society of London, the International Statistics Congresses, and the Statistical Section of the British Association for the Advancement of Science. He was also the first foreign member of the American Statistical Association.\n",
        "*   **Racial Differences** Quetelet viewed average physical and mental qualities as real properties awaiting discovery, which gave strength to ideas of racial differences in 19th-century European thought.\n",
        "*   **Eugenics** Quetelet's normal curve provided a scale to grade people. When Galton used the curve, he predicted that it would always apply to \"men of the same race\".\n",
        "\n",
        "Quetelet's use of the \"average man\" concept relied on the idea of the \"persistence of causes,\" suggesting that the average of large datasets would remain stable if the underlying causal relationships did not change. Quetelet believed that as the number of observations increased, individual peculiarities would diminish, allowing general societal facts to become more prominent.\n"
      ],
      "metadata": {
        "id": "qT6i_pz2UHP0"
      },
      "id": "qT6i_pz2UHP0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Francis Galton\n",
        "\n",
        "* 1820 - 1911\n",
        "* Francis Galton and Correlation:\n",
        "    * Galton is recognized for discovering the concept of correlation.\n",
        "    * His work stemmed from his research on heredity, particularly how physical traits like height are passed down.\n",
        "    * He defined \"co-relation\" as the tendency for variables to vary together in a consistent direction.\n",
        "* Eugenics and Galton's Beliefs:\n",
        "    * Galton coined the term \"eugenics\" and advocated for \"race betterment.\"\n",
        "    * He believed in a racial hierarchy.\n",
        "    * He promoted selective breeding, encouraging it among elites and discouraging it among those he deemed \"undesirable\" (e.g., those with \"lunacy, feeble-mindedness, habitual criminality, and pauperism\").\n",
        "* The Connection Between Eugenics and Statistics (How Eugenics Shaped Statistics - Nautilus):\n",
        "    * Statistical methods, including significance testing, were developed in part to support eugenicist goals, such as identifying perceived racial differences.\n",
        "    * This historical connection reveals the deep intertwining of statistical thinking and eugenicist ideology.\n"
      ],
      "metadata": {
        "id": "X9f8j-EwG0Qi"
      },
      "id": "X9f8j-EwG0Qi"
    },
    {
      "cell_type": "markdown",
      "id": "715565d5",
      "metadata": {
        "id": "715565d5"
      },
      "source": [
        "### Pearson\n",
        "\n",
        "Content for the following individuals draw heavily on Wikipedia and The Book of Why Chapter 2 by Judea Pearl and Dana Mackenzie\n",
        "\n",
        "* 1857 - 1936\n",
        "* An English mathematician and biostatistician\n",
        "* That's Karl Pearson with a C\n",
        "* Spent most of the 1880s in Germany / Austria\n",
        "* Loved Germany so much he changed his name from Carl to Karl\n",
        "* A women's right and equality activist, founder of the Men and Women's Club\n",
        "* A socialist that offered help translating some of Karl Marx's work (Das Kapital)\n",
        "* Secured a grant for a biometrics lab at the University of College London\n",
        "* The lab became a department when Galton passed and left an endowment for a professorship as long as Pearson was the first holder\n",
        "* Had to explain what he calls genuine (organic) correlation and spurious correlation\n",
        "* For example, there's a strong correlation between a country's chocolate consumption and Nobel Prize winners\n",
        "* Pearson felt that Galton did away with causation and 1 was just perfect correlation\n",
        "* Data is all there is to science\n",
        "* Galton says that relationships didn't need a casual explanation\n",
        "* Pearson went further by removing causation from science\n",
        "* Pearson belonged to the Positivist School which holds that the universe is a product of human thought and that science is just the expression of these thoughts\n",
        "* Thus causation, outside our thoughts, does not exist\n",
        "* Thoughts can only reflect patterns of observations and can be completely described by correlations\n",
        "* Laws of Nature as Descriptive, Not Causal: Pearson viewed the laws of nature as tools for making accurate predictions and for concisely describing trends in observed data, rather than identifying biological mechanisms. He saw causation as simply the experience \"that a certain sequence has occurred and recurred in the past\".\n",
        "* Emphasis on Mathematical Descriptions: Pearson believed that biologists should focus on providing mathematical descriptions of empirical data rather than trying to identify particular mechanisms of genetics.\n",
        "* Criticism of Biologists' Speculation: He criticized biologists who, in his view, succumbed to \"almost metaphysical speculation as to the causes of heredity,\" which replaced the process of experimental data collection. Pearson stressed the importance of statistical validity in theories. He stated that \"before we can accept any cause of a progressive change as a factor we must have not only shown its plausibility but if possible have demonstrated its quantitative ability\".\n",
        "* Idealism and the Role of the Mind: Pearson's perspective was rooted in idealism, emphasizing ideas or pictures in a mind. He asserted that science is essentially a classification and analysis of the contents of the mind, and scientific law is a product of the human mind with no meaning apart from man.\n",
        "* Pearson's focus was on using statistical methods to reveal fundamental truths about people, presenting them as unquestionable as the law of gravity. He believed that by allowing numbers to tell their own story, objective truths could be revealed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0e4eaf",
      "metadata": {
        "id": "dd0e4eaf"
      },
      "source": [
        "### R. A. Fisher\n",
        "\n",
        "* 1890 - 1962\n",
        "* Worked as a statistician in the City of London and taught physics and maths\n",
        "* Popularized the p-value\n",
        "* Linear discriminant analysis\n",
        "* F-distribution\n",
        "* Student's t-distribution\n",
        "* He was from an early age a supporter of certain eugenic ideas, and it is for this reason that he has been accused of being a racist and an advocate of forced sterilisation (Evans 2020). His promotion of eugenics has recently caused various organisations to remove his name from awards and dedications of buildings (Tarran 2020; Rothamsted Research 2020; Society for the Study of Evolution 2020; Gonville and Caius College 2020). https://www.nature.com/articles/s41437-020-00394-6\n",
        "Fisher's Eugenics Background: Ronald Fisher was a committed eugenicist. He was the chair of the Cambridge Eugenics Society as a student. Fisher wrote prolifically for The Eugenics Review. He was the Galton Professor of Eugenics at University College London and editor of the Annals of Eugenics.\n",
        "* Fisher's Statistical Contributions: Fisher is responsible for key statistical terms and concepts including \"parameter estimation,\" \"maximum likelihood,\" and \"sufficient statistic\". His 1925 textbook, Statistical Methods for Research Workers, introduced significance testing. Fisher formalized the use of the p-value in statistics. He proposed p=0.05 as a limit for statistical significance. Fisher supplied statistical tests like Fisher's F-test, ANOVA (analysis of variance), and Fisher's exact test.\n",
        "* Fisher promoted significance testing as a way to decide all manner of questions, viewing it as having an objective basis.\n",
        "* Galton, Pearson, and Fisher needed a quantitative way to argue for the existence of eugenic differences, and they used significance testing to do so.\n",
        "* Significance testing was useful for stating that racial subgroups existed, or that there was a \"significant\" correlation between intelligence and cleanliness, or a \"significant\" difference in criminality, fertility, or disease between socioeconomic classes.\n",
        "* Fisher’s eugenicist proposals aimed to exclude \"inferior types\" from the statistics profession.\n",
        "* In 1904, Karl Pearson published a study that reported similar correlations between inherited traits like eye color and mental qualities like \"vivacity\" among siblings. He concluded that these traits were equally hereditary, leading to eugenicist conclusions about breeding intelligence.\n",
        "* Pearson argued that an asymmetry in skull measurements indicated different races, further arguing for racial superiority.\n",
        "* Criticism of Statistical Significance: Critics argue that a scientific hypothesis is more than a statistical hypothesis, and should explain why, by how much, and why it matters. Significance testing only asks \"whether\" an effect or association exists, not how much or why it matters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32f6f0a",
      "metadata": {
        "id": "d32f6f0a"
      },
      "source": [
        "### Sewall Wright and Guinea Pigs\n",
        "\n",
        "* 1889 - 1988\n",
        "* Statistics may be regarded as the study of methods of the reductions of data\n",
        "* Wright argued that statistics was more than just a collection of mathematical methods\n",
        "* Wright went to Harvard to study genetics and about 1915 got a job with the USDA taking care of Guinea Pigs\n",
        "* The Guinea Pigs turned out to be the spring board to Wright's success\n",
        "* Evolution was not gradual, as Darwin posited, but happens in bursts\n",
        "* 1925, Wright was faculty at University of Chicago and stayed close to Guinea Pigs\n",
        "* A story is that he was handling a Gunea Pig while lecturing at the chalk board and mistakenely used the Guinea Pig to erase the board\n",
        "* Guinea Pig coat color refused to play by the genetic understanding of the time\n",
        "* It proved impossible to breed an all white / all colored guinea pig\n",
        "* Even the most inbred had a wide variation\n",
        "* Wright postulated that genetics alone governed coat color and added developmental factors in the womb\n",
        "* Something in the womb was `causing` coat color\n",
        "* 20 generations eliminated the genetic variation while maintaining the developmental factors\n",
        "* Wright's work with guinea pigs contributed to his development of path analysis, which is now used in the social sciences\n",
        "* Path Analysis is a causal modeling approach to exploring the correlations within a defined network. The method is also known as Structural Equation Modeling\n",
        "* https://en.wikipedia.org/wiki/Sewall_Wright#Evolutionary_theory\n",
        "Path analysis can disredit causation but cannot prove causation\n",
        "* Wright, even though right, according to Judea Pearl, was severely attacked at the time by Fisher\n",
        "Sewall Wright's contributions to statistics, particularly his development of path analysis, offer a distinct approach that, in some ways, rivaled Fisher's statistical methods.\n",
        "*   **Path Analysis:** Wright invented path analysis in 1921, which is a statistical method that uses a graphical model and is still widely used in social science. Path analysis is a technique for estimating unknown parameters given a set of simultaneous equations, and of mapping out the interrelations among a pre-determined network of variables. It is credited to biometrician Sewall Wright.\n",
        "\n",
        "*   **Population Genetics**: Wright was a founder of population genetics alongside Ronald Fisher and J. B. S. Haldane. Their theoretical work is the origin of the modern evolutionary synthesis or neo-Darwinian synthesis. Wright, along with Fisher, pioneered methods for computing the distribution of gene frequencies among populations as a result of the interaction of natural selection, mutation, migration and genetic drift.\n",
        "\n",
        "*   **Coefficient of Determination:** Wright is credited with creating the statistical coefficient of determination, first published in 1921. This metric is commonly employed to evaluate regression analyses in computational statistics and machine learning.\n",
        "\n",
        "*   **Inbreeding Coefficient:** Wright discovered the inbreeding coefficient and methods of computing it in pedigree animals and extended this work to populations, computing the amount of inbreeding between members of populations as a result of random genetic drift.\n",
        "\n",
        "*   **Wright's View on Causation:** Wright introduced his method of path coefficients in the context of causality and perhaps unintentionally, forever linked the statistical method with causal issues. Essential to the controversy surrounding Wright’s methods were claims, originally advanced by Wright, that the method could be applied to problems in which causality among variables could be assumed. Wright outlined two ways in which the method of path coefficients may be correctly employed: 1) should one have presumed knowledge of the causal relations inherent in a system of variables, path analysis could be used to find “the degree to which variation of a given effect is determined by each particular cause”, and 2) in those situations in which causal relations were uncertain, the method of path coefficients could be used to deduce the logical consequences inherent in the system.\n",
        "\n",
        "*   **Rivalry with Fisher**:\n",
        "    *   **Interpretation of Population Genetics:** By the mid-1920s, interpretation of the mathematical theories of population genetics became a point of contention between Fisher and Wright, and the issue became acrimonious. Dispute persisted for the rest of Fisher's life.\n",
        "    *   **Genetic Theory of Dominance:** Wright did not accept Fisher's genetic theory of dominance, but instead considered it to arise from biochemical considerations.\n",
        "\n",
        "*   **How Wright's Approach Differed from Fisher's**:\n",
        "    *   **Causal Modeling**: Wright explicitly linked his path analysis to causal inference.\n",
        "    *   **Emphasis on Multiple Factors**: Wright focused on the interaction of genetic drift and other evolutionary forces.\n",
        "    *   **Fitness Landscapes**: Wright described the relationship between genotype or phenotype and fitness as fitness surfaces or evolutionary landscapes.\n",
        "    *   **Holistic View**: Wright's approach incorporated a broader view of evolutionary processes, considering multiple interacting factors and emphasizing the importance of genetic drift alongside natural selection.\n",
        "    *   **Panpsychism**: Wright was one of the few geneticists of his time to venture into philosophy. He endorsed a form of panpsychism, believing that consciousness was an inherent property of elementary particles rather than an emergent property of complexity.\n",
        "\n",
        "While Fisher focused heavily on statistical significance and hypothesis testing, Wright's path analysis provided a method for exploring and visualizing complex relationships among variables, particularly in the context of genetics and evolution. The two scientists also held differing views regarding population genetics and the genetic theory of dominance.\n",
        "\n",
        "Sewall Wright's work with guinea pigs significantly influenced his theories on evolution, offering a perspective that differed from prevailing views at the time.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "*   **Empirical Observations**\n",
        "    *   Wright conducted extensive experiments with approximately 80,000 guinea pigs.\n",
        "    *   He analyzed characters of some 40,000 guinea pigs in 23 strains of brother-sister matings against a random-bred stock.\n",
        "\n",
        "*   **Insights into Genetic Drift**:\n",
        "    *   His observations of guinea pig populations over many years demonstrated \"cumulative accidents of sampling\".\n",
        "    *   Wright noted extreme differences between stocks that had been inbred in parallel for many years.\n",
        "\n",
        "*   **Shifting Balance Theory**:\n",
        "    *   Wright's guinea pig studies contributed to his \"shifting balance\" theory of evolution.\n",
        "    *   This theory posits that optimal conditions for adaptive evolution occur in large populations subdivided into partially isolated groups.\n",
        "    *   In smaller subpopulations, genetic drift has a significant influence, allowing groups to diverge more rapidly.\n",
        "    *   Subpopulations that happen upon a more adaptive combination of alleles will spread and take over the population.\n",
        "\n",
        "*   **Adaptive Landscape**:\n",
        "    *   Wright introduced the concept of an \"adaptive landscape\" with fitness peaks and maladaptive valleys.\n",
        "    *   He argued that large, freely mixing populations tend to get trapped in locally adaptive peaks, while subdivided populations can more efficiently explore the broader landscape of gene combinations.\n",
        "    *   According to Wright, small populations would be able to drift away from a locally adaptive peak in the fitness landscape, across a saddle to scale an even higher peak.\n",
        "\n",
        "*   **Differences from prevailing evolutionary thinking**:\n",
        "    *   **Emphasis on Genetic Drift**: Wright emphasized the role of random genetic drift, which he sometimes referred to as the Sewall Wright effect.\n",
        "    * **Genetic drift** refers to **cumulative, stochastic (random) changes in gene frequencies** that arise from random births, deaths, and Mendelian segregations in reproduction. It is also sometimes known as the Sewall Wright effect. Sewall Wright's guinea pig studies demonstrated genetic drift. Wright emphasized the role of random genetic drift as an important evolutionary force, in addition to natural selection.\n",
        "    *   **Mendelian segregations** are a component of genetic drift.\n",
        "    *   **Genetic drift** refers to cumulative, stochastic (random) changes in gene frequencies that arise from random births, deaths, and **Mendelian segregations** in reproduction.\n",
        "    *   **Interaction of Evolutionary Forces**: Wright was convinced that the interaction of genetic drift and other evolutionary forces was important in adaptation, not simply natural selection.\n",
        "    *   **Subdivided Populations**: He theorized that subdivided populations could more efficiently explore gene combinations, which runs contrary to large, freely mixing populations.\n",
        "\n",
        "*   **Conflict with Fisher**: Wright's shifting balance theory and emphasis on genetic drift led to conflict with R.A. Fisher, another pioneer in population genetics. Fisher believed that most natural populations were too large for the effects of genetic drift to be significant.\n",
        "\n",
        "Wright's guinea pig experiments provided empirical evidence for the power of genetic drift and the importance of population structure in evolution. His theories offered a nuanced perspective that took into account the interplay of multiple evolutionary forces, which contrasted with views that prioritized natural selection as the primary driver of adaptation.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mendel\n",
        "\n",
        "* 1824 - 1888\n",
        "* **Mendel's laws of inheritance**, established by Gregor Mendel through experiments with pea plants, include the \"Law of Dominance,\" the \"Law of Segregation,\" and the \"Law of Independent Assortment,\" which explain how traits are passed from parents to offspring, stating that each individual carries two alleles for a trait, with one allele being passed on to each offspring during gamete formation, and that different traits are inherited independently of one another.\n",
        "*   **Mendel's laws of inheritance** were reconciled with Darwin's vision of natural selection through formal frameworks such as Sewall Wright’s “Evolution in Mendelian populations”.\n",
        "*   **Mendel's principles were rediscovered in 1900**, which resulted in a conflict between the biometricians, who followed Galton's Law of Ancestral Heredity, and those who advocated for Mendel's principles.\n",
        "*   Ronald Fisher's 1930 book *The Genetical Theory of Natural Selection* helped reconcile **Mendelian genetics with Darwinian evolution**.\n",
        "*   In 1944, Fisher used a Pearson's chi-squared test to analyze Mendel's data and concluded that **Mendel's results were far too perfect**, suggesting that adjustments (intentional or unconscious) had been made to the data to make the observations fit the hypothesis."
      ],
      "metadata": {
        "id": "WRb5VnLYOTgd"
      },
      "id": "WRb5VnLYOTgd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Darwin\n",
        "\n",
        "* 1809 - 1882\n",
        "* Evolution by natural selection\n",
        "  * within a species, individuals exhibit variation in traits\n",
        "  * these variations are heritable (passed on to offspring)\n",
        "  * more offspring are produced than can survive, leading to a \"struggle for existence\"\n",
        "  * individuals with advantageous traits for their environment are more likely to survive and reproduce, causing these beneficial traits to become more common in the population over time, potentially leading to new species through gradual change. [1, 2, 3, 4, 5]\n",
        "* Variation exists: Individuals within a population have natural variations in their traits. [3, 5, 6]  \n",
        "* Inheritance: These variations are passed on from parents to offspring through genetic material. [3, 4, 5]  \n",
        "* Competition for survival: Due to overpopulation, there is a struggle for limited resources, leading to competition among individuals. [4, 5, 6]  \n",
        "* Natural selection: Individuals with advantageous traits are more likely to survive and reproduce, passing on those traits to their offspring. [3, 4, 5]  \n",
        "* Gradual change: Over many generations, these small variations accumulate, leading to significant changes in a population and potentially the emergence of new species. [2, 5, 6]  \n",
        "* Darwin and eugenics: https://hsm.stackexchange.com/questions/3328/did-darwin-ever-express-his-views-on-eugenics\n",
        "\n",
        "Generative AI is experimental.\n",
        "\n",
        "* [1] https://www.pewresearch.org/religion/2009/02/04/darwin-and-his-theory-of-evolution/\n",
        "* [2] https://www.khanacademy.org/science/ap-biology/natural-selection/natural-selection-ap/a/darwin-evolution-natural-selection\n",
        "* [3] https://en.wikipedia.org/wiki/Darwinism\n",
        "* [4] https://study.com/learn/lesson/darwins-theory-of-natural-selection-concept-overview.html\n",
        "* [5] https://www.open.edu/openlearn/nature-environment/natural-history/evolution-through-natural-selection/content-section-4.1\n",
        "* [6] https://en.wikipedia.org/wiki/On_the_Origin_of_Species\n",
        "* [-] https://anthroholic.com/theories-of-evolution\n"
      ],
      "metadata": {
        "id": "DEAxtFWhOI24"
      },
      "id": "DEAxtFWhOI24"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Reconciliation\n",
        "\n",
        "\"Initially, the groundbreaking work of Gregor Mendel on inheritance and Charles Darwin's theory of natural selection were perceived as distinct, and at times, conflicting concepts. Mendel's meticulous experiments with pea plants revealed the fundamental principles of inheritance, demonstrating the transmission of traits through discrete units, now known as genes (Mendel, 1866). Conversely, Darwin's theory of natural selection posited that species evolve through the differential survival and reproduction of individuals with advantageous traits (Darwin, 1859).\n",
        "\n",
        "However, a pivotal shift occurred when scientists recognized the complementary nature of these theories, forming a more comprehensive understanding of evolutionary processes. Darwin's theory elucidated how favorable traits proliferate within a population, yet it lacked a clear explanation of the inheritance mechanism (Darwin, 1859). Mendel's laws, on the other hand, provided this crucial missing link, elucidating the precise manner in which traits are passed across generations (Mendel, 1866).\n",
        "\n",
        "This reconciliation, often referred to as the modern evolutionary synthesis, underscored that Mendelian inheritance serves as the fundamental genetic mechanism underpinning Darwinian natural selection. In essence, Mendel's laws explained the 'how' of trait inheritance, while Darwin's theory explained the 'why' of evolutionary change over time, resulting from the selection of those inherited traits.\"\n",
        "\n",
        "**General References for Further Reading:**\n",
        "\n",
        "* **Darwin, C. (1859). *On the origin of species by means of natural selection, or the preservation of favoured races in the struggle for life*. John Murray.** (This is the foundational text for Darwin's theory of natural selection.)\n",
        "* **Mendel, G. (1866). Versuche über Pflanzen-Hybriden. *Verhandlungen des naturforschenden Vereines in Brünn, IV. Abhandlungen*, 3-47.** (This is Mendel's original paper on his pea plant experiments, a key work in genetics.)\n",
        "* **Any modern Evolutionary biology or Genetics Textbook.** (These will have sections covering the work of both scientists and the Modern Synthesis.)\n",
        "* **Biographies of Charles Darwin and Gregor Mendel.** (These provide valuable context and historical perspective.)\n",
        "\n"
      ],
      "metadata": {
        "id": "9ESsXCDfrhLa"
      },
      "id": "9ESsXCDfrhLa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sewall Wright Comparison\n",
        "\n",
        "\"While both Sewall Wright and the combined contributions of Mendel and Darwin have profoundly shaped our understanding of evolution, their works differ significantly in focus, emphasis, key concepts, and methods.\n",
        "\n",
        "**Focus:**\n",
        "\n",
        "The collaborative work of Mendel and Darwin centers on the fundamental mechanisms of heredity, as elucidated by Mendel's laws, and the subsequent evolutionary changes driven by natural selection, as outlined by Darwin (e.g., Mendel, 1866; Darwin, 1859). In contrast, Sewall Wright's research delves into the intricate complexities of evolutionary processes, encompassing factors such as genetic drift, population structure, and the interplay of multiple evolutionary forces (e.g., Wright, 1931).\n",
        "\n",
        "**Emphasis:**\n",
        "\n",
        "Mendel and Darwin's work emphasizes the role of variation and selection in driving gradual evolutionary changes within populations over time (e.g., Darwin, 1859). Wright, however, underscored the significance of random genetic drift and the impact of population structure, offering a more nuanced perspective on evolutionary dynamics (e.g., Wright, 1931).\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "The core concepts associated with Mendel and Darwin include genes, alleles, inheritance patterns, variation, natural selection, and adaptation (e.g., Darwin, 1859). Sewall Wright introduced pivotal concepts such as genetic drift, the \"shifting balance\" theory, adaptive landscapes, and path analysis (e.g., Wright, 1931).\n",
        "\n",
        "**Methods:**\n",
        "\n",
        "Mendel employed controlled breeding experiments with pea plants, while Darwin relied on meticulous observation and comparative analysis (e.g., Mendel, 1866; Darwin, 1859). Wright's methodologies encompassed both empirical studies, such as his experiments with guinea pigs, and sophisticated theoretical modeling, including path analysis (e.g., Wright, 1931).\n",
        "\n",
        "In essence, Mendel and Darwin laid the groundwork for understanding the fundamental principles of inheritance and natural selection, whereas Sewall Wright expanded upon this foundation by exploring the more intricate and multifaceted aspects of evolutionary processes.\"\n",
        "\n",
        "**General References for Further Reading:**\n",
        "\n",
        "* **Darwin, C. (1859). *On the origin of species by means of natural selection, or the preservation of favoured races in the struggle for life*. John Murray.** (This is the foundational text for Darwin's theory of natural selection.)\n",
        "* **Mendel, G. (1866). Versuche über Pflanzen-Hybriden. *Verhandlungen des naturforschenden Vereines in Brünn, IV. Abhandlungen*, 3-47.** (This is Mendel's original paper on his pea plant experiments, a key work in genetics.)\n",
        "* **Wright, S. (1931). Evolution in Mendelian populations. *Genetics*, *16*(2), 97–159.** (This is a significant paper by Wright that introduced many of his key ideas about population genetics.)\n",
        "* **Provine, W. B. (1986). *Sewall Wright: geneticist and evolutionist*. University of Chicago Press.** (This is a comprehensive biography of Sewall Wright.)\n",
        "* **Any modern Evolutionary biology or Genetics Textbook.** (These will have sections covering the work of all three scientists.)\n"
      ],
      "metadata": {
        "id": "7NKn9kxisQNZ"
      },
      "id": "7NKn9kxisQNZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sources\n",
        "\n",
        "Here is a list of the sources used for these talking points:\n",
        "\n",
        "*   **Complete Guide to Categorical Features Encoding Guide:** [https://github.com/utkarshkant/Helpful-Python/blob/master/Encoding\\_Categorical\\_Features.ipynb](https://github.com/utkarshkant/Helpful-Python/blob/master/Encoding_Categorical_Features.ipynb)\n",
        "*   **Confounding - Wikipedia:** The general URL for Wikipedia is [https://en.wikipedia.org](https://en.wikipedia.org), but a specific URL for the confounding page was not provided.\n",
        "*   **Correlation - Wikipedia:** The general URL for Wikipedia is [https://en.wikipedia.org](https://en.wikipedia.org), but a specific URL for the correlation page was not provided.\n",
        "*   **Feature engineering - Wikipedia:** [https://en.wikipedia.org/w/index.php?title=Feature\\_engineering&oldid=1264344303](https://en.wikipedia.org/w/index.php?title=Feature_engineering&oldid=1264344303)\n",
        "*   **Francis Galton - Wikipedia:** [https://en.wikipedia.org/w/index.php?title=Francis\\_Galton&oldid=1276619382](https://en.wikipedia.org/w/index.php?title=Francis_Galton&oldid=1276619382)\n",
        "*   **How Eugenics Shaped Statistics - Nautilus:** The article is part of Nautilus Magazine, accessible via their website.\n",
        "*   **Karl Pearson - Wikipedia:** The general URL for Wikipedia is [https://en.wikipedia.org](https://en.wikipedia.org), but a specific URL for the Karl Pearson page was not provided.\n",
        "*   **Multicollinearity - Wikipedia:** [https://en.wikipedia.org/w/index.php?title=Multicollinearity&oldid=1262962229](https://en.wikipedia.org/w/index.php?title=Multicollinearity&oldid=1262962229)\n",
        "*   **One-hot - Wikipedia:** [https://en.wikipedia.org/w/index.php?title=One-hot&oldid=1255080026](https://en.wikipedia.org/w/index.php?title=One-hot&oldid=1255080026)\n",
        "*   **Outlier - Wikipedia:** [https://en.wikipedia.org/w/index.php?title=Outlier&oldid=1274763599](https://en.wikipedia.org/w/index.php?title=Outlier&oldid=1274763599)\n",
        "*   **Pearson correlation coefficient - Wikipedia:** [https://en.wikipedia.org/w/index.php?title=Pearson\\_correlation\\_coefficient&oldid=1276331429](https://en.wikipedia.org/w/index.php?title=Pearson_correlation_coefficient&oldid=1276331429)\n",
        "*   **Ronald Fisher - Wikipedia:** The general URL for Wikipedia is [https://en.wikipedia.org](https://en.wikipedia.org), but a specific URL for the Ronald Fisher page was not provided.\n",
        "*   **Sewall Wright - Wikipedia:** The general URL for Wikipedia is [https://en.wikipedia.org](https://en.wikipedia.org), but a specific URL for the Sewall Wright page was not provided.\n",
        "*   **Spurious relationship - Wikipedia:** [https://en.wikipedia.org/w/index.php?title=Spurious\\_relationship&oldid=1258682470](https://en.wikipedia.org/w/index.php?title=Spurious_relationship&oldid=1258682470)\n",
        "*   **Facing History & Ourselves:** [https://www.facinghistory.org](https://www.facinghistory.org)\n",
        "*   **Dummy Variable Trap – LearnDataSci:** The LearnDataSci homepage is located at: [https://www.learndatasci.com](https://www.learndatasci.com)\n",
        "*   **Normalization vs. Standardization: Key Differences Explained | DataCamp:** The DataCamp homepage is located at: [https://www.datacamp.com](https://www.datacamp.com)\n",
        "*   **Lurking Variable: Definition & Examples - Statistics By Jim:** The Statistics By Jim homepage is located at: [https://statisticsbyjim.com](https://statisticsbyjim.com)\n",
        "\n",
        "*   \"Adolphe Quetelet (1796-1874)\" https://en.wikipedia.org/wiki/Adolphe_Quetelet\n",
        "*   \"ORIGINS OF PATH ANALYSIS: Causal Modeling and the Origins of Path Analysis\" https://www.sciencedirect.com/topics/economics-econometrics-and-finance/path-analysis\n",
        "*   \"Sewall Wright: Evolving Mendel – Genes to Genomes\" https://genestogenomes.org/sewall-wright-evolving-mendel/\n",
        "*   \"Imputation (statistics) - Wikipedia\" https://en.wikipedia.org/wiki/Imputation_(statistics)\n"
      ],
      "metadata": {
        "id": "AvukL7_XY5If"
      },
      "id": "AvukL7_XY5If"
    },
    {
      "cell_type": "markdown",
      "id": "0ea41731",
      "metadata": {
        "id": "0ea41731"
      },
      "source": [
        "## Correlation\n",
        "\n",
        "Galton's Statistical Innovations: In his efforts to study heredity and eugenics, Galton independently rediscovered the concept of correlation in 1888 and demonstrated its application in the study of heredity, anthropology, and psychology. Galton invented the use of the regression line and chose \"r\" (for reversion or regression) to represent the correlation coefficient. His work examining forearm and height measurements led to these discoveries. As early as 1877, Galton was using the term \"reversion\" and the symbol \"r\" for what would become \"regression\".\n",
        "\n",
        "Previous discovery: The concept of correlation had actually been introduced earlier by Auguste Bravais in 1846. However, Bravais' work was not widely known or recognized at the time.\n",
        "\n",
        "**Note on Regression**\n",
        "\n",
        "It's very common to experience fluctuations in mood and energy, with \"good days\" and \"bad days.\" When we connect this to the statistical concept of \"regression,\" we're essentially talking about how, over time, extreme highs and lows tend to move back towards a more average or typical state. Here's a simplified example:\n",
        "\n",
        "**Example: Daily Productivity**\n",
        "\n",
        "* **Scenario:**\n",
        "    * Let's say you track your daily productivity on a scale of 1 to 10.\n",
        "    * Occasionally, you have exceptionally \"good days\" where you're incredibly focused and productive (a 10).\n",
        "    * Similarly, you have \"bad days\" where you feel sluggish and accomplish very little (a 1).\n",
        "    * Most days, however, you fall somewhere in the middle (around a 5 or 6).\n",
        "* **Regression to the Mean:**\n",
        "    * If you have a string of exceptionally productive days (several 9s or 10s in a row), it's likely that your productivity will eventually regress towards your average. You probably won't maintain that peak level indefinitely.\n",
        "    * Conversely, if you experience a series of unproductive days (several 1s or 2s), your productivity will likely improve and move back towards your average.\n",
        "    * This is the regression to the mean. It means that extreme values are likely to be followed by values that are closer to the average.\n",
        "* **Real-World Application:**\n",
        "    * This concept applies to many areas of life, including:\n",
        "        * **Mood:** After a period of intense happiness or sadness, our mood tends to stabilize.\n",
        "        * **Performance:** An athlete who has an exceptionally good game is likely to perform closer to their average in subsequent games.\n",
        "        * **Health:** A person whose blood pressure spikes very high is likely to see their blood pressure decrease toward their average.\n",
        "\n",
        "In essence, regression to the mean reminds us that extreme fluctuations are often temporary, and things tend to return to a more typical state over time.\n",
        "\n",
        "\n",
        "In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it usually refers to the degree to which a pair of variables are linearly related. Familiar examples of dependent phenomena include the correlation between the height of parents and their offspring, and the correlation between the price of a good and the quantity the consumers are willing to purchase, as it is depicted in the so-called demand curve.\n",
        "\n",
        "Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation).\n",
        "\n",
        "https://en.wikipedia.org/wiki/Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f5a5a3",
      "metadata": {
        "id": "14f5a5a3"
      },
      "outputs": [],
      "source": [
        "# # data from https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset\n",
        "# import pandas as pd\n",
        "\n",
        "# salary = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/Salary_Data.csv')\n",
        "# salary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2301fe9",
      "metadata": {
        "id": "d2301fe9"
      },
      "outputs": [],
      "source": [
        "# # bivariate scatter plot\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.regplot(data=salary, x='YearsExperience', y='Salary', ci=False)\n",
        "# plt.title('Correlation with Line of Best Fit');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ed403e",
      "metadata": {
        "id": "89ed403e"
      },
      "source": [
        "### Spurious Correlations\n",
        "\n",
        "https://www.tylervigen.com/spurious-correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf554c0d",
      "metadata": {
        "id": "cf554c0d"
      },
      "source": [
        "### Pearson's Correlation Coefficient\n",
        "\n",
        "A measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus, it is essentially a normalized measurement of the covariance.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
        "\n",
        "Galton conceived the idea of correlation as a measure of how two variables relate such as heights and arm lengths\n",
        "\n",
        "$r = \\rho_{x,y} = \\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{\\frac{1}{N}\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt\\frac{\\sum(x-\\bar{x})^2}{N}\\sqrt\\frac{\\sum(y-\\bar{y})^2}{N}}  = \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt{\\sum(x-\\bar{x})^2}\\sqrt{\\sum(y-\\bar{y})^2}}$\n",
        "\n",
        "* r has a range of -1 to 1\n",
        "* When it equals (-)1, one feature can predict another feature precisely\n",
        "* When it equals 0, the prediction is no better than a random guess\n",
        "* The slope is agnostic in regards to cause and effect\n",
        "* One could cause the other, or there may be a third variable controlling both."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c722e0ff",
      "metadata": {
        "id": "c722e0ff"
      },
      "source": [
        "$\\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{\\frac{1}{N}\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt\\frac{\\sum(x-\\bar{x})^2}{N}\\sqrt\\frac{\\sum(y-\\bar{y})^2}{N}}$\n",
        "\n",
        "* Covariance measures the directional relationship between the returns on two assets. A positive covariance means asset returns move together, while a negative covariance means they move inversely. Covariance is calculated by analyzing at-return surprises (standard deviations from the expected return) or multiplying the correlation between the two random variables by the standard deviation of each variable. https://www.investopedia.com/terms/c/covariance.asp\n",
        "* A standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean. Low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out. https://www.nlm.nih.gov/nichsr/stats_tutorial/section2/mod8_sd.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce37da4b",
      "metadata": {
        "id": "ce37da4b"
      },
      "source": [
        "**r = 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c13bfb",
      "metadata": {
        "id": "54c13bfb"
      },
      "outputs": [],
      "source": [
        "# import sklearn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# y = pd.Series([1, 2, 3, 4, 5, 6])\n",
        "# x = pd.Series([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# correlation = y.corr(x)\n",
        "# print('r = ', correlation)\n",
        "\n",
        "# # plotting the data\n",
        "# plt.scatter(x, y)\n",
        "\n",
        "# # This will fit the best line into the graph\n",
        "# plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))\n",
        "#          (np.unique(x)), color='red');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**r = -1**"
      ],
      "metadata": {
        "id": "0BXsQa850aGN"
      },
      "id": "0BXsQa850aGN"
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# y = pd.Series([1, 2, 3, 4, 5, 6])\n",
        "# x = pd.Series([6, 5, 4, 3, 2, 1])\n",
        "\n",
        "# correlation = y.corr(x)\n",
        "# print('r = ', correlation)\n",
        "\n",
        "# # plotting the data\n",
        "# plt.scatter(x, y)\n",
        "\n",
        "# # This will fit the best line into the graph\n",
        "# plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))\n",
        "#          (np.unique(x)), color='red');"
      ],
      "metadata": {
        "id": "_E4kvSRp0dbk"
      },
      "id": "_E4kvSRp0dbk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5b6bbd34",
      "metadata": {
        "id": "5b6bbd34"
      },
      "source": [
        "**r = 0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50dcbf8d",
      "metadata": {
        "id": "50dcbf8d"
      },
      "outputs": [],
      "source": [
        "# import sklearn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# y = pd.Series([9, 4, 1, 0, 1, 4, 9])\n",
        "# x = pd.Series([-3, -2, -1, 0, 1, 2, 3])\n",
        "\n",
        "# correlation = y.corr(x)\n",
        "# print('r = ', correlation)\n",
        "\n",
        "# plt.scatter(x, y)\n",
        "\n",
        "# # line of best fit\n",
        "# plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))\n",
        "#          (np.unique(x)), color='red');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# salary.corr()"
      ],
      "metadata": {
        "id": "Lfbcr4hP0rbf"
      },
      "id": "Lfbcr4hP0rbf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spearman's Rank Correlation\n",
        "\n",
        "$\\rho = 1 - \\frac{6\\sum{d_i^2}}{n(n^2-1)}$\n",
        "\n",
        "* $\\rho$ = Spearman's rank correlation coefficient\n",
        "* $d^i$ = difference between the two ranks of each observation\n",
        "* $n$ = number of observations\n",
        "\n",
        "Definition\n",
        "\n",
        "* A Spearman correlation coefficient is also referred to as Spearman rank correlation or Spearman’s rho.  It is typically denoted either with the Greek letter rho (ρ), or rs.  Like all correlation coefficients, Spearman’s rho measures the strength of association between two variables.  As such, the Spearman correlation coefficient is similar to the Pearson correlation coefficient.\n",
        "\n",
        "Sources\n",
        "* https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/spearman-rank-correlation/\n",
        "* https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient"
      ],
      "metadata": {
        "id": "xO-v4po5zJEx"
      },
      "id": "xO-v4po5zJEx"
    },
    {
      "cell_type": "code",
      "source": [
        "# # data from https://data.mendeley.com/datasets/tttrffnjtd/1\n",
        "# import pandas as pd\n",
        "\n",
        "# spearman = pd.read_excel('https://raw.githubusercontent.com/gitmystuff/Datasets/main/Spearman.xls', index_col=0)\n",
        "# spearman.head()"
      ],
      "metadata": {
        "id": "lPkKC_1Z5yhU"
      },
      "id": "lPkKC_1Z5yhU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # method = spearman\n",
        "# spearman._get_numeric_data().corr(method='spearman')"
      ],
      "metadata": {
        "id": "azKYnRrF_Cbx"
      },
      "id": "azKYnRrF_Cbx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "37f760b2",
      "metadata": {
        "id": "37f760b2"
      },
      "source": [
        "### Multicollinearity\n",
        "\n",
        "* Makes it difficult to determine which independent variables are influencing the dependent variable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0eb76d",
      "metadata": {
        "id": "ef0eb76d"
      },
      "source": [
        "### Correlation vs Multicollinearity\n",
        "\n",
        "* Correlation measures how two or more variables move together (good between independent and dependent variables)\n",
        "* (Mutli)collinearity shows a linear relationship, usually high, between features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b8f0c9",
      "metadata": {
        "id": "54b8f0c9"
      },
      "source": [
        "### Correlation Between Features\n",
        "\n",
        "* anything above .9 do something about it\n",
        "* between .5 and .7 may need a closer look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2343619b",
      "metadata": {
        "id": "2343619b"
      },
      "outputs": [],
      "source": [
        "# # data from https://www.kaggle.com/rohankayan/years-of-experience-and-salary-dataset\n",
        "# import pandas as pd\n",
        "\n",
        "# salary = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/INFO4050/main/Datasets/Salary_Data.csv')\n",
        "# salary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96595f1d",
      "metadata": {
        "id": "96595f1d"
      },
      "outputs": [],
      "source": [
        "# X_salary = salary.drop('Salary', axis=1)\n",
        "# y_salary = salary['Salary']\n",
        "# X_salary.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4950d6be",
      "metadata": {
        "id": "4950d6be"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # correlation matrix\n",
        "# sns.set(style=\"white\")\n",
        "\n",
        "# # compute the correlation matrix\n",
        "# corr = X_salary.corr()\n",
        "\n",
        "# # generate a mask for the upper triangle\n",
        "# mask = np.zeros_like(corr, dtype=bool)\n",
        "# mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# # set up the matplotlib figure\n",
        "# f, ax = plt.subplots()\n",
        "\n",
        "# # generate a custom diverging colormap\n",
        "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# # draw the heatmap with the mask and correct aspect ratio\n",
        "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6c1d1c",
      "metadata": {
        "id": "af6c1d1c"
      },
      "source": [
        "## Derived Variables\n",
        "\n",
        "age and salary usually are correlated but ambition can create outliers because a younger person can make a million off a great idea or an older person may be an artist etc.\n",
        "\n",
        "Ambition = YearsExperience / Age"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of derived variables can be used, in certain ways, to mitigate the effects of multicollinearity. However, it's crucial to understand that it's not a universal \"fix,\" and it needs to be applied carefully. Here's how it works:\n",
        "\n",
        "**How Derived Variables Can Help:**\n",
        "\n",
        "* **Combining Correlated Variables:**\n",
        "    * If you have two or more highly correlated variables that represent a similar underlying concept, you can create a single derived variable that captures that concept.\n",
        "    * For example, if you have variables for \"height in inches\" and \"height in centimeters,\" which are perfectly correlated, you could create a single \"height\" variable.\n",
        "    * This reduces the redundancy and eliminates the direct multicollinearity between the original variables.\n",
        "* **Creating Ratio or Index Variables:**\n",
        "    * Sometimes, multicollinearity arises from using raw values of related variables. You can create ratio or index variables that represent the relationship between those variables.\n",
        "    * For instance, instead of using \"income\" and \"number of people in household\" separately, you could create a \"income per capita\" variable.\n",
        "    * This can reduce multicollinearity by focusing on the relative relationship rather than the absolute values.\n",
        "* **Principal Component Analysis (PCA):**\n",
        "    * PCA is a more advanced technique that creates derived variables called principal components. These components are linear combinations of the original variables and are designed to be uncorrelated.\n",
        "    * PCA effectively transforms the original correlated variables into a new set of uncorrelated variables, addressing multicollinearity.\n",
        "    * This is a form of creating derived variables, that are then used in place of the original variables.\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "* **Information Loss:**\n",
        "    * Combining variables or creating indices can lead to some loss of information. You need to carefully consider whether the derived variable adequately captures the information you need.\n",
        "* **Interpretability:**\n",
        "    * Derived variables can sometimes be more difficult to interpret than the original variables. This is especially true with techniques like PCA, where the principal components may not have a clear real-world meaning.\n",
        "* **Domain Knowledge:**\n",
        "    * The decision of how to create derived variables to address multicollinearity should be guided by domain knowledge. You need to understand the relationships between your variables and choose transformations that make sense in the context of your problem.\n",
        "* **Not a Universal Solution:**\n",
        "    * Derived variables are not always the best solution for multicollinearity. Sometimes, simpler approaches like removing one of the correlated variables or using regularization techniques may be more appropriate.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Derived variables can be a valuable tool for addressing multicollinearity, particularly when you can combine correlated variables or create meaningful ratios or indices. However, it's essential to use them judiciously and consider the potential trade-offs.\n"
      ],
      "metadata": {
        "id": "HIR_LtHvXneJ"
      },
      "id": "HIR_LtHvXneJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b87d9c",
      "metadata": {
        "id": "d1b87d9c"
      },
      "outputs": [],
      "source": [
        "# # combine YearsExperience and Age\n",
        "# X_salary['Ambition'] = X_salary['YearsExperience'] / X_salary['Age']\n",
        "# salary['Ambition'] = salary['YearsExperience'] / salary['Age']\n",
        "# X_salary.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a7bb7f",
      "metadata": {
        "id": "86a7bb7f"
      },
      "outputs": [],
      "source": [
        "# # correlation matrix\n",
        "# sns.set(style=\"white\")\n",
        "\n",
        "# # compute the correlation matrix\n",
        "# corr = X_salary.corr()\n",
        "\n",
        "# # generate a mask for the upper triangle\n",
        "# mask = np.zeros_like(corr, dtype=bool)\n",
        "# mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# # set up the matplotlib figure\n",
        "# f, ax = plt.subplots()\n",
        "\n",
        "# # generate a custom diverging colormap\n",
        "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# # draw the heatmap with the mask and correct aspect ratio\n",
        "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9244ec",
      "metadata": {
        "id": "6b9244ec"
      },
      "outputs": [],
      "source": [
        "# # showing correlation of multiple features with one target\n",
        "# X_salary.corrwith(y_salary).plot.bar(\n",
        "#         title = \"Correlation with Target\", fontsize = 15,\n",
        "#         rot = 45, grid = True);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8614e8b8",
      "metadata": {
        "id": "8614e8b8"
      },
      "source": [
        "### Example of Engineering a Feature by Transforming its Values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dbb75c0",
      "metadata": {
        "id": "5dbb75c0"
      },
      "source": [
        "### Logarithm and Moore's Law\n",
        "\n",
        "Moore's law is the observation that the number of transistors in a dense integrated circuit (IC) doubles about every two years. Moore's law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship linked to gains from experience in production.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Moore's_law"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4745aa5",
      "metadata": {
        "id": "d4745aa5"
      },
      "outputs": [],
      "source": [
        "# # get the data\n",
        "# import pandas as pd\n",
        "\n",
        "# moores = pd.read_csv('https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv', header=None)\n",
        "# moores.columns = ['year', 'transistors']\n",
        "# moores.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0897ddb2",
      "metadata": {
        "id": "0897ddb2"
      },
      "outputs": [],
      "source": [
        "# # plot the data\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.scatter(moores['year'], moores['transistors']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ddd9ee8",
      "metadata": {
        "id": "2ddd9ee8"
      },
      "outputs": [],
      "source": [
        "# # apply log to transistors\n",
        "# import numpy as np\n",
        "\n",
        "# moores['log_trans'] = np.log(moores.transistors)\n",
        "# plt.scatter(moores['year'], moores['log_trans']);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6416fcd5",
      "metadata": {
        "id": "6416fcd5"
      },
      "source": [
        "### More on Logarithms\n",
        "\n",
        "* 10 * 10 (10 and 100)\n",
        "* 10 * 10 * 10 (10 and 1000)\n",
        "* power of 0 = 1 (single item)\n",
        "* power of 1 = 10\n",
        "* power of 3 = thousand\n",
        "* power of 6 = million\n",
        "* power of 9 = billion\n",
        "* power of 12 = trillion\n",
        "* power of 23 = number of molecules in a dozen grams of carbon\n",
        "* power of 80 = number of molecules in the universe\n",
        "\n",
        "A 0 to 80 scale took us from a single item to the number of things in the universe.\n",
        "\n",
        "https://betterexplained.com/articles/using-logs-in-the-real-world/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation"
      ],
      "metadata": {
        "id": "nxH0SvgSEoHP"
      },
      "id": "nxH0SvgSEoHP"
    },
    {
      "cell_type": "markdown",
      "id": "415adab4",
      "metadata": {
        "id": "415adab4"
      },
      "source": [
        "### Mean, Median, Mode Imputation\n",
        "\n",
        "* Mean if normal\n",
        "* Median if skewed\n",
        "* Used for MCAR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # get data\n",
        "# import pandas as pd\n",
        "\n",
        "# houses = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/house-prices.csv')\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     houses.drop('SalePrice', axis=1),\n",
        "#     houses['SalePrice'],\n",
        "#     test_size=0.25,\n",
        "#     random_state=42)\n",
        "\n",
        "# X_train.head()"
      ],
      "metadata": {
        "id": "HL4APoTAABj0"
      },
      "id": "HL4APoTAABj0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # find nulls\n",
        "# X_train.info()"
      ],
      "metadata": {
        "id": "AesVK55RA0Kk"
      },
      "id": "AesVK55RA0Kk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train.isnull().sum()"
      ],
      "metadata": {
        "id": "Ois4HcLTBnuq"
      },
      "id": "Ois4HcLTBnuq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for feat in X_train.columns:\n",
        "#     if X_train[feat].isnull().any():\n",
        "#         print(feat, X_train[feat].isnull().sum())"
      ],
      "metadata": {
        "id": "zh_1vB8hBsl2"
      },
      "id": "zh_1vB8hBsl2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nulls = [feat for feat in X_train.columns if X_train[feat].isnull().any()]\n",
        "# nulls"
      ],
      "metadata": {
        "id": "WDAI8csVBvYP"
      },
      "id": "WDAI8csVBvYP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0568e5b9",
      "metadata": {
        "id": "0568e5b9"
      },
      "outputs": [],
      "source": [
        "# # example of some nulls\n",
        "# X_train[['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4023bf40",
      "metadata": {
        "id": "4023bf40"
      },
      "outputs": [],
      "source": [
        "# X_train[['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb279d8a",
      "metadata": {
        "id": "cb279d8a"
      },
      "outputs": [],
      "source": [
        "# # fill na with mean median mode\n",
        "# mmm = pd.DataFrame(columns = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt'])\n",
        "# mmm['LotFrontage'] = X_train['LotFrontage'].fillna(round(X_train['LotFrontage'].mean(), 2))\n",
        "# mmm['MasVnrArea'] = X_train['MasVnrArea'].fillna(X_train['MasVnrArea'].median())\n",
        "# mmm['GarageYrBlt'] = X_train['GarageYrBlt'].fillna(X_train['GarageYrBlt'].mode()[0])\n",
        "# mmm.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3537932",
      "metadata": {
        "id": "b3537932"
      },
      "source": [
        "### Arbitrary Constants\n",
        "\n",
        "* Discovers if MNAR\n",
        "* Goal is to flag missing values\n",
        "* Use values not in distribution\n",
        "* Importance of missingness if present\n",
        "* Depends on the model (Linear models maybe not because more arbitrary values in distribution, Trees maybe)\n",
        "\n",
        "We don't want to impute mean, median, etc because it looks like the data. We want to emphasize the missing data because we believe it's missing not at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fa86b1",
      "metadata": {
        "id": "94fa86b1"
      },
      "outputs": [],
      "source": [
        "# # recall missing values (non-null)\n",
        "# print(X_train.shape)\n",
        "# print(X_train[nulls].info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbad662",
      "metadata": {
        "id": "3fbad662"
      },
      "outputs": [],
      "source": [
        "# X_train['GarageType'].fillna('Missing', inplace=True)\n",
        "# X_train['GarageType'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b41f7c7",
      "metadata": {
        "id": "6b41f7c7"
      },
      "source": [
        "### End of Distribution\n",
        "\n",
        "* If normal we can use -3, 3 standard deviations\n",
        "* If skewed we can use IQR proximity rule (iqr x 1.5, or iqr x 3)\n",
        "* Flag the missing value where observations are rarely represented\n",
        "* Used in finances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eaa8bc2",
      "metadata": {
        "id": "8eaa8bc2"
      },
      "outputs": [],
      "source": [
        "# # histogram of LotFrontage\n",
        "# X_train['LotFrontage'].hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b44dc1",
      "metadata": {
        "id": "17b44dc1"
      },
      "outputs": [],
      "source": [
        "# # boxplot of LotFrontage\n",
        "# X_train.boxplot('LotFrontage');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cfd338",
      "metadata": {
        "id": "92cfd338"
      },
      "outputs": [],
      "source": [
        "# # iqr as na\n",
        "# iqr = X_train['LotFrontage'].quantile(0.75) - X_train['LotFrontage'].quantile(0.25)\n",
        "# end_of_distribution = X_train['LotFrontage'].quantile(0.75) + (1.5 * iqr)\n",
        "# X_train['LotFrontage_Imputed'] = X_train['LotFrontage'].fillna(end_of_distribution)\n",
        "# print(end_of_distribution)\n",
        "# print(X_train['LotFrontage_Imputed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc3f8ec",
      "metadata": {
        "id": "0bc3f8ec"
      },
      "outputs": [],
      "source": [
        "# X_train.boxplot('LotFrontage_Imputed');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28c3f668",
      "metadata": {
        "id": "28c3f668"
      },
      "source": [
        "## Categorical Encoding\n",
        "* Sklearn One Hot Encoding\n",
        "* Dummy Trap\n",
        "* Pandas get_dummies\n",
        "* Labelizer\n",
        "* Weight of Evidence\n",
        "* Frequency Encoding\n",
        "\n",
        "### Categorical Data\n",
        "* Nominal (Cat or Dog)\n",
        "* Ordinal (Grades)\n",
        "* Works better for limited labels in a category\n",
        "* Engineer features with many labels\n",
        "\n",
        "### Multicollinearity\n",
        "* Predictors need to be independent of each other\n",
        "* https://www.theanalysisfactor.com/multicollinearity-explained-visually/\n",
        "* https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/\n",
        "* Cats_and_Dogs = [Cat, Dog, Dog, Cat, Cat, Dog]\n",
        "* Cats = [1, 0, 0, 1, 1, 0]\n",
        "* Dogs = [0, 1, 1, 0, 0, 1]\n",
        "\n",
        "### Mismatch in Training and Test\n",
        "\n",
        "* Some labels in the train set don't show up in the test set\n",
        "\n",
        "https://towardsdatascience.com/beware-of-the-dummy-variable-trap-in-pandas-727e8e6b8bde"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1beccfa2",
      "metadata": {
        "id": "1beccfa2"
      },
      "source": [
        "### One Hot Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2451d6",
      "metadata": {
        "id": "7e2451d6"
      },
      "outputs": [],
      "source": [
        "# # sklearn OneHotEncoder\n",
        "# # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "# # https://stackoverflow.com/questions/50473381/scikit-learns-labelbinarizer-vs-onehotencoder\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# pets = ['dog', 'cat', 'cat', 'dog', 'turtle', 'cat', 'cat', 'turtle', 'dog', 'cat']\n",
        "# print('cat = 0; dog = 1; turtle = 2')\n",
        "# le = LabelEncoder()\n",
        "# int_values = le.fit_transform(pets)\n",
        "# print('Pets:', pets)\n",
        "# print('Label Encoder:', int_values)\n",
        "# int_values = int_values.reshape(len(int_values), 1)\n",
        "# print(pd.Series(pets))\n",
        "\n",
        "# ohe = OneHotEncoder(sparse_output=False)\n",
        "# ohe = ohe.fit_transform(int_values)\n",
        "# print('One Hot Encoder:\\n', ohe)\n",
        "\n",
        "# lb = LabelBinarizer()\n",
        "# print('Label Binarizer:\\n', lb.fit_transform(int_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a558af81",
      "metadata": {
        "id": "a558af81"
      },
      "outputs": [],
      "source": [
        "# pets = pd.DataFrame(pd.Series(pets), columns=['Pets'])\n",
        "# pets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af38b01",
      "metadata": {
        "id": "3af38b01"
      },
      "outputs": [],
      "source": [
        "# ohe = OneHotEncoder(sparse_output=False)\n",
        "# ohe_pets = ohe.fit_transform(pets)\n",
        "# pets_df = pd.DataFrame(ohe_pets, columns=ohe.get_feature_names_out(['Pets']))\n",
        "# pets_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jq5DV6Z4zwYj"
      },
      "id": "jq5DV6Z4zwYj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install Faker"
      ],
      "metadata": {
        "id": "viPiM1e9ycLy"
      },
      "execution_count": null,
      "outputs": [],
      "id": "viPiM1e9ycLy"
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from faker import Faker\n",
        "# fake = Faker()\n",
        "\n",
        "# output = []\n",
        "# for x in range(100):\n",
        "#   sex = np.random.choice(['egg', 'seed'], p=[0.5, 0.5])\n",
        "#   output.append(\n",
        "#     {\n",
        "#         'sex': sex,\n",
        "#         'brain_wave': np.random.choice(['BETA', 'ALPHA', 'THETA']),\n",
        "#         'given_name': fake.first_name_female() if sex=='egg' else fake.first_name_male(),\n",
        "#         'surname': fake.last_name(),\n",
        "#         'space_zone': fake.zipcode(),\n",
        "#         })\n",
        "\n",
        "# df = pd.DataFrame(output)\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "iNmuq_t4ygg9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "iNmuq_t4ygg9"
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy_cols = ['sex', 'space_zone', 'brain_wave']\n",
        "# df_dummies = pd.get_dummies(df, columns=dummy_cols)\n",
        "\n",
        "# print(df_dummies.shape)\n",
        "# df_dummies.head()"
      ],
      "metadata": {
        "id": "6iIKCyiKzTud"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6iIKCyiKzTud"
    },
    {
      "cell_type": "markdown",
      "id": "3eaf9f4e",
      "metadata": {
        "id": "3eaf9f4e"
      },
      "source": [
        "### Dummy Trap\n",
        "\n",
        "The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
        "\n",
        "https://www.learndatasci.com/glossary/dummy-variable-trap/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f408874",
      "metadata": {
        "id": "7f408874"
      },
      "outputs": [],
      "source": [
        "# pets_df.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ec8b974",
      "metadata": {
        "id": "5ec8b974"
      },
      "outputs": [],
      "source": [
        "# ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
        "# ohe_pets = ohe.fit_transform(pets)\n",
        "# pets_df = pd.DataFrame(ohe_pets, columns=ohe.get_feature_names_out(['Pets']))\n",
        "# pets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2271c79c",
      "metadata": {
        "id": "2271c79c"
      },
      "outputs": [],
      "source": [
        "# pets_df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day of Week Encoding\n",
        "\n",
        "* https://mikulskibartosz.name/time-in-machine-learning"
      ],
      "metadata": {
        "id": "6yia75mH8p43"
      },
      "id": "6yia75mH8p43"
    },
    {
      "cell_type": "markdown",
      "id": "084c15ac",
      "metadata": {
        "id": "084c15ac"
      },
      "source": [
        "### Get Dummies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576cb600",
      "metadata": {
        "id": "576cb600"
      },
      "outputs": [],
      "source": [
        "# # using pandas get_dummies\n",
        "# import pandas as pd\n",
        "\n",
        "# X_dummy = pd.get_dummies(X_train[['GarageType', 'GarageQual']], drop_first=True)\n",
        "# y_dummy = pd.get_dummies(X_test[['GarageType', 'GarageQual']], drop_first=True)\n",
        "# print(X_dummy.shape)\n",
        "# print(y_dummy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270bc2d6",
      "metadata": {
        "id": "270bc2d6"
      },
      "outputs": [],
      "source": [
        "# # using one hot encoder\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
        "\n",
        "# ohe_train = ohe.fit_transform(X_train[['GarageType', 'GarageQual']].dropna())\n",
        "# ohe_train = pd.DataFrame(ohe_train, columns=ohe.get_feature_names_out(['GarageType', 'GarageQual']))\n",
        "# print(ohe_train.shape)\n",
        "# ohe_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2c740cf",
      "metadata": {
        "id": "e2c740cf"
      },
      "outputs": [],
      "source": [
        "# # ohe is already trained\n",
        "# ohe_test = ohe.transform(X_test[['GarageType', 'GarageQual']].dropna())\n",
        "# ohe_test = pd.DataFrame(ohe_test, columns=ohe_train.columns)\n",
        "# print(ohe_test.shape)\n",
        "# ohe_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45820551",
      "metadata": {
        "id": "45820551"
      },
      "source": [
        "### One Hot Encoding Alternatives\n",
        "\n",
        "For features with many labels\n",
        "\n",
        "* https://medium.com/analytics-vidhya/stop-one-hot-encoding-your-categorical-variables-bbb0fba89809\n",
        "* https://medium.com/swlh/stop-one-hot-encoding-your-categorical-features-avoid-curse-of-dimensionality-16743c32cea4\n",
        "* https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02 (frequency and mean encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b079a6a9",
      "metadata": {
        "id": "b079a6a9"
      },
      "outputs": [],
      "source": [
        "# # review features with multiple labels\n",
        "# # identify features with more than 5 features\n",
        "# mult_labels = []\n",
        "# freq_feats = []\n",
        "\n",
        "# for val in X_train.columns.sort_values():\n",
        "#   if val in nulls:\n",
        "#     print(val, len(X_train[val].dropna().unique()))\n",
        "#     mult_labels.append(val)\n",
        "#     if len(X_train[val].dropna().unique()) > 4:\n",
        "#       freq_feats.append(val)\n",
        "\n",
        "# print(mult_labels)\n",
        "# print(freq_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e742289",
      "metadata": {
        "id": "1e742289"
      },
      "outputs": [],
      "source": [
        "# # fill frequency\n",
        "# print(X_train['GarageType'].value_counts())\n",
        "# for feat in freq_feats:\n",
        "#     freq = X_train.groupby(feat).size()/len(X_train)\n",
        "#     X_train[feat] = X_train[feat].map(freq)\n",
        "#     freq = X_test.groupby(feat).size()/len(X_test)\n",
        "#     X_test[feat] = X_test[feat].map(freq)\n",
        "\n",
        "# print(X_train['GarageType'].value_counts())\n",
        "# print(X_train['GarageType'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5049aa",
      "metadata": {
        "id": "5e5049aa"
      },
      "source": [
        "### Bi-Label Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dba2646",
      "metadata": {
        "id": "6dba2646"
      },
      "outputs": [],
      "source": [
        "# # get and train test split data\n",
        "# import seaborn as sns\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# titanic = sns.load_dataset('titanic')\n",
        "# X_train, X_test, y_train, y_test = train_test_split(titanic.drop(['survived'], axis=1), titanic['survived'], test_size=.25, random_state=42)\n",
        "# X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a98ba2",
      "metadata": {
        "id": "48a98ba2"
      },
      "outputs": [],
      "source": [
        "# titanic.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af697a72",
      "metadata": {
        "id": "af697a72"
      },
      "outputs": [],
      "source": [
        "# # bi-label mapping\n",
        "# # whatever you do for X_train, do for X_test\n",
        "# X_train['sex'] = X_train['sex'].map({'male':0,'female':1})\n",
        "# X_test['sex'] = X_test['sex'].map({'male':0,'female':1})\n",
        "# X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38db741f",
      "metadata": {
        "id": "38db741f"
      },
      "source": [
        "### Encoding Order\n",
        "\n",
        "* Bilabel Mapping (2 labels)\n",
        "* Frequency (5+ labels)\n",
        "* One Hot Encoding (3 - 5 labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8598a4b1",
      "metadata": {
        "id": "8598a4b1"
      },
      "source": [
        "## Outliers\n",
        "\n",
        "* Treat outliers as missing data and impute accordingly\n",
        "* Impose min max values\n",
        "* Take care of altering meaningful data\n",
        "* Outliers should be detected and removed from train only\n",
        "\n",
        "https://www.projectpro.io/recipes/deal-with-outliers-in-python\n",
        "\n",
        "* Drop\n",
        "* Mark\n",
        "* Rescale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940596a5",
      "metadata": {
        "id": "940596a5"
      },
      "outputs": [],
      "source": [
        "# # get data\n",
        "# from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# housing = fetch_california_housing()\n",
        "# print(housing.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d33784",
      "metadata": {
        "id": "b6d33784"
      },
      "outputs": [],
      "source": [
        "# # get keys\n",
        "# housing.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda64127",
      "metadata": {
        "id": "dda64127"
      },
      "outputs": [],
      "source": [
        "# # create housing dataframe\n",
        "# import pandas as pd\n",
        "\n",
        "# housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "# housing_df['MedHouseVal'] = housing.target\n",
        "# housing_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43dd0266",
      "metadata": {
        "id": "43dd0266"
      },
      "outputs": [],
      "source": [
        "# # train test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     housing_df.drop('MedHouseVal', axis=1),\n",
        "#     housing_df['MedHouseVal'],\n",
        "#     test_size=0.25,\n",
        "#     random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea99d3d",
      "metadata": {
        "id": "cea99d3d"
      },
      "outputs": [],
      "source": [
        "# # histograms\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X_train.hist()\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32083b0e",
      "metadata": {
        "id": "32083b0e"
      },
      "outputs": [],
      "source": [
        "# # boxplots\n",
        "# X_train.boxplot(rot=45)\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f68766",
      "metadata": {
        "id": "46f68766"
      },
      "outputs": [],
      "source": [
        "# X_train.boxplot('MedInc');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c412c887",
      "metadata": {
        "id": "c412c887"
      },
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# sns.violinplot(x=X_train['MedInc']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb046d6e",
      "metadata": {
        "id": "eb046d6e"
      },
      "outputs": [],
      "source": [
        "# # prob plot\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# stats.probplot(X_train['MedInc'], plot=plt);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10a61b2",
      "metadata": {
        "id": "d10a61b2"
      },
      "source": [
        "#### Boxplot and Normal Curve Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d745e2f",
      "metadata": {
        "id": "3d745e2f"
      },
      "outputs": [],
      "source": [
        "# # compare boxplot with normal distribution\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# data = stats.norm.rvs(size=1000)\n",
        "# mu, std = stats.norm.fit(data)\n",
        "\n",
        "# x = np.linspace(-3, 3, 1000)\n",
        "# p = stats.norm.pdf(x, mu, std)\n",
        "# plt.plot(x, p, 'k', linewidth=2)\n",
        "# plt.boxplot(data, vert=False)\n",
        "# plt.xlabel('Values')\n",
        "# plt.ylabel('Probabilities')\n",
        "# plt.title(f'mu = {mu:.2f},  std = {std:.2f}')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3bb198",
      "metadata": {
        "id": "8d3bb198"
      },
      "outputs": [],
      "source": [
        "# # compare with AveBedrms\n",
        "# X_train['MedInc'].plot.kde()\n",
        "# X_train.boxplot('MedInc', vert=False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b133ce96",
      "metadata": {
        "scrolled": true,
        "id": "b133ce96"
      },
      "outputs": [],
      "source": [
        "# # find iqr and inner outer boundaries\n",
        "# q1 = X_train['MedInc'].quantile(0.25)\n",
        "# q3 = X_train['MedInc'].quantile(0.75)\n",
        "# iqr = q3 - q1\n",
        "\n",
        "# lower_inner_fence = q1 - (1.5 * iqr)\n",
        "# upper_inner_fence = q3 + (1.5 * iqr)\n",
        "# lower_outer_fence = q1 - (1.5 * iqr)\n",
        "# upper_outer_fence = q3 + (1.5 * iqr)\n",
        "\n",
        "# print(f'Q1: {q1:.2f} - Q3: {q3:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19d61574",
      "metadata": {
        "id": "19d61574"
      },
      "outputs": [],
      "source": [
        "# # print outliers by feature\n",
        "# for feat in X_train._get_numeric_data().columns[1:]:\n",
        "#     q1 = X_train[feat].quantile(0.25)\n",
        "#     q3 = X_train[feat].quantile(0.75)\n",
        "#     iqr = q3 - q1\n",
        "#     lower_fence = (q1 - 1.5 * iqr)\n",
        "#     upper_fence = (q3 + 1.5 * iqr)\n",
        "#     lower_count = X_train[feat][X_train[feat] < lower_fence].count()\n",
        "#     upper_count = X_train[feat][X_train[feat] > upper_fence].count()\n",
        "#     print(f'{feat} outliers = {lower_count + upper_count}: lower_fence: {lower_fence}, upper_fence: {upper_fence}, lower_count: {lower_count}, upper_count: {upper_count}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df780c12",
      "metadata": {
        "id": "df780c12"
      },
      "outputs": [],
      "source": [
        "# # check our numbers\n",
        "# X_train['MedInc'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c392138",
      "metadata": {
        "id": "3c392138"
      },
      "source": [
        "### Outlier Trimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc2800b",
      "metadata": {
        "id": "5fc2800b"
      },
      "outputs": [],
      "source": [
        "# # flag the rows with outliers\n",
        "# import numpy as np\n",
        "\n",
        "# outliers = np.where(X_train['MedInc'] < lower_inner_fence, True,\n",
        "#                    np.where(X_train['MedInc'] > upper_inner_fence, True, False))\n",
        "\n",
        "# X_train_trimmed = X_train.loc[outliers]\n",
        "# print(X_train.shape, X_train_trimmed.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fcbcdbc",
      "metadata": {
        "id": "9fcbcdbc"
      },
      "source": [
        "### IQR Proximity Rule Capping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d39473",
      "metadata": {
        "scrolled": true,
        "id": "d7d39473"
      },
      "outputs": [],
      "source": [
        "# # cap outliers\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# X_train['capped'] = np.where(X_train['MedInc'] < lower_inner_fence, lower_inner_fence,\n",
        "#                    np.where(X_train['MedInc'] > upper_inner_fence, upper_inner_fence, X_train['MedInc']))\n",
        "\n",
        "# stats.probplot(X_train['capped'], plot=plt);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "889970a8",
      "metadata": {
        "id": "889970a8"
      },
      "source": [
        "## Scaling\n",
        "\n",
        "* Coefficients of linear models are influenced by the scale of the feature\n",
        "* Features with larger scales dominate smaller scales\n",
        "* Some algorithms, like PCA, require features to be centered at 0\n",
        "\n",
        "https://www.atoti.io/articles/when-to-perform-a-feature-scaling/\n",
        "\n",
        "* from sklearn.preprocessing import MinMaxScaler\n",
        "* from sklearn.preprocessing import minmax_scale\n",
        "* from sklearn.preprocessing import MaxAbsScaler\n",
        "* from sklearn.preprocessing import StandardScaler\n",
        "* from sklearn.preprocessing import RobustScaler\n",
        "* from sklearn.preprocessing import Normalizer\n",
        "* from sklearn.preprocessing import QuantileTransformer\n",
        "* from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98764eee",
      "metadata": {
        "id": "98764eee"
      },
      "source": [
        "### Standardization\n",
        "\n",
        "$z = \\frac{(x - \\bar{x})}{\\sigma}$\n",
        "\n",
        "* Centers data around 0\n",
        "* Scales the std to 1\n",
        "* Preserves original shape\n",
        "* Preserves outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86aa2e23",
      "metadata": {
        "id": "86aa2e23"
      },
      "outputs": [],
      "source": [
        "# X_train.drop('capped', axis=1, inplace=True)\n",
        "# X_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151db938",
      "metadata": {
        "id": "151db938"
      },
      "source": [
        "Characteristics of X_train\n",
        "* Mean values not centered around 0\n",
        "* Std not 1\n",
        "* Features have various magnitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb47dd54",
      "metadata": {
        "id": "cb47dd54"
      },
      "outputs": [],
      "source": [
        "# # standardize features\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(X_train)\n",
        "# standardized_X = scaler.transform(X_train)\n",
        "# standardized_yX = scaler.transform(X_test) # we use the scaler that was trained on the X_train\n",
        "# X_train_standardized = pd.DataFrame(standardized_X, columns=X_train.columns)\n",
        "# X_train_standardized.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35b4037",
      "metadata": {
        "id": "f35b4037"
      },
      "outputs": [],
      "source": [
        "# # compare histograms\n",
        "# X_train.hist()\n",
        "# X_train_standardized.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd52719",
      "metadata": {
        "id": "9dd52719"
      },
      "source": [
        "### MinMaxScaling (Normalization)\n",
        "\n",
        "* Does not center the mean around 0\n",
        "* Std (variance) differ\n",
        "* May not preserve original shape\n",
        "* 0 to 1 range\n",
        "* Sensitive to outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4669626",
      "metadata": {
        "id": "f4669626"
      },
      "outputs": [],
      "source": [
        "# # minmax scaling\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scaler = MinMaxScaler()\n",
        "# scaler.fit(X_train)\n",
        "# minmax = scaler.transform(X_train)\n",
        "# # don't forget X_test\n",
        "# X_train_minmax = pd.DataFrame(minmax, columns=X_train.columns)\n",
        "# X_train_minmax.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2e620d",
      "metadata": {
        "id": "1d2e620d"
      },
      "outputs": [],
      "source": [
        "# # visual comparison\n",
        "# X_train.hist()\n",
        "# X_train_minmax.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "103b867c",
      "metadata": {
        "id": "103b867c"
      },
      "source": [
        "### Mean Normalization\n",
        "\n",
        "* Centers the mean at 0\n",
        "* Std (variance) will differ\n",
        "* May alter original distribution\n",
        "* -1 to 1 range\n",
        "* Preserves outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e8ad1e",
      "metadata": {
        "id": "d4e8ad1e"
      },
      "outputs": [],
      "source": [
        "# # find the means\n",
        "# means = X_train.mean(axis=0)\n",
        "# means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "030fe657",
      "metadata": {
        "id": "030fe657"
      },
      "outputs": [],
      "source": [
        "# # find the ranges\n",
        "# ranges = X_train.max(axis=0) - X_train.min(axis=0)\n",
        "# ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8b79c1",
      "metadata": {
        "id": "9c8b79c1"
      },
      "outputs": [],
      "source": [
        "# # mean scale the data\n",
        "# X_train_meanscale = (X_train - means) / ranges\n",
        "# # don't forget X_test\n",
        "# X_train_meanscale.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6627095",
      "metadata": {
        "id": "a6627095"
      },
      "outputs": [],
      "source": [
        "# # visual comparison\n",
        "# X_train.hist()\n",
        "# X_train_meanscale.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab381088",
      "metadata": {
        "id": "ab381088"
      },
      "source": [
        "### RobustScaler\n",
        "\n",
        "* Replaces median with iqr\n",
        "* Variance varies\n",
        "* May not preserve distribution\n",
        "* Min max varies\n",
        "* Robust to outliers https://www.statisticshowto.com/robust-statistics/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af432858",
      "metadata": {
        "id": "af432858"
      },
      "outputs": [],
      "source": [
        "# # robust scaler\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# scaler = RobustScaler()\n",
        "# scaler.fit(X_train)\n",
        "# robust = scaler.transform(X_train)\n",
        "# # don't forget X_test\n",
        "# X_train_robust = pd.DataFrame(robust, columns=X_train.columns)\n",
        "# X_train_robust.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708f0ac5",
      "metadata": {
        "id": "708f0ac5"
      },
      "outputs": [],
      "source": [
        "# # visual comparison\n",
        "# X_train.hist()\n",
        "# X_train_robust.hist()\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b6765f",
      "metadata": {
        "id": "e7b6765f"
      },
      "source": [
        "### PowerTransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80eefceb",
      "metadata": {
        "id": "80eefceb"
      },
      "outputs": [],
      "source": [
        "# # PowerTransformer scaler for outliers\n",
        "# from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# feat_scales = []\n",
        "\n",
        "# scaler = PowerTransformer()\n",
        "\n",
        "# for feat in feat_scales:\n",
        "#     X_train[feat] = scaler.fit_transform(X_train[feat].values.reshape(-1,1))\n",
        "\n",
        "# for feat in feat_scales:\n",
        "#     X_test[feat] = scaler.fit_transform(X_test[feat].values.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9945ceeb",
      "metadata": {
        "id": "9945ceeb"
      },
      "source": [
        "### Scaling to Unit Length\n",
        "\n",
        "* Scales a feature vector to 1, norm of 1\n",
        "* Normalizes feature not observation\n",
        "* Divides each observation vector by some norm\n",
        "* Manhattan distance (l1)\n",
        "* Euclidean distance (12)\n",
        "\n",
        "l1(X) = |x1| + |x2| ... + |xn|\n",
        "\n",
        "l2(X) = square root of x1^2 + x2^2 ... + xn^2\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Taxicab_geometry\n",
        "* https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
        "* https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\n",
        "* https://www.atoti.io/articles/when-to-perform-a-feature-scaling/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d086598",
      "metadata": {
        "scrolled": true,
        "id": "8d086598"
      },
      "outputs": [],
      "source": [
        "# # unit length scaling\n",
        "# from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# scaler = Normalizer('l1')\n",
        "# scaler.fit(X_train)\n",
        "# unitlength = scaler.transform(X_train)\n",
        "# # don't forget X_test\n",
        "# X_train_unitlength = pd.DataFrame(unitlength, columns=X_train.columns)\n",
        "# X_train_unitlength.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8e73f1",
      "metadata": {
        "id": "3d8e73f1"
      },
      "outputs": [],
      "source": [
        "# # recall values from original X_train\n",
        "# X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d01d75",
      "metadata": {
        "id": "88d01d75"
      },
      "outputs": [],
      "source": [
        "# # normalize the values\n",
        "# np.round( np.linalg.norm(X_train, ord=1, axis=1), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afac943a",
      "metadata": {
        "id": "afac943a"
      },
      "outputs": [],
      "source": [
        "# # compare the following with the first value in X_train_unitlength.head() below\n",
        "# 4.2 / 1062"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3abd9a2",
      "metadata": {
        "id": "a3abd9a2"
      },
      "outputs": [],
      "source": [
        "# # see above\n",
        "# X_train_unitlength.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fddb10",
      "metadata": {
        "id": "41fddb10"
      },
      "outputs": [],
      "source": [
        "# # visual comparison\n",
        "# X_train.hist()\n",
        "# X_train_unitlength.hist()\n",
        "# plt.tight_layout();"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}